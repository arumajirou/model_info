{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44d3fcf",
   "metadata": {},
   "source": [
    "# timesfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3abc25ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== timesfm Library Structure ===\n",
      "                     Name            Type  \\\n",
      "0          ForecastConfig     Class (クラス)   \n",
      "1  TimesFM_2p5_200M_torch     Class (クラス)   \n",
      "2                 configs  Module (モジュール)   \n",
      "3             timesfm_2p5  Module (モジュール)   \n",
      "4       timesfm_2p5_torch  Module (モジュール)   \n",
      "5                   torch  Module (モジュール)   \n",
      "\n",
      "                                             Docstring_Summary  \n",
      "0                                     Options for forecasting.  \n",
      "1  PyTorch implementation of TimesFM 2.5 with 200M parameters.  \n",
      "2                         Abstract configs for TimesFM layers.  \n",
      "3                                     No description available  \n",
      "4                                              TimesFM models.  \n",
      "5                                     No description available  \n"
     ]
    }
   ],
   "source": [
    "import timesfm\n",
    "import pandas as pd\n",
    "import inspect\n",
    "\n",
    "def get_library_attributes(lib_module):\n",
    "    \"\"\"\n",
    "    ライブラリからパブリックな属性（関数、クラスなど）を取得し、DataFrameに格納する関数。\n",
    "    \n",
    "    Args:\n",
    "        lib_module (module): 調査対象のモジュール（ここではtimesfm）\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: 名前、タイプ、ドキュメントを含むDataFrame\n",
    "    \"\"\"\n",
    "    attributes_data = []\n",
    "\n",
    "    # inspect.getmembersでライブラリ内の全メンバーを取得\n",
    "    for name, obj in inspect.getmembers(lib_module):\n",
    "        # プライベート属性（_で始まるもの）は除外する（通常は内部利用のため）\n",
    "        if name.startswith(\"_\"):\n",
    "            continue\n",
    "\n",
    "        # オブジェクトのタイプを判定\n",
    "        if inspect.isclass(obj):\n",
    "            obj_type = \"Class (クラス)\"\n",
    "        elif inspect.isfunction(obj):\n",
    "            obj_type = \"Function (関数)\"\n",
    "        elif inspect.ismodule(obj):\n",
    "            obj_type = \"Module (モジュール)\"\n",
    "        else:\n",
    "            obj_type = \"Variable/Other (変数/その他)\"\n",
    "\n",
    "        # Docstring（ドキュメンテーション文字列：説明文）を取得\n",
    "        doc = inspect.getdoc(obj)\n",
    "        # ドキュメントがある場合、最初の1行のみを要約として取得（改行コードを除去）\n",
    "        doc_summary = doc.split('\\n')[0] if doc else \"No description available\"\n",
    "\n",
    "        # リストに追加\n",
    "        attributes_data.append({\n",
    "            \"Name\": name,\n",
    "            \"Type\": obj_type,\n",
    "            \"Docstring_Summary\": doc_summary\n",
    "        })\n",
    "\n",
    "    # DataFrameを作成\n",
    "    df = pd.DataFrame(attributes_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# メイン処理の実行\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # timesfmの情報を取得\n",
    "        df_timesfm = get_library_attributes(timesfm)\n",
    "\n",
    "        # 結果の表示（全行表示の設定をしておく）\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        \n",
    "        print(\"=== timesfm Library Structure ===\")\n",
    "        print(df_timesfm)\n",
    "\n",
    "        # 必要に応じてCSV等に出力可能\n",
    "        # df_timesfm.to_csv(\"timesfm_structure.csv\", index=False)\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"エラー: ライブラリのインポートに失敗しました。\\n{e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"予期せぬエラーが発生しました。\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fd8d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/google-research/timesfm.git\n",
      "  Cloning https://github.com/google-research/timesfm.git to c:\\users\\hashimoto.ryohei\\appdata\\local\\temp\\pip-req-build-m2zp6xks\n",
      "  Resolved https://github.com/google-research/timesfm.git to commit 2dcc66fbfe2155adba1af66aa4d564a0ee52f61e\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.26.4 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from timesfm==2.0.0) (2.4.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.23.0 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.5.3 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from timesfm==2.0.0) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (2026.1.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (4.15.0)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli]>=0.23.0->timesfm==2.0.0)\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0)\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (3.0.51)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\lib\\site-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm==2.0.0) (2026.1.4)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Building wheels for collected packages: timesfm\n",
      "  Building wheel for timesfm (pyproject.toml): started\n",
      "  Building wheel for timesfm (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for timesfm: filename=timesfm-2.0.0-py3-none-any.whl size=43839 sha256=7e7044b7b37c38b3ed00bf9970840eabb235a1d21e36d3d17ccb3f836c98080b\n",
      "  Stored in directory: C:\\Users\\hashimoto.ryohei\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-flvnuovf\\wheels\\15\\82\\86\\25cb4996f164a9e58ff452725de32b56291a099a808d2b4c14\n",
      "Successfully built timesfm\n",
      "Installing collected packages: pfzy, InquirerPy, timesfm\n",
      "\n",
      "   ------------- -------------------------- 1/3 [InquirerPy]\n",
      "   ---------------------------------------- 3/3 [timesfm]\n",
      "\n",
      "Successfully installed InquirerPy-0.3.4 pfzy-0.3.4 timesfm-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/timesfm.git 'C:\\Users\\hashimoto.ryohei\\AppData\\Local\\Temp\\pip-req-build-m2zp6xks'\n",
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/google-research/timesfm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02046368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 module                                             name  \\\n",
      "0               timesfm                                   ForecastConfig   \n",
      "1               timesfm                           TimesFM_2p5_200M_torch   \n",
      "2               timesfm                   TimesFM_2p5_200M_torch.compile   \n",
      "3               timesfm                  TimesFM_2p5_200M_torch.forecast   \n",
      "4               timesfm  TimesFM_2p5_200M_torch.forecast_with_covariates   \n",
      "5               timesfm           TimesFM_2p5_200M_torch.from_pretrained   \n",
      "6               timesfm       TimesFM_2p5_200M_torch.generate_model_card   \n",
      "7               timesfm           TimesFM_2p5_200M_torch.load_checkpoint   \n",
      "8               timesfm                     TimesFM_2p5_200M_torch.model   \n",
      "9               timesfm               TimesFM_2p5_200M_torch.push_to_hub   \n",
      "10              timesfm           TimesFM_2p5_200M_torch.save_pretrained   \n",
      "16      timesfm.configs                                          Literal   \n",
      "17      timesfm.configs                                   ForecastConfig   \n",
      "18      timesfm.configs                      RandomFourierFeaturesConfig   \n",
      "19      timesfm.configs                              ResidualBlockConfig   \n",
      "20      timesfm.configs                        StackedTransformersConfig   \n",
      "21      timesfm.configs                                TransformerConfig   \n",
      "31  timesfm.torch.dense                            RandomFourierFeatures   \n",
      "32  timesfm.torch.dense                                    ResidualBlock   \n",
      "33  timesfm.torch.dense                 RandomFourierFeatures.add_module   \n",
      "\n",
      "        kind  \\\n",
      "0      class   \n",
      "1      class   \n",
      "2     method   \n",
      "3     method   \n",
      "4     method   \n",
      "5     method   \n",
      "6     method   \n",
      "7     method   \n",
      "8     method   \n",
      "9     method   \n",
      "10    method   \n",
      "16  callable   \n",
      "17     class   \n",
      "18     class   \n",
      "19     class   \n",
      "20     class   \n",
      "21     class   \n",
      "31     class   \n",
      "32     class   \n",
      "33    method   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           signature  \\\n",
      "0                                                                                                                                                                                                                                                       (max_context: int = 0, max_horizon: int = 0, normalize_inputs: bool = False, window_size: int = 0, per_core_batch_size: int = 1, use_continuous_quantile_head: bool = False, force_flip_invariance: bool = True, infer_is_positive: bool = True, fix_quantile_crossing: bool = False, return_backcast: bool = False) -> None   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (*args, **kwargs) -> ~T   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, forecast_config: timesfm.configs.ForecastConfig, **kwargs) -> None   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, horizon: int, inputs: list[numpy.ndarray]) -> tuple[numpy.ndarray, numpy.ndarray]   \n",
      "4   (self, inputs: list[typing.Sequence[float]], dynamic_numerical_covariates: dict[str, typing.Sequence[typing.Sequence[float]]] | None = None, dynamic_categorical_covariates: dict[str, typing.Sequence[typing.Sequence[int | str]]] | None = None, static_numerical_covariates: dict[str, typing.Sequence[float]] | None = None, static_categorical_covariates: dict[str, typing.Sequence[int | str]] | None = None, xreg_mode: str = 'xreg + timesfm', normalize_xreg_target_per_input: bool = True, ridge: float = 0.0, max_rows_per_col: int = 0, force_on_cpu: bool = False)   \n",
      "5                                                                                                                                                                                                                           (pretrained_model_name_or_path: Union[str, pathlib.Path], *, force_download: bool = False, resume_download: Optional[bool] = None, proxies: Optional[Dict] = None, token: Union[bool, str, NoneType] = None, cache_dir: Union[str, pathlib.Path, NoneType] = None, local_files_only: bool = False, revision: Optional[str] = None, **model_kwargs) -> ~T   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, *args, **kwargs) -> huggingface_hub.repocard.ModelCard   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (self, path: str)   \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (*args, **kwargs)   \n",
      "9                                            (self, repo_id: str, *, config: Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType] = None, commit_message: str = 'Push model using huggingface_hub.', private: Optional[bool] = None, token: Optional[str] = None, branch: Optional[str] = None, create_pr: Optional[bool] = None, allow_patterns: Union[List[str], str, NoneType] = None, ignore_patterns: Union[List[str], str, NoneType] = None, delete_patterns: Union[List[str], str, NoneType] = None, model_card_kwargs: Optional[Dict[str, Any]] = None) -> str   \n",
      "10                                                                                                                                                                                                                                                                                       (self, save_directory: Union[str, pathlib.Path], *, config: Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType] = None, repo_id: Optional[str] = None, push_to_hub: bool = False, model_card_kwargs: Optional[Dict[str, Any]] = None, **push_to_hub_kwargs) -> Optional[str]   \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (*args, **kwds)   \n",
      "17                                                                                                                                                                                                                                                      (max_context: int = 0, max_horizon: int = 0, normalize_inputs: bool = False, window_size: int = 0, per_core_batch_size: int = 1, use_continuous_quantile_head: bool = False, force_flip_invariance: bool = True, infer_is_positive: bool = True, fix_quantile_crossing: bool = False, return_backcast: bool = False) -> None   \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (input_dims: int, output_dims: int, projection_stddev: float, use_bias: bool) -> None   \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                       (input_dims: int, hidden_dims: int, output_dims: int, use_bias: bool, activation: Literal['relu', 'swish', 'none']) -> None   \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (num_layers: int, transformer: timesfm.configs.TransformerConfig) -> None   \n",
      "21                                                                                                                                                                                                                                                                                             (model_dims: int, hidden_dims: int, num_heads: int, attention_norm: Literal['rms'], feedforward_norm: Literal['rms'], qk_norm: Literal['rms', 'none'], use_bias: bool, use_rotary_position_embeddings: bool, ff_activation: Literal['relu', 'swish', 'none'], fuse_qkv: bool) -> None   \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (config: timesfm.configs.RandomFourierFeaturesConfig)   \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (config: timesfm.configs.ResidualBlockConfig)   \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 doc  \\\n",
      "0     Options for forecasting.\\n\\nAttributes:\\n  max_context: The maximum context length. This is used by the complied decode\\n    function at inference time during batched inference. Any input time series\\n    with length less than max_context will be padded with zeros, and with\\n    length greater than max_context will be truncated.\\n  max_horizon: The maximum horizon length. This is used by the complied decode\\n    function at inference time during batched inference. The compiled cached\\n    decoding function will by default forecast till max_horizon.\\n  normalize_inputs: Whether to normalize the inputs. This is useful when the\\n    raw inputs are of extremely large or small magnitudes which may result in\\n    numerical issues.\\n  window_size: The window size for decomposed forecasting.\\n    TODO(siriuz42)   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        PyTorch implementation of TimesFM 2.5 with 200M parameters.   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Attempts to compile the model for fast decoding.\\n\\nSee configs.ForecastConfig for more details on the supported flags.\\n\\nArgs:\\n  forecast_config: Configuration for forecasting flags.\\n  **kwargs: Additional keyword arguments to pass to model.compile().   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Forecasts the time series.   \n",
      "4     Forecasts on a list of time series with covariates.\\n\\nTo optimize inference speed, avoid string valued categorical covariates.\\n\\nArgs:\\n  inputs: A list of time series forecast contexts. Each context time series\\n    should be in a format convertible to JTensor by `jnp.array`.\\n  dynamic_numerical_covariates: A dict of dynamic numerical covariates.\\n  dynamic_categorical_covariates: A dict of dynamic categorical covariates.\\n  static_numerical_covariates: A dict of static numerical covariates.\\n  static_categorical_covariates: A dict of static categorical covariates.\\n  xreg_mode: one of \"xreg + timesfm\" or \"timesfm + xreg\". \"xreg + timesfm\"\\n    fits a model on the residuals of the TimesFM forecast. \"timesfm + xreg\"\\n    fits a model on the targets then forecasts on the residuals via TimesFM.\\n  norm   \n",
      "5      Download a model from the Huggingface Hub and instantiate it.\\n\\nArgs:\\n    pretrained_model_name_or_path (`str`, `Path`):\\n        - Either the `model_id` (string) of a model hosted on the Hub, e.g. `bigscience/bloom`.\\n        - Or a path to a `directory` containing model weights saved using\\n            [`~transformers.PreTrainedModel.save_pretrained`], e.g., `../path/to/my_model_directory/`.\\n    revision (`str`, *optional*):\\n        Revision of the model on the Hub. Can be a branch name, a git tag or any commit id.\\n        Defaults to the latest commit on `main` branch.\\n    force_download (`bool`, *optional*, defaults to `False`):\\n        Whether to force (re-)downloading the model weights and configuration files from the Hub, overriding\\n        the existing cache.\\n    proxies (`Dict[str, st   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Loads a TimesFM model from a checkpoint.   \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  TimesFM 2.5 with 200M parameters.   \n",
      "9   Upload model checkpoint to the Hub.\\n\\nUse `allow_patterns` and `ignore_patterns` to precisely filter which files should be pushed to the hub. Use\\n`delete_patterns` to delete existing remote files in the same commit. See [`upload_folder`] reference for more\\ndetails.\\n\\nArgs:\\n    repo_id (`str`):\\n        ID of the repository to push to (example: `\"username/my-model\"`).\\n    config (`dict` or `DataclassInstance`, *optional*):\\n        Model configuration specified as a key/value dictionary or a dataclass instance.\\n    commit_message (`str`, *optional*):\\n        Message to commit while pushing.\\n    private (`bool`, *optional*):\\n        Whether the repository created should be private.\\n        If `None` (default), the repo will be public unless the organization's default is private.\\n    token (`str`   \n",
      "10   Save weights in local directory.\\n\\nArgs:\\n    save_directory (`str` or `Path`):\\n        Path to directory in which the model weights and configuration will be saved.\\n    config (`dict` or `DataclassInstance`, *optional*):\\n        Model configuration specified as a key/value dictionary or a dataclass instance.\\n    push_to_hub (`bool`, *optional*, defaults to `False`):\\n        Whether or not to push your model to the Huggingface Hub after saving it.\\n    repo_id (`str`, *optional*):\\n        ID of your repository on the Hub. Used only if `push_to_hub=True`. Will default to the folder name if\\n        not provided.\\n    model_card_kwargs (`Dict[str, Any]`, *optional*):\\n        Additional arguments passed to the model card template to customize the model card.\\n    push_to_hub_kwargs:\\n        Additio   \n",
      "16                                                                                 Special typing form to define literal types (a.k.a. value types).\\n\\nThis form can be used to indicate to type checkers that the corresponding\\nvariable or function parameter has a value equivalent to the provided\\nliteral (or one of several literals)::\\n\\n    def validate_simple(data: Any) -> Literal[True]:  # always returns True\\n        ...\\n\\n    MODE = Literal['r', 'rb', 'w', 'wb']\\n    def open_helper(file: str, mode: MODE) -> str:\\n        ...\\n\\n    open_helper('/some/path', 'r')  # Passes type check\\n    open_helper('/other/path', 'typo')  # Error in type checker\\n\\nLiteral[...] cannot be subclassed. At runtime, an arbitrary value\\nis allowed as type argument to Literal[...], but type checkers may\\nimpose restrictions.   \n",
      "17    Options for forecasting.\\n\\nAttributes:\\n  max_context: The maximum context length. This is used by the complied decode\\n    function at inference time during batched inference. Any input time series\\n    with length less than max_context will be padded with zeros, and with\\n    length greater than max_context will be truncated.\\n  max_horizon: The maximum horizon length. This is used by the complied decode\\n    function at inference time during batched inference. The compiled cached\\n    decoding function will by default forecast till max_horizon.\\n  normalize_inputs: Whether to normalize the inputs. This is useful when the\\n    raw inputs are of extremely large or small magnitudes which may result in\\n    numerical issues.\\n  window_size: The window size for decomposed forecasting.\\n    TODO(siriuz42)   \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Framework-agnostic config for random fourier features.   \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Framework-agnostic config for a residual block.   \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Framework-agnostic config for a stacked transformers.   \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Framework-agnostic config for a transformer.   \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Random Fourier features layer.   \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Residual block with two linear layers and a linear residual connection.   \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
      "\n",
      "                                                                                          file  \n",
      "0      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "1      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "2      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "3      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "4      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "5      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "6      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "7      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "8      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "9      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "10     c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
      "16      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
      "17      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
      "18      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
      "19      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
      "20      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
      "21      c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
      "31  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
      "32  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
      "33  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
      "\n",
      "Saved catalog to: C:\\model_info\\timesfm_api_catalog.csv\n",
      "Rows: 483\n"
     ]
    }
   ],
   "source": [
    "# /absolute/path は環境ごとに異なるので、出力は Path.resolve() で「フルパス化」します。\n",
    "from __future__ import annotations\n",
    "\n",
    "import importlib\n",
    "import inspect\n",
    "import pkgutil\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import timesfm\n",
    "\n",
    "\n",
    "def _safe_signature(obj: Any) -> Optional[str]:\n",
    "    \"\"\"関数/クラス等のシグネチャ(signature=引数の形)を安全に文字列化\"\"\"\n",
    "    try:\n",
    "        return str(inspect.signature(obj))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _safe_doc(obj: Any, max_chars: int = 800) -> Optional[str]:\n",
    "    \"\"\"docstring(説明文)を安全に取得して、長すぎる場合は切る\"\"\"\n",
    "    try:\n",
    "        doc = inspect.getdoc(obj) or \"\"\n",
    "        doc = doc.strip()\n",
    "        if not doc:\n",
    "            return None\n",
    "        return doc[:max_chars]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _iter_public_members(module: Any):\n",
    "    \"\"\"モジュールの公開メンバー（先頭が_でない）を列挙\"\"\"\n",
    "    for name in dir(module):\n",
    "        if name.startswith(\"_\"):\n",
    "            continue\n",
    "        try:\n",
    "            yield name, getattr(module, name)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "\n",
    "def collect_timesfm_api_catalog(\n",
    "    include_methods: bool = True,\n",
    "    max_doc_chars: int = 800,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    timesfm配下のサブモジュールを探索し、\n",
    "    公開関数/クラス/メソッド等を DataFrame にまとめる。\n",
    "\n",
    "    include_methods=True で class の公開メソッドも展開。\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    pkg_name = timesfm.__name__\n",
    "    pkg_paths = list(getattr(timesfm, \"__path__\", []))  # パッケージの探索パス\n",
    "\n",
    "    # 1) まずルートモジュール自身\n",
    "    modules: List[tuple[str, Any]] = [(pkg_name, timesfm)]\n",
    "\n",
    "    # 2) サブモジュール発見 → import（失敗も記録）\n",
    "    for m in pkgutil.walk_packages(pkg_paths, prefix=pkg_name + \".\"):\n",
    "        modname = m.name\n",
    "        try:\n",
    "            mod = importlib.import_module(modname)\n",
    "            modules.append((modname, mod))\n",
    "        except Exception as e:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"module\": modname,\n",
    "                    \"name\": None,\n",
    "                    \"kind\": \"import_error\",\n",
    "                    \"signature\": None,\n",
    "                    \"doc\": str(e),\n",
    "                    \"file\": None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # 3) 各モジュールから公開メンバーを収集\n",
    "    for modname, mod in modules:\n",
    "        mod_file = getattr(mod, \"__file__\", None)\n",
    "\n",
    "        for name, obj in _iter_public_members(mod):\n",
    "            # 種別判定\n",
    "            if inspect.isclass(obj):\n",
    "                kind = \"class\"\n",
    "            elif inspect.isfunction(obj) or inspect.isbuiltin(obj):\n",
    "                kind = \"function\"\n",
    "            elif inspect.ismodule(obj):\n",
    "                kind = \"module\"\n",
    "            elif callable(obj):\n",
    "                kind = \"callable\"\n",
    "            else:\n",
    "                kind = \"other\"\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"module\": modname,\n",
    "                    \"name\": name,\n",
    "                    \"kind\": kind,\n",
    "                    \"signature\": _safe_signature(obj) if kind in {\"class\", \"function\", \"callable\"} else None,\n",
    "                    \"doc\": _safe_doc(obj, max_chars=max_doc_chars),\n",
    "                    \"file\": mod_file,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # 4) クラスなら公開メソッドも展開（多くなるのでスイッチ式）\n",
    "            if include_methods and inspect.isclass(obj):\n",
    "                try:\n",
    "                    for meth_name, meth_obj in inspect.getmembers(obj):\n",
    "                        if meth_name.startswith(\"_\"):\n",
    "                            continue\n",
    "                        if not callable(meth_obj):\n",
    "                            continue\n",
    "\n",
    "                        rows.append(\n",
    "                            {\n",
    "                                \"module\": modname,\n",
    "                                \"name\": f\"{name}.{meth_name}\",\n",
    "                                \"kind\": \"method\",\n",
    "                                \"signature\": _safe_signature(meth_obj),\n",
    "                                \"doc\": _safe_doc(meth_obj, max_chars=max_doc_chars),\n",
    "                                \"file\": mod_file,\n",
    "                            }\n",
    "                        )\n",
    "                except Exception:\n",
    "                    # クラスによっては getmembers が不安定なことがあるので握りつぶす\n",
    "                    pass\n",
    "\n",
    "    # 5) DataFrame化・整形\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.drop_duplicates(subset=[\"module\", \"name\", \"kind\"], keep=\"first\")\n",
    "    df = df.sort_values([\"module\", \"kind\", \"name\"], na_position=\"last\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = collect_timesfm_api_catalog(include_methods=True, max_doc_chars=800)\n",
    "\n",
    "    # どんな「機能」があるかを見る例（関数・クラスだけ抽出）\n",
    "    api_df = df[df[\"kind\"].isin([\"function\", \"class\", \"callable\", \"method\"])].copy()\n",
    "\n",
    "    # CSVに吐く（フルパスで保存先を表示）\n",
    "    out_path = (Path.cwd() / \"timesfm_api_catalog.csv\").resolve()\n",
    "    api_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(api_df.head(20))\n",
    "    print(f\"\\nSaved catalog to: {out_path}\\nRows: {len(api_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933f137c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>module</th>\n",
       "      <th>name</th>\n",
       "      <th>kind</th>\n",
       "      <th>signature</th>\n",
       "      <th>doc</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>class</td>\n",
       "      <td>(max_context: int = 0, max_horizon: int = 0, normalize_inputs: bool = False, window_size: int = 0, per_core_batch_size: int = 1, use_continuous_quantile_head: bool = False, force_flip_invariance: bool = True, infer_is_positive: bool = True, fix_quantile_crossing: bool = False, return_backcast: bool = False) -&gt; None</td>\n",
       "      <td>Options for forecasting.\\n\\nAttributes:\\n  max_context: The maximum context length. This is used by the complied decode\\n    function at inference time during batched inference. Any input time series\\n    with length less than max_context will be padded with zeros, and with\\n    length greater than max_context will be truncated.\\n  max_horizon: The maximum horizon length. This is used by the complied decode\\n    function at inference time during batched inference. The compiled cached\\n    decoding function will by default forecast till max_horizon.\\n  normalize_inputs: Whether to normalize the inputs. This is useful when the\\n    raw inputs are of extremely large or small magnitudes which may result in\\n    numerical issues.\\n  window_size: The window size for decomposed forecasting.\\n    TODO(siriuz42)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch</td>\n",
       "      <td>class</td>\n",
       "      <td>(*args, **kwargs) -&gt; ~T</td>\n",
       "      <td>PyTorch implementation of TimesFM 2.5 with 200M parameters.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, forecast_config: timesfm.configs.ForecastConfig, **kwargs) -&gt; None</td>\n",
       "      <td>Attempts to compile the model for fast decoding.\\n\\nSee configs.ForecastConfig for more details on the supported flags.\\n\\nArgs:\\n  forecast_config: Configuration for forecasting flags.\\n  **kwargs: Additional keyword arguments to pass to model.compile().</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch.forecast</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, horizon: int, inputs: list[numpy.ndarray]) -&gt; tuple[numpy.ndarray, numpy.ndarray]</td>\n",
       "      <td>Forecasts the time series.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch.forecast_with_covariates</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, inputs: list[typing.Sequence[float]], dynamic_numerical_covariates: dict[str, typing.Sequence[typing.Sequence[float]]] | None = None, dynamic_categorical_covariates: dict[str, typing.Sequence[typing.Sequence[int | str]]] | None = None, static_numerical_covariates: dict[str, typing.Sequence[float]] | None = None, static_categorical_covariates: dict[str, typing.Sequence[int | str]] | None = None, xreg_mode: str = 'xreg + timesfm', normalize_xreg_target_per_input: bool = True, ridge: float = 0.0, max_rows_per_col: int = 0, force_on_cpu: bool = False)</td>\n",
       "      <td>Forecasts on a list of time series with covariates.\\n\\nTo optimize inference speed, avoid string valued categorical covariates.\\n\\nArgs:\\n  inputs: A list of time series forecast contexts. Each context time series\\n    should be in a format convertible to JTensor by `jnp.array`.\\n  dynamic_numerical_covariates: A dict of dynamic numerical covariates.\\n  dynamic_categorical_covariates: A dict of dynamic categorical covariates.\\n  static_numerical_covariates: A dict of static numerical covariates.\\n  static_categorical_covariates: A dict of static categorical covariates.\\n  xreg_mode: one of \"xreg + timesfm\" or \"timesfm + xreg\". \"xreg + timesfm\"\\n    fits a model on the residuals of the TimesFM forecast. \"timesfm + xreg\"\\n    fits a model on the targets then forecasts on the residuals via TimesFM.\\n  norm</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch.from_pretrained</td>\n",
       "      <td>method</td>\n",
       "      <td>(pretrained_model_name_or_path: Union[str, pathlib.Path], *, force_download: bool = False, resume_download: Optional[bool] = None, proxies: Optional[Dict] = None, token: Union[bool, str, NoneType] = None, cache_dir: Union[str, pathlib.Path, NoneType] = None, local_files_only: bool = False, revision: Optional[str] = None, **model_kwargs) -&gt; ~T</td>\n",
       "      <td>Download a model from the Huggingface Hub and instantiate it.\\n\\nArgs:\\n    pretrained_model_name_or_path (`str`, `Path`):\\n        - Either the `model_id` (string) of a model hosted on the Hub, e.g. `bigscience/bloom`.\\n        - Or a path to a `directory` containing model weights saved using\\n            [`~transformers.PreTrainedModel.save_pretrained`], e.g., `../path/to/my_model_directory/`.\\n    revision (`str`, *optional*):\\n        Revision of the model on the Hub. Can be a branch name, a git tag or any commit id.\\n        Defaults to the latest commit on `main` branch.\\n    force_download (`bool`, *optional*, defaults to `False`):\\n        Whether to force (re-)downloading the model weights and configuration files from the Hub, overriding\\n        the existing cache.\\n    proxies (`Dict[str, st</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch.generate_model_card</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; huggingface_hub.repocard.ModelCard</td>\n",
       "      <td>None</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch.load_checkpoint</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, path: str)</td>\n",
       "      <td>Loads a TimesFM model from a checkpoint.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch.model</td>\n",
       "      <td>method</td>\n",
       "      <td>(*args, **kwargs)</td>\n",
       "      <td>TimesFM 2.5 with 200M parameters.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch.push_to_hub</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, repo_id: str, *, config: Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType] = None, commit_message: str = 'Push model using huggingface_hub.', private: Optional[bool] = None, token: Optional[str] = None, branch: Optional[str] = None, create_pr: Optional[bool] = None, allow_patterns: Union[List[str], str, NoneType] = None, ignore_patterns: Union[List[str], str, NoneType] = None, delete_patterns: Union[List[str], str, NoneType] = None, model_card_kwargs: Optional[Dict[str, Any]] = None) -&gt; str</td>\n",
       "      <td>Upload model checkpoint to the Hub.\\n\\nUse `allow_patterns` and `ignore_patterns` to precisely filter which files should be pushed to the hub. Use\\n`delete_patterns` to delete existing remote files in the same commit. See [`upload_folder`] reference for more\\ndetails.\\n\\nArgs:\\n    repo_id (`str`):\\n        ID of the repository to push to (example: `\"username/my-model\"`).\\n    config (`dict` or `DataclassInstance`, *optional*):\\n        Model configuration specified as a key/value dictionary or a dataclass instance.\\n    commit_message (`str`, *optional*):\\n        Message to commit while pushing.\\n    private (`bool`, *optional*):\\n        Whether the repository created should be private.\\n        If `None` (default), the repo will be public unless the organization's default is private.\\n    token (`str`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>timesfm</td>\n",
       "      <td>TimesFM_2p5_200M_torch.save_pretrained</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, save_directory: Union[str, pathlib.Path], *, config: Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType] = None, repo_id: Optional[str] = None, push_to_hub: bool = False, model_card_kwargs: Optional[Dict[str, Any]] = None, **push_to_hub_kwargs) -&gt; Optional[str]</td>\n",
       "      <td>Save weights in local directory.\\n\\nArgs:\\n    save_directory (`str` or `Path`):\\n        Path to directory in which the model weights and configuration will be saved.\\n    config (`dict` or `DataclassInstance`, *optional*):\\n        Model configuration specified as a key/value dictionary or a dataclass instance.\\n    push_to_hub (`bool`, *optional*, defaults to `False`):\\n        Whether or not to push your model to the Huggingface Hub after saving it.\\n    repo_id (`str`, *optional*):\\n        ID of your repository on the Hub. Used only if `push_to_hub=True`. Will default to the folder name if\\n        not provided.\\n    model_card_kwargs (`Dict[str, Any]`, *optional*):\\n        Additional arguments passed to the model card template to customize the model card.\\n    push_to_hub_kwargs:\\n        Additio</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>timesfm.configs</td>\n",
       "      <td>Literal</td>\n",
       "      <td>callable</td>\n",
       "      <td>(*args, **kwds)</td>\n",
       "      <td>Special typing form to define literal types (a.k.a. value types).\\n\\nThis form can be used to indicate to type checkers that the corresponding\\nvariable or function parameter has a value equivalent to the provided\\nliteral (or one of several literals)::\\n\\n    def validate_simple(data: Any) -&gt; Literal[True]:  # always returns True\\n        ...\\n\\n    MODE = Literal['r', 'rb', 'w', 'wb']\\n    def open_helper(file: str, mode: MODE) -&gt; str:\\n        ...\\n\\n    open_helper('/some/path', 'r')  # Passes type check\\n    open_helper('/other/path', 'typo')  # Error in type checker\\n\\nLiteral[...] cannot be subclassed. At runtime, an arbitrary value\\nis allowed as type argument to Literal[...], but type checkers may\\nimpose restrictions.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>timesfm.configs</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>class</td>\n",
       "      <td>(max_context: int = 0, max_horizon: int = 0, normalize_inputs: bool = False, window_size: int = 0, per_core_batch_size: int = 1, use_continuous_quantile_head: bool = False, force_flip_invariance: bool = True, infer_is_positive: bool = True, fix_quantile_crossing: bool = False, return_backcast: bool = False) -&gt; None</td>\n",
       "      <td>Options for forecasting.\\n\\nAttributes:\\n  max_context: The maximum context length. This is used by the complied decode\\n    function at inference time during batched inference. Any input time series\\n    with length less than max_context will be padded with zeros, and with\\n    length greater than max_context will be truncated.\\n  max_horizon: The maximum horizon length. This is used by the complied decode\\n    function at inference time during batched inference. The compiled cached\\n    decoding function will by default forecast till max_horizon.\\n  normalize_inputs: Whether to normalize the inputs. This is useful when the\\n    raw inputs are of extremely large or small magnitudes which may result in\\n    numerical issues.\\n  window_size: The window size for decomposed forecasting.\\n    TODO(siriuz42)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>timesfm.configs</td>\n",
       "      <td>RandomFourierFeaturesConfig</td>\n",
       "      <td>class</td>\n",
       "      <td>(input_dims: int, output_dims: int, projection_stddev: float, use_bias: bool) -&gt; None</td>\n",
       "      <td>Framework-agnostic config for random fourier features.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>timesfm.configs</td>\n",
       "      <td>ResidualBlockConfig</td>\n",
       "      <td>class</td>\n",
       "      <td>(input_dims: int, hidden_dims: int, output_dims: int, use_bias: bool, activation: Literal['relu', 'swish', 'none']) -&gt; None</td>\n",
       "      <td>Framework-agnostic config for a residual block.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>timesfm.configs</td>\n",
       "      <td>StackedTransformersConfig</td>\n",
       "      <td>class</td>\n",
       "      <td>(num_layers: int, transformer: timesfm.configs.TransformerConfig) -&gt; None</td>\n",
       "      <td>Framework-agnostic config for a stacked transformers.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>timesfm.configs</td>\n",
       "      <td>TransformerConfig</td>\n",
       "      <td>class</td>\n",
       "      <td>(model_dims: int, hidden_dims: int, num_heads: int, attention_norm: Literal['rms'], feedforward_norm: Literal['rms'], qk_norm: Literal['rms', 'none'], use_bias: bool, use_rotary_position_embeddings: bool, ff_activation: Literal['relu', 'swish', 'none'], fuse_qkv: bool) -&gt; None</td>\n",
       "      <td>Framework-agnostic config for a transformer.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures</td>\n",
       "      <td>class</td>\n",
       "      <td>(config: timesfm.configs.RandomFourierFeaturesConfig)</td>\n",
       "      <td>Random Fourier features layer.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock</td>\n",
       "      <td>class</td>\n",
       "      <td>(config: timesfm.configs.ResidualBlockConfig)</td>\n",
       "      <td>Residual block with two linear layers and a linear residual connection.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.add_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.apply</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</td>\n",
       "      <td>Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -&gt; None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    &gt;&gt;&gt; @torch.no_grad()\\n    &gt;&gt;&gt; def init_weights(m):\\n    &gt;&gt;&gt;     print(m)\\n    &gt;&gt;&gt;     if type(m) is nn.Linear:\\n    &gt;&gt;&gt;         m.weight.fill_(1.0)\\n    &gt;&gt;&gt;         print(m.weight)\\n    &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    &gt;&gt;&gt; net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.bfloat16</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</td>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for buf in model.buffers():\\n    &gt;&gt;&gt;     print(type(buf), buf.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; None</td>\n",
       "      <td>Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.cpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.cuda</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.double</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.eval</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.extra_repr</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; str</td>\n",
       "      <td>Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.float</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.forward</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, x: torch.Tensor) -&gt; torch.Tensor</td>\n",
       "      <td>Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.get_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Tensor'</td>\n",
       "      <td>Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.get_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Any</td>\n",
       "      <td>Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.get_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Parameter'</td>\n",
       "      <td>Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.get_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Module'</td>\n",
       "      <td>Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.half</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.ipu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.load_state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</td>\n",
       "      <td>Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.mtia</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.named_buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</td>\n",
       "      <td>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, buf in self.named_buffers():\\n    &gt;&gt;&gt;     if name in ['running_var']:\\n    &gt;&gt;&gt;         print(buf.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.named_children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</td>\n",
       "      <td>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, module in model.named_children():\\n    &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\\n    &gt;&gt;&gt;         print(module)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.named_modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</td>\n",
       "      <td>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.named_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</td>\n",
       "      <td>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, param in self.named_parameters():\\n    &gt;&gt;&gt;     if name in ['bias']:\\n    &gt;&gt;&gt;         print(param.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</td>\n",
       "      <td>Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for param in model.parameters():\\n    &gt;&gt;&gt;     print(type(param), param.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</td>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_forward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -&gt; None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_forward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -&gt; None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_full_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_full_backward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -&gt; tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_load_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -&gt; None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_load_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</td>\n",
       "      <td>Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -&gt; None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.register_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -&gt; None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.requires_grad_</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, requires_grad: bool = True) -&gt; Self</td>\n",
       "      <td>Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.set_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state: Any) -&gt; None</td>\n",
       "      <td>Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.set_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</td>\n",
       "      <td>Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.share_memory</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, destination=None, prefix='', keep_vars=False)</td>\n",
       "      <td>Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.to</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs)</td>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.to_empty</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -&gt; Self</td>\n",
       "      <td>Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.train</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, mode: bool = True) -&gt; Self</td>\n",
       "      <td>Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.type</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, dst_type: torch.dtype | str) -&gt; Self</td>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.xpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>RandomFourierFeatures.zero_grad</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, set_to_none: bool = True) -&gt; None</td>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.add_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.apply</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</td>\n",
       "      <td>Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -&gt; None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    &gt;&gt;&gt; @torch.no_grad()\\n    &gt;&gt;&gt; def init_weights(m):\\n    &gt;&gt;&gt;     print(m)\\n    &gt;&gt;&gt;     if type(m) is nn.Linear:\\n    &gt;&gt;&gt;         m.weight.fill_(1.0)\\n    &gt;&gt;&gt;         print(m.weight)\\n    &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    &gt;&gt;&gt; net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.bfloat16</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</td>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for buf in model.buffers():\\n    &gt;&gt;&gt;     print(type(buf), buf.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; None</td>\n",
       "      <td>Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.cpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.cuda</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.double</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.eval</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.extra_repr</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; str</td>\n",
       "      <td>Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.float</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.forward</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, x: torch.Tensor) -&gt; torch.Tensor</td>\n",
       "      <td>Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.get_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Tensor'</td>\n",
       "      <td>Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.get_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Any</td>\n",
       "      <td>Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.get_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Parameter'</td>\n",
       "      <td>Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.get_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Module'</td>\n",
       "      <td>Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.half</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.ipu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.load_state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</td>\n",
       "      <td>Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.mtia</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.named_buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</td>\n",
       "      <td>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, buf in self.named_buffers():\\n    &gt;&gt;&gt;     if name in ['running_var']:\\n    &gt;&gt;&gt;         print(buf.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.named_children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</td>\n",
       "      <td>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, module in model.named_children():\\n    &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\\n    &gt;&gt;&gt;         print(module)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.named_modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</td>\n",
       "      <td>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.named_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</td>\n",
       "      <td>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, param in self.named_parameters():\\n    &gt;&gt;&gt;     if name in ['bias']:\\n    &gt;&gt;&gt;         print(param.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</td>\n",
       "      <td>Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for param in model.parameters():\\n    &gt;&gt;&gt;     print(type(param), param.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</td>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_forward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -&gt; None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_forward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -&gt; None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_full_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_full_backward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -&gt; tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_load_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -&gt; None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_load_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</td>\n",
       "      <td>Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -&gt; None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.register_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -&gt; None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.requires_grad_</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, requires_grad: bool = True) -&gt; Self</td>\n",
       "      <td>Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.set_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state: Any) -&gt; None</td>\n",
       "      <td>Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.set_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</td>\n",
       "      <td>Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.share_memory</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, destination=None, prefix='', keep_vars=False)</td>\n",
       "      <td>Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.to</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs)</td>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.to_empty</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -&gt; Self</td>\n",
       "      <td>Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.train</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, mode: bool = True) -&gt; Self</td>\n",
       "      <td>Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.type</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, dst_type: torch.dtype | str) -&gt; Self</td>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.xpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>timesfm.torch.dense</td>\n",
       "      <td>ResidualBlock.zero_grad</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, set_to_none: bool = True) -&gt; None</td>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm</td>\n",
       "      <td>class</td>\n",
       "      <td>(num_features: int, *, epsilon: float = 1e-06)</td>\n",
       "      <td>RMS normalization.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.add_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.apply</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</td>\n",
       "      <td>Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -&gt; None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    &gt;&gt;&gt; @torch.no_grad()\\n    &gt;&gt;&gt; def init_weights(m):\\n    &gt;&gt;&gt;     print(m)\\n    &gt;&gt;&gt;     if type(m) is nn.Linear:\\n    &gt;&gt;&gt;         m.weight.fill_(1.0)\\n    &gt;&gt;&gt;         print(m.weight)\\n    &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    &gt;&gt;&gt; net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.bfloat16</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</td>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for buf in model.buffers():\\n    &gt;&gt;&gt;     print(type(buf), buf.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; None</td>\n",
       "      <td>Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.cpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.cuda</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.double</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.eval</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.extra_repr</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; str</td>\n",
       "      <td>Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.float</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.forward</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, inputs: torch.Tensor) -&gt; torch.Tensor</td>\n",
       "      <td>Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.get_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Tensor'</td>\n",
       "      <td>Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.get_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Any</td>\n",
       "      <td>Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.get_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Parameter'</td>\n",
       "      <td>Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.get_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Module'</td>\n",
       "      <td>Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.half</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.ipu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.load_state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</td>\n",
       "      <td>Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.mtia</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.named_buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</td>\n",
       "      <td>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, buf in self.named_buffers():\\n    &gt;&gt;&gt;     if name in ['running_var']:\\n    &gt;&gt;&gt;         print(buf.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.named_children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</td>\n",
       "      <td>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, module in model.named_children():\\n    &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\\n    &gt;&gt;&gt;         print(module)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.named_modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</td>\n",
       "      <td>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.named_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</td>\n",
       "      <td>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, param in self.named_parameters():\\n    &gt;&gt;&gt;     if name in ['bias']:\\n    &gt;&gt;&gt;         print(param.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</td>\n",
       "      <td>Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for param in model.parameters():\\n    &gt;&gt;&gt;     print(type(param), param.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</td>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_forward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -&gt; None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_forward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -&gt; None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_full_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_full_backward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -&gt; tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_load_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -&gt; None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_load_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</td>\n",
       "      <td>Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -&gt; None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.register_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -&gt; None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.requires_grad_</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, requires_grad: bool = True) -&gt; Self</td>\n",
       "      <td>Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.set_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state: Any) -&gt; None</td>\n",
       "      <td>Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.set_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</td>\n",
       "      <td>Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.share_memory</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, destination=None, prefix='', keep_vars=False)</td>\n",
       "      <td>Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.to</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs)</td>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.to_empty</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -&gt; Self</td>\n",
       "      <td>Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.train</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, mode: bool = True) -&gt; Self</td>\n",
       "      <td>Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.type</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, dst_type: torch.dtype | str) -&gt; Self</td>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.xpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>timesfm.torch.normalization</td>\n",
       "      <td>RMSNorm.zero_grad</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, set_to_none: bool = True) -&gt; None</td>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Callable</td>\n",
       "      <td>callable</td>\n",
       "      <td>(*args, **kwargs)</td>\n",
       "      <td>Deprecated alias to collections.abc.Callable.\\n\\nCallable[[int], str] signifies a function that takes a single\\nparameter of type int and returns a str.\\n\\nThe subscription syntax must always be used with exactly two\\nvalues: the argument list and the return type.\\nThe argument list must be a list of types, a ParamSpec,\\nConcatenate or ellipsis. The return type must be a single type.\\n\\nThere is no syntax to indicate optional or keyword arguments;\\nsuch function types are rarely used as callback types.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>DecodeCache</td>\n",
       "      <td>class</td>\n",
       "      <td>(next_index: torch.Tensor, num_masked: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -&gt; None</td>\n",
       "      <td>Cache for decoding.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>class</td>\n",
       "      <td>(normalized_shape: Union[int, list[int], torch.Size], eps: float = 1e-05, elementwise_affine: bool = True, bias: bool = True, device=None, dtype=None) -&gt; None</td>\n",
       "      <td>Applies Layer Normalization over a mini-batch of inputs.\\n\\nThis layer implements the operation as described in\\nthe paper `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`__\\n\\n.. math::\\n    y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\n\\nThe mean and standard-deviation are calculated over the last `D` dimensions, where `D`\\nis the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape`\\nis ``(3, 5)`` (a 2-dimensional shape), the mean and standard-deviation are computed over\\nthe last 2 dimensions of the input (i.e. ``input.mean((-2, -1))``).\\n:math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of\\n:attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\\nThe variance is calculated via the biased esti</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention</td>\n",
       "      <td>class</td>\n",
       "      <td>(num_heads: int, in_features: int, *, use_per_dim_scale: bool = True, use_rotary_position_embeddings: bool = True, use_bias: bool = False, attention_fn: Callable[..., torch.Tensor] = &lt;function _torch_dot_product_attention at 0x00000212F95A87C0&gt;, qk_norm: str = 'rms', fuse_qkv: bool = False)</td>\n",
       "      <td>Multi-head attention.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale</td>\n",
       "      <td>class</td>\n",
       "      <td>(num_dims: int)</td>\n",
       "      <td>Per-dimension scaling.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm</td>\n",
       "      <td>class</td>\n",
       "      <td>(num_features: int, *, epsilon: float = 1e-06)</td>\n",
       "      <td>RMS normalization.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding</td>\n",
       "      <td>class</td>\n",
       "      <td>(embedding_dims: int, min_timescale: float = 1.0, max_timescale: float = 10000.0)</td>\n",
       "      <td>Rotary positional embedding.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>class</td>\n",
       "      <td>(config: timesfm.configs.TransformerConfig)</td>\n",
       "      <td>Classic Transformer used in TimesFM.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>make_attn_mask</td>\n",
       "      <td>function</td>\n",
       "      <td>(query_length: int, num_all_masked_kv: torch.Tensor, query_index_offset: torch.Tensor | None = None, kv_length: int = 0) -&gt; torch.Tensor</td>\n",
       "      <td>Makes attention mask.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.add_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.apply</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</td>\n",
       "      <td>Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -&gt; None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    &gt;&gt;&gt; @torch.no_grad()\\n    &gt;&gt;&gt; def init_weights(m):\\n    &gt;&gt;&gt;     print(m)\\n    &gt;&gt;&gt;     if type(m) is nn.Linear:\\n    &gt;&gt;&gt;         m.weight.fill_(1.0)\\n    &gt;&gt;&gt;         print(m.weight)\\n    &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    &gt;&gt;&gt; net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.bfloat16</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</td>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for buf in model.buffers():\\n    &gt;&gt;&gt;     print(type(buf), buf.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; None</td>\n",
       "      <td>Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.cpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.cuda</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.double</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.eval</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.extra_repr</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; str</td>\n",
       "      <td>Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.float</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.forward</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, input: torch.Tensor) -&gt; torch.Tensor</td>\n",
       "      <td>Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.get_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Tensor'</td>\n",
       "      <td>Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.get_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Any</td>\n",
       "      <td>Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.get_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Parameter'</td>\n",
       "      <td>Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.get_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Module'</td>\n",
       "      <td>Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.half</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.ipu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.load_state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</td>\n",
       "      <td>Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.mtia</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.named_buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</td>\n",
       "      <td>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, buf in self.named_buffers():\\n    &gt;&gt;&gt;     if name in ['running_var']:\\n    &gt;&gt;&gt;         print(buf.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.named_children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</td>\n",
       "      <td>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, module in model.named_children():\\n    &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\\n    &gt;&gt;&gt;         print(module)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.named_modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</td>\n",
       "      <td>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.named_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</td>\n",
       "      <td>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, param in self.named_parameters():\\n    &gt;&gt;&gt;     if name in ['bias']:\\n    &gt;&gt;&gt;         print(param.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</td>\n",
       "      <td>Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for param in model.parameters():\\n    &gt;&gt;&gt;     print(type(param), param.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</td>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_forward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -&gt; None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_forward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -&gt; None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_full_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_full_backward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -&gt; tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_load_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -&gt; None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_load_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</td>\n",
       "      <td>Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -&gt; None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.register_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -&gt; None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.requires_grad_</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, requires_grad: bool = True) -&gt; Self</td>\n",
       "      <td>Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.reset_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; None</td>\n",
       "      <td>None</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.set_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state: Any) -&gt; None</td>\n",
       "      <td>Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.set_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</td>\n",
       "      <td>Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.share_memory</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, destination=None, prefix='', keep_vars=False)</td>\n",
       "      <td>Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.to</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs)</td>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.to_empty</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -&gt; Self</td>\n",
       "      <td>Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.train</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, mode: bool = True) -&gt; Self</td>\n",
       "      <td>Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.type</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, dst_type: torch.dtype | str) -&gt; Self</td>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.xpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>LayerNorm.zero_grad</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, set_to_none: bool = True) -&gt; None</td>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.add_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.apply</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</td>\n",
       "      <td>Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -&gt; None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    &gt;&gt;&gt; @torch.no_grad()\\n    &gt;&gt;&gt; def init_weights(m):\\n    &gt;&gt;&gt;     print(m)\\n    &gt;&gt;&gt;     if type(m) is nn.Linear:\\n    &gt;&gt;&gt;         m.weight.fill_(1.0)\\n    &gt;&gt;&gt;         print(m.weight)\\n    &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    &gt;&gt;&gt; net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.bfloat16</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</td>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for buf in model.buffers():\\n    &gt;&gt;&gt;     print(type(buf), buf.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; None</td>\n",
       "      <td>Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.cpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.cuda</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.double</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.eval</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.extra_repr</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; str</td>\n",
       "      <td>Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.float</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.forward</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, inputs_q: torch.Tensor, *, decode_cache: timesfm.torch.util.DecodeCache | None = None, patch_mask: torch.Tensor | None = None) -&gt; tuple[torch.Tensor, timesfm.torch.util.DecodeCache | None]</td>\n",
       "      <td>Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.get_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Tensor'</td>\n",
       "      <td>Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.get_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Any</td>\n",
       "      <td>Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.get_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Parameter'</td>\n",
       "      <td>Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.get_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Module'</td>\n",
       "      <td>Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.half</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.ipu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.load_state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</td>\n",
       "      <td>Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.mtia</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.named_buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</td>\n",
       "      <td>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, buf in self.named_buffers():\\n    &gt;&gt;&gt;     if name in ['running_var']:\\n    &gt;&gt;&gt;         print(buf.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.named_children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</td>\n",
       "      <td>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, module in model.named_children():\\n    &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\\n    &gt;&gt;&gt;         print(module)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.named_modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</td>\n",
       "      <td>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.named_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</td>\n",
       "      <td>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, param in self.named_parameters():\\n    &gt;&gt;&gt;     if name in ['bias']:\\n    &gt;&gt;&gt;         print(param.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</td>\n",
       "      <td>Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for param in model.parameters():\\n    &gt;&gt;&gt;     print(type(param), param.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</td>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_forward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -&gt; None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_forward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -&gt; None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_full_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_full_backward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -&gt; tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_load_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -&gt; None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_load_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</td>\n",
       "      <td>Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -&gt; None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.register_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -&gt; None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.requires_grad_</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, requires_grad: bool = True) -&gt; Self</td>\n",
       "      <td>Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.set_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state: Any) -&gt; None</td>\n",
       "      <td>Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.set_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</td>\n",
       "      <td>Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.share_memory</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, destination=None, prefix='', keep_vars=False)</td>\n",
       "      <td>Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.to</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs)</td>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.to_empty</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -&gt; Self</td>\n",
       "      <td>Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.train</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, mode: bool = True) -&gt; Self</td>\n",
       "      <td>Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.type</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, dst_type: torch.dtype | str) -&gt; Self</td>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.xpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>MultiHeadAttention.zero_grad</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, set_to_none: bool = True) -&gt; None</td>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.add_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.apply</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</td>\n",
       "      <td>Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -&gt; None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    &gt;&gt;&gt; @torch.no_grad()\\n    &gt;&gt;&gt; def init_weights(m):\\n    &gt;&gt;&gt;     print(m)\\n    &gt;&gt;&gt;     if type(m) is nn.Linear:\\n    &gt;&gt;&gt;         m.weight.fill_(1.0)\\n    &gt;&gt;&gt;         print(m.weight)\\n    &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    &gt;&gt;&gt; net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.bfloat16</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</td>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for buf in model.buffers():\\n    &gt;&gt;&gt;     print(type(buf), buf.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; None</td>\n",
       "      <td>Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.cpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.cuda</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.double</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.eval</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.extra_repr</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; str</td>\n",
       "      <td>Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.float</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.forward</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, x: torch.Tensor) -&gt; torch.Tensor</td>\n",
       "      <td>Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.get_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Tensor'</td>\n",
       "      <td>Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.get_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Any</td>\n",
       "      <td>Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.get_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Parameter'</td>\n",
       "      <td>Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.get_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Module'</td>\n",
       "      <td>Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.half</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.ipu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.load_state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</td>\n",
       "      <td>Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.mtia</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.named_buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</td>\n",
       "      <td>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, buf in self.named_buffers():\\n    &gt;&gt;&gt;     if name in ['running_var']:\\n    &gt;&gt;&gt;         print(buf.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.named_children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</td>\n",
       "      <td>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, module in model.named_children():\\n    &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\\n    &gt;&gt;&gt;         print(module)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.named_modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</td>\n",
       "      <td>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.named_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</td>\n",
       "      <td>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, param in self.named_parameters():\\n    &gt;&gt;&gt;     if name in ['bias']:\\n    &gt;&gt;&gt;         print(param.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</td>\n",
       "      <td>Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for param in model.parameters():\\n    &gt;&gt;&gt;     print(type(param), param.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</td>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_forward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -&gt; None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_forward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -&gt; None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_full_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_full_backward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -&gt; tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_load_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -&gt; None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_load_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</td>\n",
       "      <td>Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -&gt; None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.register_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -&gt; None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.requires_grad_</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, requires_grad: bool = True) -&gt; Self</td>\n",
       "      <td>Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.set_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state: Any) -&gt; None</td>\n",
       "      <td>Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.set_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</td>\n",
       "      <td>Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.share_memory</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, destination=None, prefix='', keep_vars=False)</td>\n",
       "      <td>Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.to</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs)</td>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.to_empty</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -&gt; Self</td>\n",
       "      <td>Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.train</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, mode: bool = True) -&gt; Self</td>\n",
       "      <td>Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.type</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, dst_type: torch.dtype | str) -&gt; Self</td>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.xpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>PerDimScale.zero_grad</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, set_to_none: bool = True) -&gt; None</td>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.add_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.apply</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</td>\n",
       "      <td>Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -&gt; None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    &gt;&gt;&gt; @torch.no_grad()\\n    &gt;&gt;&gt; def init_weights(m):\\n    &gt;&gt;&gt;     print(m)\\n    &gt;&gt;&gt;     if type(m) is nn.Linear:\\n    &gt;&gt;&gt;         m.weight.fill_(1.0)\\n    &gt;&gt;&gt;         print(m.weight)\\n    &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    &gt;&gt;&gt; net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.bfloat16</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</td>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for buf in model.buffers():\\n    &gt;&gt;&gt;     print(type(buf), buf.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; None</td>\n",
       "      <td>Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.cpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.cuda</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.double</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.eval</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.extra_repr</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; str</td>\n",
       "      <td>Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.float</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.forward</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, inputs: torch.Tensor) -&gt; torch.Tensor</td>\n",
       "      <td>Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.get_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Tensor'</td>\n",
       "      <td>Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.get_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Any</td>\n",
       "      <td>Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.get_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Parameter'</td>\n",
       "      <td>Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.get_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Module'</td>\n",
       "      <td>Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.half</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.ipu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.load_state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</td>\n",
       "      <td>Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.mtia</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.named_buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</td>\n",
       "      <td>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, buf in self.named_buffers():\\n    &gt;&gt;&gt;     if name in ['running_var']:\\n    &gt;&gt;&gt;         print(buf.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.named_children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</td>\n",
       "      <td>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, module in model.named_children():\\n    &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\\n    &gt;&gt;&gt;         print(module)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.named_modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</td>\n",
       "      <td>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.named_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</td>\n",
       "      <td>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, param in self.named_parameters():\\n    &gt;&gt;&gt;     if name in ['bias']:\\n    &gt;&gt;&gt;         print(param.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</td>\n",
       "      <td>Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for param in model.parameters():\\n    &gt;&gt;&gt;     print(type(param), param.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</td>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_forward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -&gt; None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_forward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -&gt; None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_full_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_full_backward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -&gt; tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_load_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -&gt; None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_load_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</td>\n",
       "      <td>Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -&gt; None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.register_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -&gt; None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.requires_grad_</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, requires_grad: bool = True) -&gt; Self</td>\n",
       "      <td>Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.set_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state: Any) -&gt; None</td>\n",
       "      <td>Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.set_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</td>\n",
       "      <td>Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.share_memory</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, destination=None, prefix='', keep_vars=False)</td>\n",
       "      <td>Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.to</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs)</td>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.to_empty</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -&gt; Self</td>\n",
       "      <td>Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.train</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, mode: bool = True) -&gt; Self</td>\n",
       "      <td>Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.type</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, dst_type: torch.dtype | str) -&gt; Self</td>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.xpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RMSNorm.zero_grad</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, set_to_none: bool = True) -&gt; None</td>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.add_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.apply</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</td>\n",
       "      <td>Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -&gt; None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    &gt;&gt;&gt; @torch.no_grad()\\n    &gt;&gt;&gt; def init_weights(m):\\n    &gt;&gt;&gt;     print(m)\\n    &gt;&gt;&gt;     if type(m) is nn.Linear:\\n    &gt;&gt;&gt;         m.weight.fill_(1.0)\\n    &gt;&gt;&gt;         print(m.weight)\\n    &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    &gt;&gt;&gt; net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.bfloat16</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</td>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for buf in model.buffers():\\n    &gt;&gt;&gt;     print(type(buf), buf.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; None</td>\n",
       "      <td>Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.cpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.cuda</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.double</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.eval</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.extra_repr</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; str</td>\n",
       "      <td>Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.float</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.forward</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, inputs: torch.Tensor, position: torch.Tensor | None = None)</td>\n",
       "      <td>Generates a JTensor of sinusoids with different frequencies.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.get_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Tensor'</td>\n",
       "      <td>Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.get_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Any</td>\n",
       "      <td>Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.get_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Parameter'</td>\n",
       "      <td>Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.get_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Module'</td>\n",
       "      <td>Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.half</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.ipu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.load_state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</td>\n",
       "      <td>Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.mtia</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.named_buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</td>\n",
       "      <td>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, buf in self.named_buffers():\\n    &gt;&gt;&gt;     if name in ['running_var']:\\n    &gt;&gt;&gt;         print(buf.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.named_children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</td>\n",
       "      <td>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, module in model.named_children():\\n    &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\\n    &gt;&gt;&gt;         print(module)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.named_modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</td>\n",
       "      <td>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.named_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</td>\n",
       "      <td>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, param in self.named_parameters():\\n    &gt;&gt;&gt;     if name in ['bias']:\\n    &gt;&gt;&gt;         print(param.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</td>\n",
       "      <td>Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for param in model.parameters():\\n    &gt;&gt;&gt;     print(type(param), param.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</td>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_forward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -&gt; None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_forward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -&gt; None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_full_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_full_backward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -&gt; tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_load_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -&gt; None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_load_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</td>\n",
       "      <td>Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -&gt; None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.register_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -&gt; None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.requires_grad_</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, requires_grad: bool = True) -&gt; Self</td>\n",
       "      <td>Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.set_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state: Any) -&gt; None</td>\n",
       "      <td>Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.set_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</td>\n",
       "      <td>Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.share_memory</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, destination=None, prefix='', keep_vars=False)</td>\n",
       "      <td>Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.to</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs)</td>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.to_empty</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -&gt; Self</td>\n",
       "      <td>Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.train</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, mode: bool = True) -&gt; Self</td>\n",
       "      <td>Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.type</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, dst_type: torch.dtype | str) -&gt; Self</td>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.xpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>RotaryPositionalEmbedding.zero_grad</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, set_to_none: bool = True) -&gt; None</td>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.add_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.apply</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</td>\n",
       "      <td>Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -&gt; None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    &gt;&gt;&gt; @torch.no_grad()\\n    &gt;&gt;&gt; def init_weights(m):\\n    &gt;&gt;&gt;     print(m)\\n    &gt;&gt;&gt;     if type(m) is nn.Linear:\\n    &gt;&gt;&gt;         m.weight.fill_(1.0)\\n    &gt;&gt;&gt;         print(m.weight)\\n    &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    &gt;&gt;&gt; net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.bfloat16</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</td>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for buf in model.buffers():\\n    &gt;&gt;&gt;     print(type(buf), buf.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.compile</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs) -&gt; None</td>\n",
       "      <td>Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.cpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.cuda</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.double</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.eval</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.extra_repr</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; str</td>\n",
       "      <td>Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.float</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.forward</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, input_embeddings: torch.Tensor, patch_mask: torch.Tensor, decode_cache: timesfm.torch.util.DecodeCache | None = None) -&gt; tuple[torch.Tensor, timesfm.torch.util.DecodeCache | None]</td>\n",
       "      <td>Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.get_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Tensor'</td>\n",
       "      <td>Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.get_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Any</td>\n",
       "      <td>Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.get_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Parameter'</td>\n",
       "      <td>Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.get_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str) -&gt; 'Module'</td>\n",
       "      <td>Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.half</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.ipu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.load_state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</td>\n",
       "      <td>Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator['Module']</td>\n",
       "      <td>Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.mtia</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.named_buffers</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</td>\n",
       "      <td>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, buf in self.named_buffers():\\n    &gt;&gt;&gt;     if name in ['running_var']:\\n    &gt;&gt;&gt;         print(buf.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.named_children</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</td>\n",
       "      <td>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, module in model.named_children():\\n    &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\\n    &gt;&gt;&gt;         print(module)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.named_modules</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</td>\n",
       "      <td>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    &gt;&gt;&gt; l = nn.Linear(2, 2)\\n    &gt;&gt;&gt; net = nn.Sequential(l, l)\\n    &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '-&gt;', m)\\n\\n    0 -&gt; ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.named_parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</td>\n",
       "      <td>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for name, param in self.named_parameters():\\n    &gt;&gt;&gt;     if name in ['bias']:\\n    &gt;&gt;&gt;         print(param.size())</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.parameters</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</td>\n",
       "      <td>Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\\n    &gt;&gt;&gt; for param in model.parameters():\\n    &gt;&gt;&gt;     print(type(param), param.size())\\n    &lt;class 'torch.Tensor'&gt; (20L,)\\n    &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_buffer</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</td>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_forward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -&gt; None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_forward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -&gt; None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_full_backward_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_full_backward_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</td>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -&gt; tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_load_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -&gt; None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_load_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_module</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</td>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_parameter</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</td>\n",
       "      <td>Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_state_dict_post_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -&gt; None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.register_state_dict_pre_hook</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, hook)</td>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -&gt; None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.requires_grad_</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, requires_grad: bool = True) -&gt; Self</td>\n",
       "      <td>Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.set_extra_state</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, state: Any) -&gt; None</td>\n",
       "      <td>Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.set_submodule</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</td>\n",
       "      <td>Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.share_memory</td>\n",
       "      <td>method</td>\n",
       "      <td>(self) -&gt; Self</td>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.state_dict</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, destination=None, prefix='', keep_vars=False)</td>\n",
       "      <td>Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.to</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *args, **kwargs)</td>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.to_empty</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -&gt; Self</td>\n",
       "      <td>Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.train</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, mode: bool = True) -&gt; Self</td>\n",
       "      <td>Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.type</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, dst_type: torch.dtype | str) -&gt; Self</td>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.xpu</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, device: int | torch.device | None = None) -&gt; Self</td>\n",
       "      <td>Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>timesfm.torch.transformer</td>\n",
       "      <td>Transformer.zero_grad</td>\n",
       "      <td>method</td>\n",
       "      <td>(self, set_to_none: bool = True) -&gt; None</td>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>timesfm.torch.util</td>\n",
       "      <td>DecodeCache</td>\n",
       "      <td>class</td>\n",
       "      <td>(next_index: torch.Tensor, num_masked: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -&gt; None</td>\n",
       "      <td>Cache for decoding.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\util.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>timesfm.torch.util</td>\n",
       "      <td>revin</td>\n",
       "      <td>function</td>\n",
       "      <td>(x: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor, reverse: bool = False)</td>\n",
       "      <td>Reversible instance normalization.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\util.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>timesfm.torch.util</td>\n",
       "      <td>update_running_stats</td>\n",
       "      <td>function</td>\n",
       "      <td>(n: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor, x: torch.Tensor, mask: torch.Tensor) -&gt; tuple[tuple[torch.Tensor, torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor, torch.Tensor]]</td>\n",
       "      <td>Updates the running stats.</td>\n",
       "      <td>c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\util.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          module  \\\n",
       "0                        timesfm   \n",
       "1                        timesfm   \n",
       "2                        timesfm   \n",
       "3                        timesfm   \n",
       "4                        timesfm   \n",
       "5                        timesfm   \n",
       "6                        timesfm   \n",
       "7                        timesfm   \n",
       "8                        timesfm   \n",
       "9                        timesfm   \n",
       "10                       timesfm   \n",
       "16               timesfm.configs   \n",
       "17               timesfm.configs   \n",
       "18               timesfm.configs   \n",
       "19               timesfm.configs   \n",
       "20               timesfm.configs   \n",
       "21               timesfm.configs   \n",
       "31           timesfm.torch.dense   \n",
       "32           timesfm.torch.dense   \n",
       "33           timesfm.torch.dense   \n",
       "34           timesfm.torch.dense   \n",
       "35           timesfm.torch.dense   \n",
       "36           timesfm.torch.dense   \n",
       "37           timesfm.torch.dense   \n",
       "38           timesfm.torch.dense   \n",
       "39           timesfm.torch.dense   \n",
       "40           timesfm.torch.dense   \n",
       "41           timesfm.torch.dense   \n",
       "42           timesfm.torch.dense   \n",
       "43           timesfm.torch.dense   \n",
       "44           timesfm.torch.dense   \n",
       "45           timesfm.torch.dense   \n",
       "46           timesfm.torch.dense   \n",
       "47           timesfm.torch.dense   \n",
       "48           timesfm.torch.dense   \n",
       "49           timesfm.torch.dense   \n",
       "50           timesfm.torch.dense   \n",
       "51           timesfm.torch.dense   \n",
       "52           timesfm.torch.dense   \n",
       "53           timesfm.torch.dense   \n",
       "54           timesfm.torch.dense   \n",
       "55           timesfm.torch.dense   \n",
       "56           timesfm.torch.dense   \n",
       "57           timesfm.torch.dense   \n",
       "58           timesfm.torch.dense   \n",
       "59           timesfm.torch.dense   \n",
       "60           timesfm.torch.dense   \n",
       "61           timesfm.torch.dense   \n",
       "62           timesfm.torch.dense   \n",
       "63           timesfm.torch.dense   \n",
       "64           timesfm.torch.dense   \n",
       "65           timesfm.torch.dense   \n",
       "66           timesfm.torch.dense   \n",
       "67           timesfm.torch.dense   \n",
       "68           timesfm.torch.dense   \n",
       "69           timesfm.torch.dense   \n",
       "70           timesfm.torch.dense   \n",
       "71           timesfm.torch.dense   \n",
       "72           timesfm.torch.dense   \n",
       "73           timesfm.torch.dense   \n",
       "74           timesfm.torch.dense   \n",
       "75           timesfm.torch.dense   \n",
       "76           timesfm.torch.dense   \n",
       "77           timesfm.torch.dense   \n",
       "78           timesfm.torch.dense   \n",
       "79           timesfm.torch.dense   \n",
       "80           timesfm.torch.dense   \n",
       "81           timesfm.torch.dense   \n",
       "82           timesfm.torch.dense   \n",
       "83           timesfm.torch.dense   \n",
       "84           timesfm.torch.dense   \n",
       "85           timesfm.torch.dense   \n",
       "86           timesfm.torch.dense   \n",
       "87           timesfm.torch.dense   \n",
       "88           timesfm.torch.dense   \n",
       "89           timesfm.torch.dense   \n",
       "90           timesfm.torch.dense   \n",
       "91           timesfm.torch.dense   \n",
       "92           timesfm.torch.dense   \n",
       "93           timesfm.torch.dense   \n",
       "94           timesfm.torch.dense   \n",
       "95           timesfm.torch.dense   \n",
       "96           timesfm.torch.dense   \n",
       "97           timesfm.torch.dense   \n",
       "98           timesfm.torch.dense   \n",
       "99           timesfm.torch.dense   \n",
       "100          timesfm.torch.dense   \n",
       "101          timesfm.torch.dense   \n",
       "102          timesfm.torch.dense   \n",
       "103          timesfm.torch.dense   \n",
       "104          timesfm.torch.dense   \n",
       "105          timesfm.torch.dense   \n",
       "106          timesfm.torch.dense   \n",
       "107          timesfm.torch.dense   \n",
       "108          timesfm.torch.dense   \n",
       "109          timesfm.torch.dense   \n",
       "110          timesfm.torch.dense   \n",
       "111          timesfm.torch.dense   \n",
       "112          timesfm.torch.dense   \n",
       "113          timesfm.torch.dense   \n",
       "114          timesfm.torch.dense   \n",
       "115          timesfm.torch.dense   \n",
       "116          timesfm.torch.dense   \n",
       "117          timesfm.torch.dense   \n",
       "118          timesfm.torch.dense   \n",
       "119          timesfm.torch.dense   \n",
       "120          timesfm.torch.dense   \n",
       "121          timesfm.torch.dense   \n",
       "122          timesfm.torch.dense   \n",
       "123          timesfm.torch.dense   \n",
       "124          timesfm.torch.dense   \n",
       "125          timesfm.torch.dense   \n",
       "126          timesfm.torch.dense   \n",
       "127          timesfm.torch.dense   \n",
       "128          timesfm.torch.dense   \n",
       "129          timesfm.torch.dense   \n",
       "130          timesfm.torch.dense   \n",
       "131          timesfm.torch.dense   \n",
       "132          timesfm.torch.dense   \n",
       "136  timesfm.torch.normalization   \n",
       "137  timesfm.torch.normalization   \n",
       "138  timesfm.torch.normalization   \n",
       "139  timesfm.torch.normalization   \n",
       "140  timesfm.torch.normalization   \n",
       "141  timesfm.torch.normalization   \n",
       "142  timesfm.torch.normalization   \n",
       "143  timesfm.torch.normalization   \n",
       "144  timesfm.torch.normalization   \n",
       "145  timesfm.torch.normalization   \n",
       "146  timesfm.torch.normalization   \n",
       "147  timesfm.torch.normalization   \n",
       "148  timesfm.torch.normalization   \n",
       "149  timesfm.torch.normalization   \n",
       "150  timesfm.torch.normalization   \n",
       "151  timesfm.torch.normalization   \n",
       "152  timesfm.torch.normalization   \n",
       "153  timesfm.torch.normalization   \n",
       "154  timesfm.torch.normalization   \n",
       "155  timesfm.torch.normalization   \n",
       "156  timesfm.torch.normalization   \n",
       "157  timesfm.torch.normalization   \n",
       "158  timesfm.torch.normalization   \n",
       "159  timesfm.torch.normalization   \n",
       "160  timesfm.torch.normalization   \n",
       "161  timesfm.torch.normalization   \n",
       "162  timesfm.torch.normalization   \n",
       "163  timesfm.torch.normalization   \n",
       "164  timesfm.torch.normalization   \n",
       "165  timesfm.torch.normalization   \n",
       "166  timesfm.torch.normalization   \n",
       "167  timesfm.torch.normalization   \n",
       "168  timesfm.torch.normalization   \n",
       "169  timesfm.torch.normalization   \n",
       "170  timesfm.torch.normalization   \n",
       "171  timesfm.torch.normalization   \n",
       "172  timesfm.torch.normalization   \n",
       "173  timesfm.torch.normalization   \n",
       "174  timesfm.torch.normalization   \n",
       "175  timesfm.torch.normalization   \n",
       "176  timesfm.torch.normalization   \n",
       "177  timesfm.torch.normalization   \n",
       "178  timesfm.torch.normalization   \n",
       "179  timesfm.torch.normalization   \n",
       "180  timesfm.torch.normalization   \n",
       "181  timesfm.torch.normalization   \n",
       "182  timesfm.torch.normalization   \n",
       "183  timesfm.torch.normalization   \n",
       "184  timesfm.torch.normalization   \n",
       "185  timesfm.torch.normalization   \n",
       "186  timesfm.torch.normalization   \n",
       "189    timesfm.torch.transformer   \n",
       "190    timesfm.torch.transformer   \n",
       "191    timesfm.torch.transformer   \n",
       "192    timesfm.torch.transformer   \n",
       "193    timesfm.torch.transformer   \n",
       "194    timesfm.torch.transformer   \n",
       "195    timesfm.torch.transformer   \n",
       "196    timesfm.torch.transformer   \n",
       "197    timesfm.torch.transformer   \n",
       "198    timesfm.torch.transformer   \n",
       "199    timesfm.torch.transformer   \n",
       "200    timesfm.torch.transformer   \n",
       "201    timesfm.torch.transformer   \n",
       "202    timesfm.torch.transformer   \n",
       "203    timesfm.torch.transformer   \n",
       "204    timesfm.torch.transformer   \n",
       "205    timesfm.torch.transformer   \n",
       "206    timesfm.torch.transformer   \n",
       "207    timesfm.torch.transformer   \n",
       "208    timesfm.torch.transformer   \n",
       "209    timesfm.torch.transformer   \n",
       "210    timesfm.torch.transformer   \n",
       "211    timesfm.torch.transformer   \n",
       "212    timesfm.torch.transformer   \n",
       "213    timesfm.torch.transformer   \n",
       "214    timesfm.torch.transformer   \n",
       "215    timesfm.torch.transformer   \n",
       "216    timesfm.torch.transformer   \n",
       "217    timesfm.torch.transformer   \n",
       "218    timesfm.torch.transformer   \n",
       "219    timesfm.torch.transformer   \n",
       "220    timesfm.torch.transformer   \n",
       "221    timesfm.torch.transformer   \n",
       "222    timesfm.torch.transformer   \n",
       "223    timesfm.torch.transformer   \n",
       "224    timesfm.torch.transformer   \n",
       "225    timesfm.torch.transformer   \n",
       "226    timesfm.torch.transformer   \n",
       "227    timesfm.torch.transformer   \n",
       "228    timesfm.torch.transformer   \n",
       "229    timesfm.torch.transformer   \n",
       "230    timesfm.torch.transformer   \n",
       "231    timesfm.torch.transformer   \n",
       "232    timesfm.torch.transformer   \n",
       "233    timesfm.torch.transformer   \n",
       "234    timesfm.torch.transformer   \n",
       "235    timesfm.torch.transformer   \n",
       "236    timesfm.torch.transformer   \n",
       "237    timesfm.torch.transformer   \n",
       "238    timesfm.torch.transformer   \n",
       "239    timesfm.torch.transformer   \n",
       "240    timesfm.torch.transformer   \n",
       "241    timesfm.torch.transformer   \n",
       "242    timesfm.torch.transformer   \n",
       "243    timesfm.torch.transformer   \n",
       "244    timesfm.torch.transformer   \n",
       "245    timesfm.torch.transformer   \n",
       "246    timesfm.torch.transformer   \n",
       "247    timesfm.torch.transformer   \n",
       "248    timesfm.torch.transformer   \n",
       "249    timesfm.torch.transformer   \n",
       "250    timesfm.torch.transformer   \n",
       "251    timesfm.torch.transformer   \n",
       "252    timesfm.torch.transformer   \n",
       "253    timesfm.torch.transformer   \n",
       "254    timesfm.torch.transformer   \n",
       "255    timesfm.torch.transformer   \n",
       "256    timesfm.torch.transformer   \n",
       "257    timesfm.torch.transformer   \n",
       "258    timesfm.torch.transformer   \n",
       "259    timesfm.torch.transformer   \n",
       "260    timesfm.torch.transformer   \n",
       "261    timesfm.torch.transformer   \n",
       "262    timesfm.torch.transformer   \n",
       "263    timesfm.torch.transformer   \n",
       "264    timesfm.torch.transformer   \n",
       "265    timesfm.torch.transformer   \n",
       "266    timesfm.torch.transformer   \n",
       "267    timesfm.torch.transformer   \n",
       "268    timesfm.torch.transformer   \n",
       "269    timesfm.torch.transformer   \n",
       "270    timesfm.torch.transformer   \n",
       "271    timesfm.torch.transformer   \n",
       "272    timesfm.torch.transformer   \n",
       "273    timesfm.torch.transformer   \n",
       "274    timesfm.torch.transformer   \n",
       "275    timesfm.torch.transformer   \n",
       "276    timesfm.torch.transformer   \n",
       "277    timesfm.torch.transformer   \n",
       "278    timesfm.torch.transformer   \n",
       "279    timesfm.torch.transformer   \n",
       "280    timesfm.torch.transformer   \n",
       "281    timesfm.torch.transformer   \n",
       "282    timesfm.torch.transformer   \n",
       "283    timesfm.torch.transformer   \n",
       "284    timesfm.torch.transformer   \n",
       "285    timesfm.torch.transformer   \n",
       "286    timesfm.torch.transformer   \n",
       "287    timesfm.torch.transformer   \n",
       "288    timesfm.torch.transformer   \n",
       "289    timesfm.torch.transformer   \n",
       "290    timesfm.torch.transformer   \n",
       "291    timesfm.torch.transformer   \n",
       "292    timesfm.torch.transformer   \n",
       "293    timesfm.torch.transformer   \n",
       "294    timesfm.torch.transformer   \n",
       "295    timesfm.torch.transformer   \n",
       "296    timesfm.torch.transformer   \n",
       "297    timesfm.torch.transformer   \n",
       "298    timesfm.torch.transformer   \n",
       "299    timesfm.torch.transformer   \n",
       "300    timesfm.torch.transformer   \n",
       "301    timesfm.torch.transformer   \n",
       "302    timesfm.torch.transformer   \n",
       "303    timesfm.torch.transformer   \n",
       "304    timesfm.torch.transformer   \n",
       "305    timesfm.torch.transformer   \n",
       "306    timesfm.torch.transformer   \n",
       "307    timesfm.torch.transformer   \n",
       "308    timesfm.torch.transformer   \n",
       "309    timesfm.torch.transformer   \n",
       "310    timesfm.torch.transformer   \n",
       "311    timesfm.torch.transformer   \n",
       "312    timesfm.torch.transformer   \n",
       "313    timesfm.torch.transformer   \n",
       "314    timesfm.torch.transformer   \n",
       "315    timesfm.torch.transformer   \n",
       "316    timesfm.torch.transformer   \n",
       "317    timesfm.torch.transformer   \n",
       "318    timesfm.torch.transformer   \n",
       "319    timesfm.torch.transformer   \n",
       "320    timesfm.torch.transformer   \n",
       "321    timesfm.torch.transformer   \n",
       "322    timesfm.torch.transformer   \n",
       "323    timesfm.torch.transformer   \n",
       "324    timesfm.torch.transformer   \n",
       "325    timesfm.torch.transformer   \n",
       "326    timesfm.torch.transformer   \n",
       "327    timesfm.torch.transformer   \n",
       "328    timesfm.torch.transformer   \n",
       "329    timesfm.torch.transformer   \n",
       "330    timesfm.torch.transformer   \n",
       "331    timesfm.torch.transformer   \n",
       "332    timesfm.torch.transformer   \n",
       "333    timesfm.torch.transformer   \n",
       "334    timesfm.torch.transformer   \n",
       "335    timesfm.torch.transformer   \n",
       "336    timesfm.torch.transformer   \n",
       "337    timesfm.torch.transformer   \n",
       "338    timesfm.torch.transformer   \n",
       "339    timesfm.torch.transformer   \n",
       "340    timesfm.torch.transformer   \n",
       "341    timesfm.torch.transformer   \n",
       "342    timesfm.torch.transformer   \n",
       "343    timesfm.torch.transformer   \n",
       "344    timesfm.torch.transformer   \n",
       "345    timesfm.torch.transformer   \n",
       "346    timesfm.torch.transformer   \n",
       "347    timesfm.torch.transformer   \n",
       "348    timesfm.torch.transformer   \n",
       "349    timesfm.torch.transformer   \n",
       "350    timesfm.torch.transformer   \n",
       "351    timesfm.torch.transformer   \n",
       "352    timesfm.torch.transformer   \n",
       "353    timesfm.torch.transformer   \n",
       "354    timesfm.torch.transformer   \n",
       "355    timesfm.torch.transformer   \n",
       "356    timesfm.torch.transformer   \n",
       "357    timesfm.torch.transformer   \n",
       "358    timesfm.torch.transformer   \n",
       "359    timesfm.torch.transformer   \n",
       "360    timesfm.torch.transformer   \n",
       "361    timesfm.torch.transformer   \n",
       "362    timesfm.torch.transformer   \n",
       "363    timesfm.torch.transformer   \n",
       "364    timesfm.torch.transformer   \n",
       "365    timesfm.torch.transformer   \n",
       "366    timesfm.torch.transformer   \n",
       "367    timesfm.torch.transformer   \n",
       "368    timesfm.torch.transformer   \n",
       "369    timesfm.torch.transformer   \n",
       "370    timesfm.torch.transformer   \n",
       "371    timesfm.torch.transformer   \n",
       "372    timesfm.torch.transformer   \n",
       "373    timesfm.torch.transformer   \n",
       "374    timesfm.torch.transformer   \n",
       "375    timesfm.torch.transformer   \n",
       "376    timesfm.torch.transformer   \n",
       "377    timesfm.torch.transformer   \n",
       "378    timesfm.torch.transformer   \n",
       "379    timesfm.torch.transformer   \n",
       "380    timesfm.torch.transformer   \n",
       "381    timesfm.torch.transformer   \n",
       "382    timesfm.torch.transformer   \n",
       "383    timesfm.torch.transformer   \n",
       "384    timesfm.torch.transformer   \n",
       "385    timesfm.torch.transformer   \n",
       "386    timesfm.torch.transformer   \n",
       "387    timesfm.torch.transformer   \n",
       "388    timesfm.torch.transformer   \n",
       "389    timesfm.torch.transformer   \n",
       "390    timesfm.torch.transformer   \n",
       "391    timesfm.torch.transformer   \n",
       "392    timesfm.torch.transformer   \n",
       "393    timesfm.torch.transformer   \n",
       "394    timesfm.torch.transformer   \n",
       "395    timesfm.torch.transformer   \n",
       "396    timesfm.torch.transformer   \n",
       "397    timesfm.torch.transformer   \n",
       "398    timesfm.torch.transformer   \n",
       "399    timesfm.torch.transformer   \n",
       "400    timesfm.torch.transformer   \n",
       "401    timesfm.torch.transformer   \n",
       "402    timesfm.torch.transformer   \n",
       "403    timesfm.torch.transformer   \n",
       "404    timesfm.torch.transformer   \n",
       "405    timesfm.torch.transformer   \n",
       "406    timesfm.torch.transformer   \n",
       "407    timesfm.torch.transformer   \n",
       "408    timesfm.torch.transformer   \n",
       "409    timesfm.torch.transformer   \n",
       "410    timesfm.torch.transformer   \n",
       "411    timesfm.torch.transformer   \n",
       "412    timesfm.torch.transformer   \n",
       "413    timesfm.torch.transformer   \n",
       "414    timesfm.torch.transformer   \n",
       "415    timesfm.torch.transformer   \n",
       "416    timesfm.torch.transformer   \n",
       "417    timesfm.torch.transformer   \n",
       "418    timesfm.torch.transformer   \n",
       "419    timesfm.torch.transformer   \n",
       "420    timesfm.torch.transformer   \n",
       "421    timesfm.torch.transformer   \n",
       "422    timesfm.torch.transformer   \n",
       "423    timesfm.torch.transformer   \n",
       "424    timesfm.torch.transformer   \n",
       "425    timesfm.torch.transformer   \n",
       "426    timesfm.torch.transformer   \n",
       "427    timesfm.torch.transformer   \n",
       "428    timesfm.torch.transformer   \n",
       "429    timesfm.torch.transformer   \n",
       "430    timesfm.torch.transformer   \n",
       "431    timesfm.torch.transformer   \n",
       "432    timesfm.torch.transformer   \n",
       "433    timesfm.torch.transformer   \n",
       "434    timesfm.torch.transformer   \n",
       "435    timesfm.torch.transformer   \n",
       "436    timesfm.torch.transformer   \n",
       "437    timesfm.torch.transformer   \n",
       "438    timesfm.torch.transformer   \n",
       "439    timesfm.torch.transformer   \n",
       "440    timesfm.torch.transformer   \n",
       "441    timesfm.torch.transformer   \n",
       "442    timesfm.torch.transformer   \n",
       "443    timesfm.torch.transformer   \n",
       "444    timesfm.torch.transformer   \n",
       "445    timesfm.torch.transformer   \n",
       "446    timesfm.torch.transformer   \n",
       "447    timesfm.torch.transformer   \n",
       "448    timesfm.torch.transformer   \n",
       "449    timesfm.torch.transformer   \n",
       "450    timesfm.torch.transformer   \n",
       "451    timesfm.torch.transformer   \n",
       "452    timesfm.torch.transformer   \n",
       "453    timesfm.torch.transformer   \n",
       "454    timesfm.torch.transformer   \n",
       "455    timesfm.torch.transformer   \n",
       "456    timesfm.torch.transformer   \n",
       "457    timesfm.torch.transformer   \n",
       "458    timesfm.torch.transformer   \n",
       "459    timesfm.torch.transformer   \n",
       "460    timesfm.torch.transformer   \n",
       "461    timesfm.torch.transformer   \n",
       "462    timesfm.torch.transformer   \n",
       "463    timesfm.torch.transformer   \n",
       "464    timesfm.torch.transformer   \n",
       "465    timesfm.torch.transformer   \n",
       "466    timesfm.torch.transformer   \n",
       "467    timesfm.torch.transformer   \n",
       "468    timesfm.torch.transformer   \n",
       "469    timesfm.torch.transformer   \n",
       "470    timesfm.torch.transformer   \n",
       "471    timesfm.torch.transformer   \n",
       "472    timesfm.torch.transformer   \n",
       "473    timesfm.torch.transformer   \n",
       "474    timesfm.torch.transformer   \n",
       "475    timesfm.torch.transformer   \n",
       "476    timesfm.torch.transformer   \n",
       "477    timesfm.torch.transformer   \n",
       "478    timesfm.torch.transformer   \n",
       "479    timesfm.torch.transformer   \n",
       "480    timesfm.torch.transformer   \n",
       "481    timesfm.torch.transformer   \n",
       "482    timesfm.torch.transformer   \n",
       "483    timesfm.torch.transformer   \n",
       "484    timesfm.torch.transformer   \n",
       "485    timesfm.torch.transformer   \n",
       "486    timesfm.torch.transformer   \n",
       "487    timesfm.torch.transformer   \n",
       "488    timesfm.torch.transformer   \n",
       "489    timesfm.torch.transformer   \n",
       "490    timesfm.torch.transformer   \n",
       "491    timesfm.torch.transformer   \n",
       "492    timesfm.torch.transformer   \n",
       "493    timesfm.torch.transformer   \n",
       "494    timesfm.torch.transformer   \n",
       "495    timesfm.torch.transformer   \n",
       "496    timesfm.torch.transformer   \n",
       "497    timesfm.torch.transformer   \n",
       "498    timesfm.torch.transformer   \n",
       "506           timesfm.torch.util   \n",
       "507           timesfm.torch.util   \n",
       "508           timesfm.torch.util   \n",
       "\n",
       "                                                             name      kind  \\\n",
       "0                                                  ForecastConfig     class   \n",
       "1                                          TimesFM_2p5_200M_torch     class   \n",
       "2                                  TimesFM_2p5_200M_torch.compile    method   \n",
       "3                                 TimesFM_2p5_200M_torch.forecast    method   \n",
       "4                 TimesFM_2p5_200M_torch.forecast_with_covariates    method   \n",
       "5                          TimesFM_2p5_200M_torch.from_pretrained    method   \n",
       "6                      TimesFM_2p5_200M_torch.generate_model_card    method   \n",
       "7                          TimesFM_2p5_200M_torch.load_checkpoint    method   \n",
       "8                                    TimesFM_2p5_200M_torch.model    method   \n",
       "9                              TimesFM_2p5_200M_torch.push_to_hub    method   \n",
       "10                         TimesFM_2p5_200M_torch.save_pretrained    method   \n",
       "16                                                        Literal  callable   \n",
       "17                                                 ForecastConfig     class   \n",
       "18                                    RandomFourierFeaturesConfig     class   \n",
       "19                                            ResidualBlockConfig     class   \n",
       "20                                      StackedTransformersConfig     class   \n",
       "21                                              TransformerConfig     class   \n",
       "31                                          RandomFourierFeatures     class   \n",
       "32                                                  ResidualBlock     class   \n",
       "33                               RandomFourierFeatures.add_module    method   \n",
       "34                                    RandomFourierFeatures.apply    method   \n",
       "35                                 RandomFourierFeatures.bfloat16    method   \n",
       "36                                  RandomFourierFeatures.buffers    method   \n",
       "37                                 RandomFourierFeatures.children    method   \n",
       "38                                  RandomFourierFeatures.compile    method   \n",
       "39                                      RandomFourierFeatures.cpu    method   \n",
       "40                                     RandomFourierFeatures.cuda    method   \n",
       "41                                   RandomFourierFeatures.double    method   \n",
       "42                                     RandomFourierFeatures.eval    method   \n",
       "43                               RandomFourierFeatures.extra_repr    method   \n",
       "44                                    RandomFourierFeatures.float    method   \n",
       "45                                  RandomFourierFeatures.forward    method   \n",
       "46                               RandomFourierFeatures.get_buffer    method   \n",
       "47                          RandomFourierFeatures.get_extra_state    method   \n",
       "48                            RandomFourierFeatures.get_parameter    method   \n",
       "49                            RandomFourierFeatures.get_submodule    method   \n",
       "50                                     RandomFourierFeatures.half    method   \n",
       "51                                      RandomFourierFeatures.ipu    method   \n",
       "52                          RandomFourierFeatures.load_state_dict    method   \n",
       "53                                  RandomFourierFeatures.modules    method   \n",
       "54                                     RandomFourierFeatures.mtia    method   \n",
       "55                            RandomFourierFeatures.named_buffers    method   \n",
       "56                           RandomFourierFeatures.named_children    method   \n",
       "57                            RandomFourierFeatures.named_modules    method   \n",
       "58                         RandomFourierFeatures.named_parameters    method   \n",
       "59                               RandomFourierFeatures.parameters    method   \n",
       "60                   RandomFourierFeatures.register_backward_hook    method   \n",
       "61                          RandomFourierFeatures.register_buffer    method   \n",
       "62                    RandomFourierFeatures.register_forward_hook    method   \n",
       "63                RandomFourierFeatures.register_forward_pre_hook    method   \n",
       "64              RandomFourierFeatures.register_full_backward_hook    method   \n",
       "65          RandomFourierFeatures.register_full_backward_pre_hook    method   \n",
       "66       RandomFourierFeatures.register_load_state_dict_post_hook    method   \n",
       "67        RandomFourierFeatures.register_load_state_dict_pre_hook    method   \n",
       "68                          RandomFourierFeatures.register_module    method   \n",
       "69                       RandomFourierFeatures.register_parameter    method   \n",
       "70            RandomFourierFeatures.register_state_dict_post_hook    method   \n",
       "71             RandomFourierFeatures.register_state_dict_pre_hook    method   \n",
       "72                           RandomFourierFeatures.requires_grad_    method   \n",
       "73                          RandomFourierFeatures.set_extra_state    method   \n",
       "74                            RandomFourierFeatures.set_submodule    method   \n",
       "75                             RandomFourierFeatures.share_memory    method   \n",
       "76                               RandomFourierFeatures.state_dict    method   \n",
       "77                                       RandomFourierFeatures.to    method   \n",
       "78                                 RandomFourierFeatures.to_empty    method   \n",
       "79                                    RandomFourierFeatures.train    method   \n",
       "80                                     RandomFourierFeatures.type    method   \n",
       "81                                      RandomFourierFeatures.xpu    method   \n",
       "82                                RandomFourierFeatures.zero_grad    method   \n",
       "83                                       ResidualBlock.add_module    method   \n",
       "84                                            ResidualBlock.apply    method   \n",
       "85                                         ResidualBlock.bfloat16    method   \n",
       "86                                          ResidualBlock.buffers    method   \n",
       "87                                         ResidualBlock.children    method   \n",
       "88                                          ResidualBlock.compile    method   \n",
       "89                                              ResidualBlock.cpu    method   \n",
       "90                                             ResidualBlock.cuda    method   \n",
       "91                                           ResidualBlock.double    method   \n",
       "92                                             ResidualBlock.eval    method   \n",
       "93                                       ResidualBlock.extra_repr    method   \n",
       "94                                            ResidualBlock.float    method   \n",
       "95                                          ResidualBlock.forward    method   \n",
       "96                                       ResidualBlock.get_buffer    method   \n",
       "97                                  ResidualBlock.get_extra_state    method   \n",
       "98                                    ResidualBlock.get_parameter    method   \n",
       "99                                    ResidualBlock.get_submodule    method   \n",
       "100                                            ResidualBlock.half    method   \n",
       "101                                             ResidualBlock.ipu    method   \n",
       "102                                 ResidualBlock.load_state_dict    method   \n",
       "103                                         ResidualBlock.modules    method   \n",
       "104                                            ResidualBlock.mtia    method   \n",
       "105                                   ResidualBlock.named_buffers    method   \n",
       "106                                  ResidualBlock.named_children    method   \n",
       "107                                   ResidualBlock.named_modules    method   \n",
       "108                                ResidualBlock.named_parameters    method   \n",
       "109                                      ResidualBlock.parameters    method   \n",
       "110                          ResidualBlock.register_backward_hook    method   \n",
       "111                                 ResidualBlock.register_buffer    method   \n",
       "112                           ResidualBlock.register_forward_hook    method   \n",
       "113                       ResidualBlock.register_forward_pre_hook    method   \n",
       "114                     ResidualBlock.register_full_backward_hook    method   \n",
       "115                 ResidualBlock.register_full_backward_pre_hook    method   \n",
       "116              ResidualBlock.register_load_state_dict_post_hook    method   \n",
       "117               ResidualBlock.register_load_state_dict_pre_hook    method   \n",
       "118                                 ResidualBlock.register_module    method   \n",
       "119                              ResidualBlock.register_parameter    method   \n",
       "120                   ResidualBlock.register_state_dict_post_hook    method   \n",
       "121                    ResidualBlock.register_state_dict_pre_hook    method   \n",
       "122                                  ResidualBlock.requires_grad_    method   \n",
       "123                                 ResidualBlock.set_extra_state    method   \n",
       "124                                   ResidualBlock.set_submodule    method   \n",
       "125                                    ResidualBlock.share_memory    method   \n",
       "126                                      ResidualBlock.state_dict    method   \n",
       "127                                              ResidualBlock.to    method   \n",
       "128                                        ResidualBlock.to_empty    method   \n",
       "129                                           ResidualBlock.train    method   \n",
       "130                                            ResidualBlock.type    method   \n",
       "131                                             ResidualBlock.xpu    method   \n",
       "132                                       ResidualBlock.zero_grad    method   \n",
       "136                                                       RMSNorm     class   \n",
       "137                                            RMSNorm.add_module    method   \n",
       "138                                                 RMSNorm.apply    method   \n",
       "139                                              RMSNorm.bfloat16    method   \n",
       "140                                               RMSNorm.buffers    method   \n",
       "141                                              RMSNorm.children    method   \n",
       "142                                               RMSNorm.compile    method   \n",
       "143                                                   RMSNorm.cpu    method   \n",
       "144                                                  RMSNorm.cuda    method   \n",
       "145                                                RMSNorm.double    method   \n",
       "146                                                  RMSNorm.eval    method   \n",
       "147                                            RMSNorm.extra_repr    method   \n",
       "148                                                 RMSNorm.float    method   \n",
       "149                                               RMSNorm.forward    method   \n",
       "150                                            RMSNorm.get_buffer    method   \n",
       "151                                       RMSNorm.get_extra_state    method   \n",
       "152                                         RMSNorm.get_parameter    method   \n",
       "153                                         RMSNorm.get_submodule    method   \n",
       "154                                                  RMSNorm.half    method   \n",
       "155                                                   RMSNorm.ipu    method   \n",
       "156                                       RMSNorm.load_state_dict    method   \n",
       "157                                               RMSNorm.modules    method   \n",
       "158                                                  RMSNorm.mtia    method   \n",
       "159                                         RMSNorm.named_buffers    method   \n",
       "160                                        RMSNorm.named_children    method   \n",
       "161                                         RMSNorm.named_modules    method   \n",
       "162                                      RMSNorm.named_parameters    method   \n",
       "163                                            RMSNorm.parameters    method   \n",
       "164                                RMSNorm.register_backward_hook    method   \n",
       "165                                       RMSNorm.register_buffer    method   \n",
       "166                                 RMSNorm.register_forward_hook    method   \n",
       "167                             RMSNorm.register_forward_pre_hook    method   \n",
       "168                           RMSNorm.register_full_backward_hook    method   \n",
       "169                       RMSNorm.register_full_backward_pre_hook    method   \n",
       "170                    RMSNorm.register_load_state_dict_post_hook    method   \n",
       "171                     RMSNorm.register_load_state_dict_pre_hook    method   \n",
       "172                                       RMSNorm.register_module    method   \n",
       "173                                    RMSNorm.register_parameter    method   \n",
       "174                         RMSNorm.register_state_dict_post_hook    method   \n",
       "175                          RMSNorm.register_state_dict_pre_hook    method   \n",
       "176                                        RMSNorm.requires_grad_    method   \n",
       "177                                       RMSNorm.set_extra_state    method   \n",
       "178                                         RMSNorm.set_submodule    method   \n",
       "179                                          RMSNorm.share_memory    method   \n",
       "180                                            RMSNorm.state_dict    method   \n",
       "181                                                    RMSNorm.to    method   \n",
       "182                                              RMSNorm.to_empty    method   \n",
       "183                                                 RMSNorm.train    method   \n",
       "184                                                  RMSNorm.type    method   \n",
       "185                                                   RMSNorm.xpu    method   \n",
       "186                                             RMSNorm.zero_grad    method   \n",
       "189                                                      Callable  callable   \n",
       "190                                                   DecodeCache     class   \n",
       "191                                                     LayerNorm     class   \n",
       "192                                            MultiHeadAttention     class   \n",
       "193                                                   PerDimScale     class   \n",
       "194                                                       RMSNorm     class   \n",
       "195                                     RotaryPositionalEmbedding     class   \n",
       "196                                                   Transformer     class   \n",
       "197                                                make_attn_mask  function   \n",
       "198                                          LayerNorm.add_module    method   \n",
       "199                                               LayerNorm.apply    method   \n",
       "200                                            LayerNorm.bfloat16    method   \n",
       "201                                             LayerNorm.buffers    method   \n",
       "202                                            LayerNorm.children    method   \n",
       "203                                             LayerNorm.compile    method   \n",
       "204                                                 LayerNorm.cpu    method   \n",
       "205                                                LayerNorm.cuda    method   \n",
       "206                                              LayerNorm.double    method   \n",
       "207                                                LayerNorm.eval    method   \n",
       "208                                          LayerNorm.extra_repr    method   \n",
       "209                                               LayerNorm.float    method   \n",
       "210                                             LayerNorm.forward    method   \n",
       "211                                          LayerNorm.get_buffer    method   \n",
       "212                                     LayerNorm.get_extra_state    method   \n",
       "213                                       LayerNorm.get_parameter    method   \n",
       "214                                       LayerNorm.get_submodule    method   \n",
       "215                                                LayerNorm.half    method   \n",
       "216                                                 LayerNorm.ipu    method   \n",
       "217                                     LayerNorm.load_state_dict    method   \n",
       "218                                             LayerNorm.modules    method   \n",
       "219                                                LayerNorm.mtia    method   \n",
       "220                                       LayerNorm.named_buffers    method   \n",
       "221                                      LayerNorm.named_children    method   \n",
       "222                                       LayerNorm.named_modules    method   \n",
       "223                                    LayerNorm.named_parameters    method   \n",
       "224                                          LayerNorm.parameters    method   \n",
       "225                              LayerNorm.register_backward_hook    method   \n",
       "226                                     LayerNorm.register_buffer    method   \n",
       "227                               LayerNorm.register_forward_hook    method   \n",
       "228                           LayerNorm.register_forward_pre_hook    method   \n",
       "229                         LayerNorm.register_full_backward_hook    method   \n",
       "230                     LayerNorm.register_full_backward_pre_hook    method   \n",
       "231                  LayerNorm.register_load_state_dict_post_hook    method   \n",
       "232                   LayerNorm.register_load_state_dict_pre_hook    method   \n",
       "233                                     LayerNorm.register_module    method   \n",
       "234                                  LayerNorm.register_parameter    method   \n",
       "235                       LayerNorm.register_state_dict_post_hook    method   \n",
       "236                        LayerNorm.register_state_dict_pre_hook    method   \n",
       "237                                      LayerNorm.requires_grad_    method   \n",
       "238                                    LayerNorm.reset_parameters    method   \n",
       "239                                     LayerNorm.set_extra_state    method   \n",
       "240                                       LayerNorm.set_submodule    method   \n",
       "241                                        LayerNorm.share_memory    method   \n",
       "242                                          LayerNorm.state_dict    method   \n",
       "243                                                  LayerNorm.to    method   \n",
       "244                                            LayerNorm.to_empty    method   \n",
       "245                                               LayerNorm.train    method   \n",
       "246                                                LayerNorm.type    method   \n",
       "247                                                 LayerNorm.xpu    method   \n",
       "248                                           LayerNorm.zero_grad    method   \n",
       "249                                 MultiHeadAttention.add_module    method   \n",
       "250                                      MultiHeadAttention.apply    method   \n",
       "251                                   MultiHeadAttention.bfloat16    method   \n",
       "252                                    MultiHeadAttention.buffers    method   \n",
       "253                                   MultiHeadAttention.children    method   \n",
       "254                                    MultiHeadAttention.compile    method   \n",
       "255                                        MultiHeadAttention.cpu    method   \n",
       "256                                       MultiHeadAttention.cuda    method   \n",
       "257                                     MultiHeadAttention.double    method   \n",
       "258                                       MultiHeadAttention.eval    method   \n",
       "259                                 MultiHeadAttention.extra_repr    method   \n",
       "260                                      MultiHeadAttention.float    method   \n",
       "261                                    MultiHeadAttention.forward    method   \n",
       "262                                 MultiHeadAttention.get_buffer    method   \n",
       "263                            MultiHeadAttention.get_extra_state    method   \n",
       "264                              MultiHeadAttention.get_parameter    method   \n",
       "265                              MultiHeadAttention.get_submodule    method   \n",
       "266                                       MultiHeadAttention.half    method   \n",
       "267                                        MultiHeadAttention.ipu    method   \n",
       "268                            MultiHeadAttention.load_state_dict    method   \n",
       "269                                    MultiHeadAttention.modules    method   \n",
       "270                                       MultiHeadAttention.mtia    method   \n",
       "271                              MultiHeadAttention.named_buffers    method   \n",
       "272                             MultiHeadAttention.named_children    method   \n",
       "273                              MultiHeadAttention.named_modules    method   \n",
       "274                           MultiHeadAttention.named_parameters    method   \n",
       "275                                 MultiHeadAttention.parameters    method   \n",
       "276                     MultiHeadAttention.register_backward_hook    method   \n",
       "277                            MultiHeadAttention.register_buffer    method   \n",
       "278                      MultiHeadAttention.register_forward_hook    method   \n",
       "279                  MultiHeadAttention.register_forward_pre_hook    method   \n",
       "280                MultiHeadAttention.register_full_backward_hook    method   \n",
       "281            MultiHeadAttention.register_full_backward_pre_hook    method   \n",
       "282         MultiHeadAttention.register_load_state_dict_post_hook    method   \n",
       "283          MultiHeadAttention.register_load_state_dict_pre_hook    method   \n",
       "284                            MultiHeadAttention.register_module    method   \n",
       "285                         MultiHeadAttention.register_parameter    method   \n",
       "286              MultiHeadAttention.register_state_dict_post_hook    method   \n",
       "287               MultiHeadAttention.register_state_dict_pre_hook    method   \n",
       "288                             MultiHeadAttention.requires_grad_    method   \n",
       "289                            MultiHeadAttention.set_extra_state    method   \n",
       "290                              MultiHeadAttention.set_submodule    method   \n",
       "291                               MultiHeadAttention.share_memory    method   \n",
       "292                                 MultiHeadAttention.state_dict    method   \n",
       "293                                         MultiHeadAttention.to    method   \n",
       "294                                   MultiHeadAttention.to_empty    method   \n",
       "295                                      MultiHeadAttention.train    method   \n",
       "296                                       MultiHeadAttention.type    method   \n",
       "297                                        MultiHeadAttention.xpu    method   \n",
       "298                                  MultiHeadAttention.zero_grad    method   \n",
       "299                                        PerDimScale.add_module    method   \n",
       "300                                             PerDimScale.apply    method   \n",
       "301                                          PerDimScale.bfloat16    method   \n",
       "302                                           PerDimScale.buffers    method   \n",
       "303                                          PerDimScale.children    method   \n",
       "304                                           PerDimScale.compile    method   \n",
       "305                                               PerDimScale.cpu    method   \n",
       "306                                              PerDimScale.cuda    method   \n",
       "307                                            PerDimScale.double    method   \n",
       "308                                              PerDimScale.eval    method   \n",
       "309                                        PerDimScale.extra_repr    method   \n",
       "310                                             PerDimScale.float    method   \n",
       "311                                           PerDimScale.forward    method   \n",
       "312                                        PerDimScale.get_buffer    method   \n",
       "313                                   PerDimScale.get_extra_state    method   \n",
       "314                                     PerDimScale.get_parameter    method   \n",
       "315                                     PerDimScale.get_submodule    method   \n",
       "316                                              PerDimScale.half    method   \n",
       "317                                               PerDimScale.ipu    method   \n",
       "318                                   PerDimScale.load_state_dict    method   \n",
       "319                                           PerDimScale.modules    method   \n",
       "320                                              PerDimScale.mtia    method   \n",
       "321                                     PerDimScale.named_buffers    method   \n",
       "322                                    PerDimScale.named_children    method   \n",
       "323                                     PerDimScale.named_modules    method   \n",
       "324                                  PerDimScale.named_parameters    method   \n",
       "325                                        PerDimScale.parameters    method   \n",
       "326                            PerDimScale.register_backward_hook    method   \n",
       "327                                   PerDimScale.register_buffer    method   \n",
       "328                             PerDimScale.register_forward_hook    method   \n",
       "329                         PerDimScale.register_forward_pre_hook    method   \n",
       "330                       PerDimScale.register_full_backward_hook    method   \n",
       "331                   PerDimScale.register_full_backward_pre_hook    method   \n",
       "332                PerDimScale.register_load_state_dict_post_hook    method   \n",
       "333                 PerDimScale.register_load_state_dict_pre_hook    method   \n",
       "334                                   PerDimScale.register_module    method   \n",
       "335                                PerDimScale.register_parameter    method   \n",
       "336                     PerDimScale.register_state_dict_post_hook    method   \n",
       "337                      PerDimScale.register_state_dict_pre_hook    method   \n",
       "338                                    PerDimScale.requires_grad_    method   \n",
       "339                                   PerDimScale.set_extra_state    method   \n",
       "340                                     PerDimScale.set_submodule    method   \n",
       "341                                      PerDimScale.share_memory    method   \n",
       "342                                        PerDimScale.state_dict    method   \n",
       "343                                                PerDimScale.to    method   \n",
       "344                                          PerDimScale.to_empty    method   \n",
       "345                                             PerDimScale.train    method   \n",
       "346                                              PerDimScale.type    method   \n",
       "347                                               PerDimScale.xpu    method   \n",
       "348                                         PerDimScale.zero_grad    method   \n",
       "349                                            RMSNorm.add_module    method   \n",
       "350                                                 RMSNorm.apply    method   \n",
       "351                                              RMSNorm.bfloat16    method   \n",
       "352                                               RMSNorm.buffers    method   \n",
       "353                                              RMSNorm.children    method   \n",
       "354                                               RMSNorm.compile    method   \n",
       "355                                                   RMSNorm.cpu    method   \n",
       "356                                                  RMSNorm.cuda    method   \n",
       "357                                                RMSNorm.double    method   \n",
       "358                                                  RMSNorm.eval    method   \n",
       "359                                            RMSNorm.extra_repr    method   \n",
       "360                                                 RMSNorm.float    method   \n",
       "361                                               RMSNorm.forward    method   \n",
       "362                                            RMSNorm.get_buffer    method   \n",
       "363                                       RMSNorm.get_extra_state    method   \n",
       "364                                         RMSNorm.get_parameter    method   \n",
       "365                                         RMSNorm.get_submodule    method   \n",
       "366                                                  RMSNorm.half    method   \n",
       "367                                                   RMSNorm.ipu    method   \n",
       "368                                       RMSNorm.load_state_dict    method   \n",
       "369                                               RMSNorm.modules    method   \n",
       "370                                                  RMSNorm.mtia    method   \n",
       "371                                         RMSNorm.named_buffers    method   \n",
       "372                                        RMSNorm.named_children    method   \n",
       "373                                         RMSNorm.named_modules    method   \n",
       "374                                      RMSNorm.named_parameters    method   \n",
       "375                                            RMSNorm.parameters    method   \n",
       "376                                RMSNorm.register_backward_hook    method   \n",
       "377                                       RMSNorm.register_buffer    method   \n",
       "378                                 RMSNorm.register_forward_hook    method   \n",
       "379                             RMSNorm.register_forward_pre_hook    method   \n",
       "380                           RMSNorm.register_full_backward_hook    method   \n",
       "381                       RMSNorm.register_full_backward_pre_hook    method   \n",
       "382                    RMSNorm.register_load_state_dict_post_hook    method   \n",
       "383                     RMSNorm.register_load_state_dict_pre_hook    method   \n",
       "384                                       RMSNorm.register_module    method   \n",
       "385                                    RMSNorm.register_parameter    method   \n",
       "386                         RMSNorm.register_state_dict_post_hook    method   \n",
       "387                          RMSNorm.register_state_dict_pre_hook    method   \n",
       "388                                        RMSNorm.requires_grad_    method   \n",
       "389                                       RMSNorm.set_extra_state    method   \n",
       "390                                         RMSNorm.set_submodule    method   \n",
       "391                                          RMSNorm.share_memory    method   \n",
       "392                                            RMSNorm.state_dict    method   \n",
       "393                                                    RMSNorm.to    method   \n",
       "394                                              RMSNorm.to_empty    method   \n",
       "395                                                 RMSNorm.train    method   \n",
       "396                                                  RMSNorm.type    method   \n",
       "397                                                   RMSNorm.xpu    method   \n",
       "398                                             RMSNorm.zero_grad    method   \n",
       "399                          RotaryPositionalEmbedding.add_module    method   \n",
       "400                               RotaryPositionalEmbedding.apply    method   \n",
       "401                            RotaryPositionalEmbedding.bfloat16    method   \n",
       "402                             RotaryPositionalEmbedding.buffers    method   \n",
       "403                            RotaryPositionalEmbedding.children    method   \n",
       "404                             RotaryPositionalEmbedding.compile    method   \n",
       "405                                 RotaryPositionalEmbedding.cpu    method   \n",
       "406                                RotaryPositionalEmbedding.cuda    method   \n",
       "407                              RotaryPositionalEmbedding.double    method   \n",
       "408                                RotaryPositionalEmbedding.eval    method   \n",
       "409                          RotaryPositionalEmbedding.extra_repr    method   \n",
       "410                               RotaryPositionalEmbedding.float    method   \n",
       "411                             RotaryPositionalEmbedding.forward    method   \n",
       "412                          RotaryPositionalEmbedding.get_buffer    method   \n",
       "413                     RotaryPositionalEmbedding.get_extra_state    method   \n",
       "414                       RotaryPositionalEmbedding.get_parameter    method   \n",
       "415                       RotaryPositionalEmbedding.get_submodule    method   \n",
       "416                                RotaryPositionalEmbedding.half    method   \n",
       "417                                 RotaryPositionalEmbedding.ipu    method   \n",
       "418                     RotaryPositionalEmbedding.load_state_dict    method   \n",
       "419                             RotaryPositionalEmbedding.modules    method   \n",
       "420                                RotaryPositionalEmbedding.mtia    method   \n",
       "421                       RotaryPositionalEmbedding.named_buffers    method   \n",
       "422                      RotaryPositionalEmbedding.named_children    method   \n",
       "423                       RotaryPositionalEmbedding.named_modules    method   \n",
       "424                    RotaryPositionalEmbedding.named_parameters    method   \n",
       "425                          RotaryPositionalEmbedding.parameters    method   \n",
       "426              RotaryPositionalEmbedding.register_backward_hook    method   \n",
       "427                     RotaryPositionalEmbedding.register_buffer    method   \n",
       "428               RotaryPositionalEmbedding.register_forward_hook    method   \n",
       "429           RotaryPositionalEmbedding.register_forward_pre_hook    method   \n",
       "430         RotaryPositionalEmbedding.register_full_backward_hook    method   \n",
       "431     RotaryPositionalEmbedding.register_full_backward_pre_hook    method   \n",
       "432  RotaryPositionalEmbedding.register_load_state_dict_post_hook    method   \n",
       "433   RotaryPositionalEmbedding.register_load_state_dict_pre_hook    method   \n",
       "434                     RotaryPositionalEmbedding.register_module    method   \n",
       "435                  RotaryPositionalEmbedding.register_parameter    method   \n",
       "436       RotaryPositionalEmbedding.register_state_dict_post_hook    method   \n",
       "437        RotaryPositionalEmbedding.register_state_dict_pre_hook    method   \n",
       "438                      RotaryPositionalEmbedding.requires_grad_    method   \n",
       "439                     RotaryPositionalEmbedding.set_extra_state    method   \n",
       "440                       RotaryPositionalEmbedding.set_submodule    method   \n",
       "441                        RotaryPositionalEmbedding.share_memory    method   \n",
       "442                          RotaryPositionalEmbedding.state_dict    method   \n",
       "443                                  RotaryPositionalEmbedding.to    method   \n",
       "444                            RotaryPositionalEmbedding.to_empty    method   \n",
       "445                               RotaryPositionalEmbedding.train    method   \n",
       "446                                RotaryPositionalEmbedding.type    method   \n",
       "447                                 RotaryPositionalEmbedding.xpu    method   \n",
       "448                           RotaryPositionalEmbedding.zero_grad    method   \n",
       "449                                        Transformer.add_module    method   \n",
       "450                                             Transformer.apply    method   \n",
       "451                                          Transformer.bfloat16    method   \n",
       "452                                           Transformer.buffers    method   \n",
       "453                                          Transformer.children    method   \n",
       "454                                           Transformer.compile    method   \n",
       "455                                               Transformer.cpu    method   \n",
       "456                                              Transformer.cuda    method   \n",
       "457                                            Transformer.double    method   \n",
       "458                                              Transformer.eval    method   \n",
       "459                                        Transformer.extra_repr    method   \n",
       "460                                             Transformer.float    method   \n",
       "461                                           Transformer.forward    method   \n",
       "462                                        Transformer.get_buffer    method   \n",
       "463                                   Transformer.get_extra_state    method   \n",
       "464                                     Transformer.get_parameter    method   \n",
       "465                                     Transformer.get_submodule    method   \n",
       "466                                              Transformer.half    method   \n",
       "467                                               Transformer.ipu    method   \n",
       "468                                   Transformer.load_state_dict    method   \n",
       "469                                           Transformer.modules    method   \n",
       "470                                              Transformer.mtia    method   \n",
       "471                                     Transformer.named_buffers    method   \n",
       "472                                    Transformer.named_children    method   \n",
       "473                                     Transformer.named_modules    method   \n",
       "474                                  Transformer.named_parameters    method   \n",
       "475                                        Transformer.parameters    method   \n",
       "476                            Transformer.register_backward_hook    method   \n",
       "477                                   Transformer.register_buffer    method   \n",
       "478                             Transformer.register_forward_hook    method   \n",
       "479                         Transformer.register_forward_pre_hook    method   \n",
       "480                       Transformer.register_full_backward_hook    method   \n",
       "481                   Transformer.register_full_backward_pre_hook    method   \n",
       "482                Transformer.register_load_state_dict_post_hook    method   \n",
       "483                 Transformer.register_load_state_dict_pre_hook    method   \n",
       "484                                   Transformer.register_module    method   \n",
       "485                                Transformer.register_parameter    method   \n",
       "486                     Transformer.register_state_dict_post_hook    method   \n",
       "487                      Transformer.register_state_dict_pre_hook    method   \n",
       "488                                    Transformer.requires_grad_    method   \n",
       "489                                   Transformer.set_extra_state    method   \n",
       "490                                     Transformer.set_submodule    method   \n",
       "491                                      Transformer.share_memory    method   \n",
       "492                                        Transformer.state_dict    method   \n",
       "493                                                Transformer.to    method   \n",
       "494                                          Transformer.to_empty    method   \n",
       "495                                             Transformer.train    method   \n",
       "496                                              Transformer.type    method   \n",
       "497                                               Transformer.xpu    method   \n",
       "498                                         Transformer.zero_grad    method   \n",
       "506                                                   DecodeCache     class   \n",
       "507                                                         revin  function   \n",
       "508                                          update_running_stats  function   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            signature  \\\n",
       "0                                                                                                                                                                                                                                                        (max_context: int = 0, max_horizon: int = 0, normalize_inputs: bool = False, window_size: int = 0, per_core_batch_size: int = 1, use_continuous_quantile_head: bool = False, force_flip_invariance: bool = True, infer_is_positive: bool = True, fix_quantile_crossing: bool = False, return_backcast: bool = False) -> None   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (*args, **kwargs) -> ~T   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, forecast_config: timesfm.configs.ForecastConfig, **kwargs) -> None   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, horizon: int, inputs: list[numpy.ndarray]) -> tuple[numpy.ndarray, numpy.ndarray]   \n",
       "4    (self, inputs: list[typing.Sequence[float]], dynamic_numerical_covariates: dict[str, typing.Sequence[typing.Sequence[float]]] | None = None, dynamic_categorical_covariates: dict[str, typing.Sequence[typing.Sequence[int | str]]] | None = None, static_numerical_covariates: dict[str, typing.Sequence[float]] | None = None, static_categorical_covariates: dict[str, typing.Sequence[int | str]] | None = None, xreg_mode: str = 'xreg + timesfm', normalize_xreg_target_per_input: bool = True, ridge: float = 0.0, max_rows_per_col: int = 0, force_on_cpu: bool = False)   \n",
       "5                                                                                                                                                                                                                            (pretrained_model_name_or_path: Union[str, pathlib.Path], *, force_download: bool = False, resume_download: Optional[bool] = None, proxies: Optional[Dict] = None, token: Union[bool, str, NoneType] = None, cache_dir: Union[str, pathlib.Path, NoneType] = None, local_files_only: bool = False, revision: Optional[str] = None, **model_kwargs) -> ~T   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, *args, **kwargs) -> huggingface_hub.repocard.ModelCard   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, path: str)   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (*args, **kwargs)   \n",
       "9                                             (self, repo_id: str, *, config: Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType] = None, commit_message: str = 'Push model using huggingface_hub.', private: Optional[bool] = None, token: Optional[str] = None, branch: Optional[str] = None, create_pr: Optional[bool] = None, allow_patterns: Union[List[str], str, NoneType] = None, ignore_patterns: Union[List[str], str, NoneType] = None, delete_patterns: Union[List[str], str, NoneType] = None, model_card_kwargs: Optional[Dict[str, Any]] = None) -> str   \n",
       "10                                                                                                                                                                                                                                                                                        (self, save_directory: Union[str, pathlib.Path], *, config: Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType] = None, repo_id: Optional[str] = None, push_to_hub: bool = False, model_card_kwargs: Optional[Dict[str, Any]] = None, **push_to_hub_kwargs) -> Optional[str]   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (*args, **kwds)   \n",
       "17                                                                                                                                                                                                                                                       (max_context: int = 0, max_horizon: int = 0, normalize_inputs: bool = False, window_size: int = 0, per_core_batch_size: int = 1, use_continuous_quantile_head: bool = False, force_flip_invariance: bool = True, infer_is_positive: bool = True, fix_quantile_crossing: bool = False, return_backcast: bool = False) -> None   \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (input_dims: int, output_dims: int, projection_stddev: float, use_bias: bool) -> None   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                        (input_dims: int, hidden_dims: int, output_dims: int, use_bias: bool, activation: Literal['relu', 'swish', 'none']) -> None   \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (num_layers: int, transformer: timesfm.configs.TransformerConfig) -> None   \n",
       "21                                                                                                                                                                                                                                                                                              (model_dims: int, hidden_dims: int, num_heads: int, attention_norm: Literal['rms'], feedforward_norm: Literal['rms'], qk_norm: Literal['rms', 'none'], use_bias: bool, use_rotary_position_embeddings: bool, ff_activation: Literal['relu', 'swish', 'none'], fuse_qkv: bool) -> None   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (config: timesfm.configs.RandomFourierFeaturesConfig)   \n",
       "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (config: timesfm.configs.ResidualBlockConfig)   \n",
       "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self, fn: collections.abc.Callable[['Module'], None]) -> Self   \n",
       "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]   \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self) -> collections.abc.Iterator['Module']   \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, *args, **kwargs) -> None   \n",
       "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, device: int | torch.device | None = None) -> Self   \n",
       "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> str   \n",
       "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, x: torch.Tensor) -> torch.Tensor   \n",
       "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, target: str) -> 'Tensor'   \n",
       "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> Any   \n",
       "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, target: str) -> 'Parameter'   \n",
       "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, target: str) -> 'Module'   \n",
       "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, device: int | torch.device | None = None) -> Self   \n",
       "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)   \n",
       "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self) -> collections.abc.Iterator['Module']   \n",
       "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, device: int | torch.device | None = None) -> Self   \n",
       "55                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]   \n",
       "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self) -> collections.abc.Iterator[tuple[str, 'Module']]   \n",
       "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)   \n",
       "58                                                                                                                                                                                                                                                                                                                                                                                                                                (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]   \n",
       "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]   \n",
       "60                                                                                                                                                                                                                                                                                                                (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle   \n",
       "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None   \n",
       "62                                                                                                                                                                                                                                            (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "63                                                                                                                                                                                                                                                                 (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "64                                                                                                                                                                                                                                                                                         (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "65                                                                                                                                                                                                                                                                                                                                               (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, hook)   \n",
       "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, hook)   \n",
       "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (self, name: str, param: torch.nn.parameter.Parameter | None) -> None   \n",
       "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, hook)   \n",
       "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, hook)   \n",
       "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (self, requires_grad: bool = True) -> Self   \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (self, state: Any) -> None   \n",
       "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, target: str, module: 'Module', strict: bool = False) -> None   \n",
       "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, *args, destination=None, prefix='', keep_vars=False)   \n",
       "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, *args, **kwargs)   \n",
       "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self   \n",
       "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (self, mode: bool = True) -> Self   \n",
       "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, dst_type: torch.dtype | str) -> Self   \n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, device: int | torch.device | None = None) -> Self   \n",
       "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, set_to_none: bool = True) -> None   \n",
       "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self, fn: collections.abc.Callable[['Module'], None]) -> Self   \n",
       "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]   \n",
       "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self) -> collections.abc.Iterator['Module']   \n",
       "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, *args, **kwargs) -> None   \n",
       "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, device: int | torch.device | None = None) -> Self   \n",
       "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> str   \n",
       "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Self   \n",
       "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, x: torch.Tensor) -> torch.Tensor   \n",
       "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, target: str) -> 'Tensor'   \n",
       "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> Any   \n",
       "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, target: str) -> 'Parameter'   \n",
       "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, target: str) -> 'Module'   \n",
       "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)   \n",
       "103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "105                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]   \n",
       "106                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self) -> collections.abc.Iterator[tuple[str, 'Module']]   \n",
       "107                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)   \n",
       "108                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]   \n",
       "109                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]   \n",
       "110                                                                                                                                                                                                                                                                                                               (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle   \n",
       "111                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None   \n",
       "112                                                                                                                                                                                                                                           (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "113                                                                                                                                                                                                                                                                (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "114                                                                                                                                                                                                                                                                                        (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "115                                                                                                                                                                                                                                                                                                                                              (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "116                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "117                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "118                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "119                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, name: str, param: torch.nn.parameter.Parameter | None) -> None   \n",
       "120                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "121                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, requires_grad: bool = True) -> Self   \n",
       "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, state: Any) -> None   \n",
       "124                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, target: str, module: 'Module', strict: bool = False) -> None   \n",
       "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, *args, destination=None, prefix='', keep_vars=False)   \n",
       "127                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, *args, **kwargs)   \n",
       "128                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self   \n",
       "129                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, mode: bool = True) -> Self   \n",
       "130                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, dst_type: torch.dtype | str) -> Self   \n",
       "131                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "132                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, set_to_none: bool = True) -> None   \n",
       "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (num_features: int, *, epsilon: float = 1e-06)   \n",
       "137                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "138                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, fn: collections.abc.Callable[['Module'], None]) -> Self   \n",
       "139                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "140                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]   \n",
       "141                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "142                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, *args, **kwargs) -> None   \n",
       "143                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "144                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> str   \n",
       "148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, inputs: torch.Tensor) -> torch.Tensor   \n",
       "150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Tensor'   \n",
       "151                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Any   \n",
       "152                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, target: str) -> 'Parameter'   \n",
       "153                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Module'   \n",
       "154                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "155                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "156                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)   \n",
       "157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "158                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "159                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]   \n",
       "160                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self) -> collections.abc.Iterator[tuple[str, 'Module']]   \n",
       "161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)   \n",
       "162                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]   \n",
       "163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]   \n",
       "164                                                                                                                                                                                                                                                                                                               (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle   \n",
       "165                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None   \n",
       "166                                                                                                                                                                                                                                           (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "167                                                                                                                                                                                                                                                                (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "168                                                                                                                                                                                                                                                                                        (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "169                                                                                                                                                                                                                                                                                                                                              (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "170                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "171                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "172                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "173                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, name: str, param: torch.nn.parameter.Parameter | None) -> None   \n",
       "174                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "175                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "176                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, requires_grad: bool = True) -> Self   \n",
       "177                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, state: Any) -> None   \n",
       "178                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, target: str, module: 'Module', strict: bool = False) -> None   \n",
       "179                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "180                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, *args, destination=None, prefix='', keep_vars=False)   \n",
       "181                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, *args, **kwargs)   \n",
       "182                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self   \n",
       "183                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, mode: bool = True) -> Self   \n",
       "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, dst_type: torch.dtype | str) -> Self   \n",
       "185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, set_to_none: bool = True) -> None   \n",
       "189                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (*args, **kwargs)   \n",
       "190                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (next_index: torch.Tensor, num_masked: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> None   \n",
       "191                                                                                                                                                                                                                                                                                                                                                                                                                    (normalized_shape: Union[int, list[int], torch.Size], eps: float = 1e-05, elementwise_affine: bool = True, bias: bool = True, device=None, dtype=None) -> None   \n",
       "192                                                                                                                                                                                                                                                                               (num_heads: int, in_features: int, *, use_per_dim_scale: bool = True, use_rotary_position_embeddings: bool = True, use_bias: bool = False, attention_fn: Callable[..., torch.Tensor] = <function _torch_dot_product_attention at 0x00000212F95A87C0>, qk_norm: str = 'rms', fuse_qkv: bool = False)   \n",
       "193                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (num_dims: int)   \n",
       "194                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (num_features: int, *, epsilon: float = 1e-06)   \n",
       "195                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (embedding_dims: int, min_timescale: float = 1.0, max_timescale: float = 10000.0)   \n",
       "196                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (config: timesfm.configs.TransformerConfig)   \n",
       "197                                                                                                                                                                                                                                                                                                                                                                                                                                          (query_length: int, num_all_masked_kv: torch.Tensor, query_index_offset: torch.Tensor | None = None, kv_length: int = 0) -> torch.Tensor   \n",
       "198                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "199                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, fn: collections.abc.Callable[['Module'], None]) -> Self   \n",
       "200                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "201                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]   \n",
       "202                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "203                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, *args, **kwargs) -> None   \n",
       "204                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "206                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "207                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "208                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> str   \n",
       "209                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "210                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, input: torch.Tensor) -> torch.Tensor   \n",
       "211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Tensor'   \n",
       "212                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Any   \n",
       "213                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, target: str) -> 'Parameter'   \n",
       "214                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Module'   \n",
       "215                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "216                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "217                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)   \n",
       "218                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "219                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "220                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]   \n",
       "221                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self) -> collections.abc.Iterator[tuple[str, 'Module']]   \n",
       "222                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)   \n",
       "223                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]   \n",
       "224                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]   \n",
       "225                                                                                                                                                                                                                                                                                                               (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle   \n",
       "226                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None   \n",
       "227                                                                                                                                                                                                                                           (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "228                                                                                                                                                                                                                                                                (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "229                                                                                                                                                                                                                                                                                        (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "230                                                                                                                                                                                                                                                                                                                                              (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "231                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "232                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "233                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "234                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, name: str, param: torch.nn.parameter.Parameter | None) -> None   \n",
       "235                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "236                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "237                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, requires_grad: bool = True) -> Self   \n",
       "238                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> None   \n",
       "239                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, state: Any) -> None   \n",
       "240                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, target: str, module: 'Module', strict: bool = False) -> None   \n",
       "241                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "242                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, *args, destination=None, prefix='', keep_vars=False)   \n",
       "243                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, *args, **kwargs)   \n",
       "244                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self   \n",
       "245                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, mode: bool = True) -> Self   \n",
       "246                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, dst_type: torch.dtype | str) -> Self   \n",
       "247                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "248                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, set_to_none: bool = True) -> None   \n",
       "249                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "250                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, fn: collections.abc.Callable[['Module'], None]) -> Self   \n",
       "251                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "252                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]   \n",
       "253                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "254                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, *args, **kwargs) -> None   \n",
       "255                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "256                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "257                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "258                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "259                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> str   \n",
       "260                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "261                                                                                                                                                                                                                                                                                                                                                                               (self, inputs_q: torch.Tensor, *, decode_cache: timesfm.torch.util.DecodeCache | None = None, patch_mask: torch.Tensor | None = None) -> tuple[torch.Tensor, timesfm.torch.util.DecodeCache | None]   \n",
       "262                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Tensor'   \n",
       "263                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Any   \n",
       "264                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, target: str) -> 'Parameter'   \n",
       "265                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Module'   \n",
       "266                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "267                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "268                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)   \n",
       "269                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "270                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "271                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]   \n",
       "272                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self) -> collections.abc.Iterator[tuple[str, 'Module']]   \n",
       "273                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)   \n",
       "274                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]   \n",
       "275                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]   \n",
       "276                                                                                                                                                                                                                                                                                                               (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle   \n",
       "277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None   \n",
       "278                                                                                                                                                                                                                                           (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "279                                                                                                                                                                                                                                                                (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "280                                                                                                                                                                                                                                                                                        (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "281                                                                                                                                                                                                                                                                                                                                              (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "282                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "283                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "284                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "285                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, name: str, param: torch.nn.parameter.Parameter | None) -> None   \n",
       "286                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "287                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "288                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, requires_grad: bool = True) -> Self   \n",
       "289                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, state: Any) -> None   \n",
       "290                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, target: str, module: 'Module', strict: bool = False) -> None   \n",
       "291                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "292                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, *args, destination=None, prefix='', keep_vars=False)   \n",
       "293                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, *args, **kwargs)   \n",
       "294                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self   \n",
       "295                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, mode: bool = True) -> Self   \n",
       "296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, dst_type: torch.dtype | str) -> Self   \n",
       "297                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "298                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, set_to_none: bool = True) -> None   \n",
       "299                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "300                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, fn: collections.abc.Callable[['Module'], None]) -> Self   \n",
       "301                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "302                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]   \n",
       "303                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, *args, **kwargs) -> None   \n",
       "305                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "306                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "307                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "308                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "309                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> str   \n",
       "310                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "311                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, x: torch.Tensor) -> torch.Tensor   \n",
       "312                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Tensor'   \n",
       "313                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Any   \n",
       "314                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, target: str) -> 'Parameter'   \n",
       "315                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Module'   \n",
       "316                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "317                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "318                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)   \n",
       "319                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "320                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "321                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]   \n",
       "322                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self) -> collections.abc.Iterator[tuple[str, 'Module']]   \n",
       "323                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)   \n",
       "324                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]   \n",
       "325                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]   \n",
       "326                                                                                                                                                                                                                                                                                                               (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle   \n",
       "327                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None   \n",
       "328                                                                                                                                                                                                                                           (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "329                                                                                                                                                                                                                                                                (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "330                                                                                                                                                                                                                                                                                        (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "331                                                                                                                                                                                                                                                                                                                                              (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "332                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "333                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "334                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "335                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, name: str, param: torch.nn.parameter.Parameter | None) -> None   \n",
       "336                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "337                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "338                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, requires_grad: bool = True) -> Self   \n",
       "339                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, state: Any) -> None   \n",
       "340                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, target: str, module: 'Module', strict: bool = False) -> None   \n",
       "341                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "342                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, *args, destination=None, prefix='', keep_vars=False)   \n",
       "343                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, *args, **kwargs)   \n",
       "344                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self   \n",
       "345                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, mode: bool = True) -> Self   \n",
       "346                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, dst_type: torch.dtype | str) -> Self   \n",
       "347                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "348                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, set_to_none: bool = True) -> None   \n",
       "349                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "350                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, fn: collections.abc.Callable[['Module'], None]) -> Self   \n",
       "351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "352                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]   \n",
       "353                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "354                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, *args, **kwargs) -> None   \n",
       "355                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "356                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "357                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "358                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "359                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> str   \n",
       "360                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "361                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, inputs: torch.Tensor) -> torch.Tensor   \n",
       "362                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Tensor'   \n",
       "363                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Any   \n",
       "364                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, target: str) -> 'Parameter'   \n",
       "365                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Module'   \n",
       "366                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "367                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "368                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)   \n",
       "369                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "370                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "371                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]   \n",
       "372                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self) -> collections.abc.Iterator[tuple[str, 'Module']]   \n",
       "373                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)   \n",
       "374                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]   \n",
       "375                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]   \n",
       "376                                                                                                                                                                                                                                                                                                               (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle   \n",
       "377                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None   \n",
       "378                                                                                                                                                                                                                                           (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "379                                                                                                                                                                                                                                                                (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "380                                                                                                                                                                                                                                                                                        (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "381                                                                                                                                                                                                                                                                                                                                              (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "382                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "383                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "384                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "385                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, name: str, param: torch.nn.parameter.Parameter | None) -> None   \n",
       "386                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "387                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "388                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, requires_grad: bool = True) -> Self   \n",
       "389                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, state: Any) -> None   \n",
       "390                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, target: str, module: 'Module', strict: bool = False) -> None   \n",
       "391                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "392                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, *args, destination=None, prefix='', keep_vars=False)   \n",
       "393                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, *args, **kwargs)   \n",
       "394                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self   \n",
       "395                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, mode: bool = True) -> Self   \n",
       "396                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, dst_type: torch.dtype | str) -> Self   \n",
       "397                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "398                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, set_to_none: bool = True) -> None   \n",
       "399                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "400                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, fn: collections.abc.Callable[['Module'], None]) -> Self   \n",
       "401                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "402                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]   \n",
       "403                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "404                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, *args, **kwargs) -> None   \n",
       "405                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "406                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "407                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "408                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "409                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> str   \n",
       "410                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "411                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, inputs: torch.Tensor, position: torch.Tensor | None = None)   \n",
       "412                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Tensor'   \n",
       "413                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Any   \n",
       "414                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, target: str) -> 'Parameter'   \n",
       "415                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Module'   \n",
       "416                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "417                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "418                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)   \n",
       "419                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "420                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "421                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]   \n",
       "422                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self) -> collections.abc.Iterator[tuple[str, 'Module']]   \n",
       "423                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)   \n",
       "424                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]   \n",
       "425                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]   \n",
       "426                                                                                                                                                                                                                                                                                                               (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle   \n",
       "427                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None   \n",
       "428                                                                                                                                                                                                                                           (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "429                                                                                                                                                                                                                                                                (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "430                                                                                                                                                                                                                                                                                        (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "431                                                                                                                                                                                                                                                                                                                                              (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "432                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "433                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "434                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "435                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, name: str, param: torch.nn.parameter.Parameter | None) -> None   \n",
       "436                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "437                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "438                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, requires_grad: bool = True) -> Self   \n",
       "439                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, state: Any) -> None   \n",
       "440                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, target: str, module: 'Module', strict: bool = False) -> None   \n",
       "441                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "442                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, *args, destination=None, prefix='', keep_vars=False)   \n",
       "443                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, *args, **kwargs)   \n",
       "444                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self   \n",
       "445                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, mode: bool = True) -> Self   \n",
       "446                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, dst_type: torch.dtype | str) -> Self   \n",
       "447                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "448                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, set_to_none: bool = True) -> None   \n",
       "449                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "450                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self, fn: collections.abc.Callable[['Module'], None]) -> Self   \n",
       "451                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "452                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]   \n",
       "453                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "454                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, *args, **kwargs) -> None   \n",
       "455                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "456                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "457                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "458                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "459                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> str   \n",
       "460                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "461                                                                                                                                                                                                                                                                                                                                                                                        (self, input_embeddings: torch.Tensor, patch_mask: torch.Tensor, decode_cache: timesfm.torch.util.DecodeCache | None = None) -> tuple[torch.Tensor, timesfm.torch.util.DecodeCache | None]   \n",
       "462                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Tensor'   \n",
       "463                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (self) -> Any   \n",
       "464                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (self, target: str) -> 'Parameter'   \n",
       "465                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, target: str) -> 'Module'   \n",
       "466                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "467                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "468                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)   \n",
       "469                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self) -> collections.abc.Iterator['Module']   \n",
       "470                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "471                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]   \n",
       "472                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self) -> collections.abc.Iterator[tuple[str, 'Module']]   \n",
       "473                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)   \n",
       "474                                                                                                                                                                                                                                                                                                                                                                                                                               (self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]   \n",
       "475                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]   \n",
       "476                                                                                                                                                                                                                                                                                                               (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle   \n",
       "477                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None   \n",
       "478                                                                                                                                                                                                                                           (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "479                                                                                                                                                                                                                                                                (self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "480                                                                                                                                                                                                                                                                                        (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "481                                                                                                                                                                                                                                                                                                                                              (self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle   \n",
       "482                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "483                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "484                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, name: str, module: Optional[ForwardRef('Module')]) -> None   \n",
       "485                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (self, name: str, param: torch.nn.parameter.Parameter | None) -> None   \n",
       "486                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "487                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (self, hook)   \n",
       "488                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, requires_grad: bool = True) -> Self   \n",
       "489                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (self, state: Any) -> None   \n",
       "490                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (self, target: str, module: 'Module', strict: bool = False) -> None   \n",
       "491                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (self) -> Self   \n",
       "492                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, *args, destination=None, prefix='', keep_vars=False)   \n",
       "493                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (self, *args, **kwargs)   \n",
       "494                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self   \n",
       "495                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (self, mode: bool = True) -> Self   \n",
       "496                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (self, dst_type: torch.dtype | str) -> Self   \n",
       "497                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, device: int | torch.device | None = None) -> Self   \n",
       "498                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (self, set_to_none: bool = True) -> None   \n",
       "506                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (next_index: torch.Tensor, num_masked: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> None   \n",
       "507                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (x: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor, reverse: bool = False)   \n",
       "508                                                                                                                                                                                                                                                                                                                                                                          (n: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor, x: torch.Tensor, mask: torch.Tensor) -> tuple[tuple[torch.Tensor, torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor, torch.Tensor]]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            doc  \\\n",
       "0                Options for forecasting.\\n\\nAttributes:\\n  max_context: The maximum context length. This is used by the complied decode\\n    function at inference time during batched inference. Any input time series\\n    with length less than max_context will be padded with zeros, and with\\n    length greater than max_context will be truncated.\\n  max_horizon: The maximum horizon length. This is used by the complied decode\\n    function at inference time during batched inference. The compiled cached\\n    decoding function will by default forecast till max_horizon.\\n  normalize_inputs: Whether to normalize the inputs. This is useful when the\\n    raw inputs are of extremely large or small magnitudes which may result in\\n    numerical issues.\\n  window_size: The window size for decomposed forecasting.\\n    TODO(siriuz42)   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   PyTorch implementation of TimesFM 2.5 with 200M parameters.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Attempts to compile the model for fast decoding.\\n\\nSee configs.ForecastConfig for more details on the supported flags.\\n\\nArgs:\\n  forecast_config: Configuration for forecasting flags.\\n  **kwargs: Additional keyword arguments to pass to model.compile().   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Forecasts the time series.   \n",
       "4                Forecasts on a list of time series with covariates.\\n\\nTo optimize inference speed, avoid string valued categorical covariates.\\n\\nArgs:\\n  inputs: A list of time series forecast contexts. Each context time series\\n    should be in a format convertible to JTensor by `jnp.array`.\\n  dynamic_numerical_covariates: A dict of dynamic numerical covariates.\\n  dynamic_categorical_covariates: A dict of dynamic categorical covariates.\\n  static_numerical_covariates: A dict of static numerical covariates.\\n  static_categorical_covariates: A dict of static categorical covariates.\\n  xreg_mode: one of \"xreg + timesfm\" or \"timesfm + xreg\". \"xreg + timesfm\"\\n    fits a model on the residuals of the TimesFM forecast. \"timesfm + xreg\"\\n    fits a model on the targets then forecasts on the residuals via TimesFM.\\n  norm   \n",
       "5                 Download a model from the Huggingface Hub and instantiate it.\\n\\nArgs:\\n    pretrained_model_name_or_path (`str`, `Path`):\\n        - Either the `model_id` (string) of a model hosted on the Hub, e.g. `bigscience/bloom`.\\n        - Or a path to a `directory` containing model weights saved using\\n            [`~transformers.PreTrainedModel.save_pretrained`], e.g., `../path/to/my_model_directory/`.\\n    revision (`str`, *optional*):\\n        Revision of the model on the Hub. Can be a branch name, a git tag or any commit id.\\n        Defaults to the latest commit on `main` branch.\\n    force_download (`bool`, *optional*, defaults to `False`):\\n        Whether to force (re-)downloading the model weights and configuration files from the Hub, overriding\\n        the existing cache.\\n    proxies (`Dict[str, st   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          None   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Loads a TimesFM model from a checkpoint.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             TimesFM 2.5 with 200M parameters.   \n",
       "9              Upload model checkpoint to the Hub.\\n\\nUse `allow_patterns` and `ignore_patterns` to precisely filter which files should be pushed to the hub. Use\\n`delete_patterns` to delete existing remote files in the same commit. See [`upload_folder`] reference for more\\ndetails.\\n\\nArgs:\\n    repo_id (`str`):\\n        ID of the repository to push to (example: `\"username/my-model\"`).\\n    config (`dict` or `DataclassInstance`, *optional*):\\n        Model configuration specified as a key/value dictionary or a dataclass instance.\\n    commit_message (`str`, *optional*):\\n        Message to commit while pushing.\\n    private (`bool`, *optional*):\\n        Whether the repository created should be private.\\n        If `None` (default), the repo will be public unless the organization's default is private.\\n    token (`str`   \n",
       "10              Save weights in local directory.\\n\\nArgs:\\n    save_directory (`str` or `Path`):\\n        Path to directory in which the model weights and configuration will be saved.\\n    config (`dict` or `DataclassInstance`, *optional*):\\n        Model configuration specified as a key/value dictionary or a dataclass instance.\\n    push_to_hub (`bool`, *optional*, defaults to `False`):\\n        Whether or not to push your model to the Huggingface Hub after saving it.\\n    repo_id (`str`, *optional*):\\n        ID of your repository on the Hub. Used only if `push_to_hub=True`. Will default to the folder name if\\n        not provided.\\n    model_card_kwargs (`Dict[str, Any]`, *optional*):\\n        Additional arguments passed to the model card template to customize the model card.\\n    push_to_hub_kwargs:\\n        Additio   \n",
       "16                                                                                            Special typing form to define literal types (a.k.a. value types).\\n\\nThis form can be used to indicate to type checkers that the corresponding\\nvariable or function parameter has a value equivalent to the provided\\nliteral (or one of several literals)::\\n\\n    def validate_simple(data: Any) -> Literal[True]:  # always returns True\\n        ...\\n\\n    MODE = Literal['r', 'rb', 'w', 'wb']\\n    def open_helper(file: str, mode: MODE) -> str:\\n        ...\\n\\n    open_helper('/some/path', 'r')  # Passes type check\\n    open_helper('/other/path', 'typo')  # Error in type checker\\n\\nLiteral[...] cannot be subclassed. At runtime, an arbitrary value\\nis allowed as type argument to Literal[...], but type checkers may\\nimpose restrictions.   \n",
       "17               Options for forecasting.\\n\\nAttributes:\\n  max_context: The maximum context length. This is used by the complied decode\\n    function at inference time during batched inference. Any input time series\\n    with length less than max_context will be padded with zeros, and with\\n    length greater than max_context will be truncated.\\n  max_horizon: The maximum horizon length. This is used by the complied decode\\n    function at inference time during batched inference. The compiled cached\\n    decoding function will by default forecast till max_horizon.\\n  normalize_inputs: Whether to normalize the inputs. This is useful when the\\n    raw inputs are of extremely large or small magnitudes which may result in\\n    numerical issues.\\n  window_size: The window size for decomposed forecasting.\\n    TODO(siriuz42)   \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Framework-agnostic config for random fourier features.   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Framework-agnostic config for a residual block.   \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Framework-agnostic config for a stacked transformers.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Framework-agnostic config for a transformer.   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Random Fourier features layer.   \n",
       "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Residual block with two linear layers and a linear residual connection.   \n",
       "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
       "34   Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -> None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    >>> @torch.no_grad()\\n    >>> def init_weights(m):\\n    >>>     print(m)\\n    >>>     if type(m) is nn.Linear:\\n    >>>         m.weight.fill_(1.0)\\n    >>>         print(m.weight)\\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    >>> net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi   \n",
       "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "36                                                                                                                                                                                                                                                                                                                                                       Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for buf in model.buffers():\\n    >>>     print(type(buf), buf.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module   \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.   \n",
       "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "40                                                                                                                                                                                                                                                                                                                                                                                                               Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "42                                                                                                                                                                                                                                                                                                                             Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self   \n",
       "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.   \n",
       "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.   \n",
       "46                                                                                                                                                                                                                         Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer   \n",
       "47                                                                                                                                                                                                                                                     Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict   \n",
       "48                                                                                                                                                                                               Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``   \n",
       "49      Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"   \n",
       "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "51                                                                                                                                                                                                                                                                                                                                                                                                          Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "52            Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional   \n",
       "53                                                                                                                                                                                                                                            Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -> Linear(in_features=2, out_features=2, bias=True)   \n",
       "54                                                                                                                                                                                                                                                                                                                                                                                                        Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "55                                                                                  Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, buf in self.named_buffers():\\n    >>>     if name in ['running_var']:\\n    >>>         print(buf.size())   \n",
       "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, module in model.named_children():\\n    >>>     if name in ['conv4', 'conv5']:\\n    >>>         print(module)   \n",
       "57     Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear   \n",
       "58                                                                                Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, param in self.named_parameters():\\n    >>>     if name in ['bias']:\\n    >>>         print(param.size())   \n",
       "59                                                                                                                                                                                                                                                                                        Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for param in model.parameters():\\n    >>>     print(type(param), param.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``   \n",
       "61            Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff   \n",
       "62            Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -> None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h   \n",
       "63            Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -> None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.   \n",
       "64              Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but    \n",
       "65           Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -> tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n   \n",
       "66              Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -> None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions   \n",
       "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.   \n",
       "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Alias for :func:`add_module`.   \n",
       "69                                                                                                                                                                                                                                                                                                                                                   Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.   \n",
       "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -> None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.   \n",
       "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -> None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.   \n",
       "72                                                                                                                                                                                                                Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`   \n",
       "74     Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li   \n",
       "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      See :meth:`torch.Tensor.share_memory_`.   \n",
       "76        Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o   \n",
       "77       Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving    \n",
       "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self   \n",
       "79                                                                                                                                                                                                                                                                                                                                                                                                            Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self   \n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                              Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.   \n",
       "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
       "84   Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -> None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    >>> @torch.no_grad()\\n    >>> def init_weights(m):\\n    >>>     print(m)\\n    >>>     if type(m) is nn.Linear:\\n    >>>         m.weight.fill_(1.0)\\n    >>>         print(m.weight)\\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    >>> net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi   \n",
       "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "86                                                                                                                                                                                                                                                                                                                                                       Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for buf in model.buffers():\\n    >>>     print(type(buf), buf.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module   \n",
       "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.   \n",
       "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "90                                                                                                                                                                                                                                                                                                                                                                                                               Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "92                                                                                                                                                                                                                                                                                                                             Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self   \n",
       "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.   \n",
       "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.   \n",
       "96                                                                                                                                                                                                                         Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer   \n",
       "97                                                                                                                                                                                                                                                     Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict   \n",
       "98                                                                                                                                                                                               Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``   \n",
       "99      Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"   \n",
       "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "101                                                                                                                                                                                                                                                                                                                                                                                                         Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "102           Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional   \n",
       "103                                                                                                                                                                                                                                           Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -> Linear(in_features=2, out_features=2, bias=True)   \n",
       "104                                                                                                                                                                                                                                                                                                                                                                                                       Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "105                                                                                 Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, buf in self.named_buffers():\\n    >>>     if name in ['running_var']:\\n    >>>         print(buf.size())   \n",
       "106                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, module in model.named_children():\\n    >>>     if name in ['conv4', 'conv5']:\\n    >>>         print(module)   \n",
       "107    Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear   \n",
       "108                                                                               Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, param in self.named_parameters():\\n    >>>     if name in ['bias']:\\n    >>>         print(param.size())   \n",
       "109                                                                                                                                                                                                                                                                                       Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for param in model.parameters():\\n    >>>     print(type(param), param.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "110                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``   \n",
       "111           Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff   \n",
       "112           Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -> None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h   \n",
       "113           Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -> None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.   \n",
       "114             Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but    \n",
       "115          Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -> tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n   \n",
       "116             Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -> None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions   \n",
       "117                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.   \n",
       "118                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Alias for :func:`add_module`.   \n",
       "119                                                                                                                                                                                                                                                                                                                                                  Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.   \n",
       "120                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -> None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.   \n",
       "121                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -> None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.   \n",
       "122                                                                                                                                                                                                               Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`   \n",
       "124    Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li   \n",
       "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     See :meth:`torch.Tensor.share_memory_`.   \n",
       "126       Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o   \n",
       "127      Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving    \n",
       "128                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self   \n",
       "129                                                                                                                                                                                                                                                                                                                                                                                                           Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "130                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self   \n",
       "131                                                                                                                                                                                                                                                                                                                                                                                                             Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "132                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.   \n",
       "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          RMS normalization.   \n",
       "137                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
       "138  Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -> None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    >>> @torch.no_grad()\\n    >>> def init_weights(m):\\n    >>>     print(m)\\n    >>>     if type(m) is nn.Linear:\\n    >>>         m.weight.fill_(1.0)\\n    >>>         print(m.weight)\\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    >>> net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi   \n",
       "139                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "140                                                                                                                                                                                                                                                                                                                                                      Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for buf in model.buffers():\\n    >>>     print(type(buf), buf.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "141                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module   \n",
       "142                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.   \n",
       "143                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "144                                                                                                                                                                                                                                                                                                                                                                                                              Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "146                                                                                                                                                                                                                                                                                                                            Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self   \n",
       "147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.   \n",
       "148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "149                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.   \n",
       "150                                                                                                                                                                                                                        Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer   \n",
       "151                                                                                                                                                                                                                                                    Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict   \n",
       "152                                                                                                                                                                                              Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``   \n",
       "153     Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"   \n",
       "154                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "155                                                                                                                                                                                                                                                                                                                                                                                                         Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "156           Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional   \n",
       "157                                                                                                                                                                                                                                           Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -> Linear(in_features=2, out_features=2, bias=True)   \n",
       "158                                                                                                                                                                                                                                                                                                                                                                                                       Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "159                                                                                 Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, buf in self.named_buffers():\\n    >>>     if name in ['running_var']:\\n    >>>         print(buf.size())   \n",
       "160                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, module in model.named_children():\\n    >>>     if name in ['conv4', 'conv5']:\\n    >>>         print(module)   \n",
       "161    Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear   \n",
       "162                                                                               Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, param in self.named_parameters():\\n    >>>     if name in ['bias']:\\n    >>>         print(param.size())   \n",
       "163                                                                                                                                                                                                                                                                                       Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for param in model.parameters():\\n    >>>     print(type(param), param.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "164                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``   \n",
       "165           Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff   \n",
       "166           Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -> None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h   \n",
       "167           Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -> None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.   \n",
       "168             Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but    \n",
       "169          Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -> tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n   \n",
       "170             Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -> None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions   \n",
       "171                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.   \n",
       "172                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Alias for :func:`add_module`.   \n",
       "173                                                                                                                                                                                                                                                                                                                                                  Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.   \n",
       "174                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -> None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.   \n",
       "175                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -> None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.   \n",
       "176                                                                                                                                                                                                               Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "177                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`   \n",
       "178    Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li   \n",
       "179                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     See :meth:`torch.Tensor.share_memory_`.   \n",
       "180       Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o   \n",
       "181      Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving    \n",
       "182                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self   \n",
       "183                                                                                                                                                                                                                                                                                                                                                                                                           Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self   \n",
       "185                                                                                                                                                                                                                                                                                                                                                                                                             Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.   \n",
       "189                                                                                                                                                                                                                                                                                                                                 Deprecated alias to collections.abc.Callable.\\n\\nCallable[[int], str] signifies a function that takes a single\\nparameter of type int and returns a str.\\n\\nThe subscription syntax must always be used with exactly two\\nvalues: the argument list and the return type.\\nThe argument list must be a list of types, a ParamSpec,\\nConcatenate or ellipsis. The return type must be a single type.\\n\\nThere is no syntax to indicate optional or keyword arguments;\\nsuch function types are rarely used as callback types.   \n",
       "190                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Cache for decoding.   \n",
       "191              Applies Layer Normalization over a mini-batch of inputs.\\n\\nThis layer implements the operation as described in\\nthe paper `Layer Normalization <https://arxiv.org/abs/1607.06450>`__\\n\\n.. math::\\n    y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\n\\nThe mean and standard-deviation are calculated over the last `D` dimensions, where `D`\\nis the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape`\\nis ``(3, 5)`` (a 2-dimensional shape), the mean and standard-deviation are computed over\\nthe last 2 dimensions of the input (i.e. ``input.mean((-2, -1))``).\\n:math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of\\n:attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\\nThe variance is calculated via the biased esti   \n",
       "192                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Multi-head attention.   \n",
       "193                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Per-dimension scaling.   \n",
       "194                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          RMS normalization.   \n",
       "195                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Rotary positional embedding.   \n",
       "196                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Classic Transformer used in TimesFM.   \n",
       "197                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Makes attention mask.   \n",
       "198                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
       "199  Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -> None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    >>> @torch.no_grad()\\n    >>> def init_weights(m):\\n    >>>     print(m)\\n    >>>     if type(m) is nn.Linear:\\n    >>>         m.weight.fill_(1.0)\\n    >>>         print(m.weight)\\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    >>> net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi   \n",
       "200                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "201                                                                                                                                                                                                                                                                                                                                                      Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for buf in model.buffers():\\n    >>>     print(type(buf), buf.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "202                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module   \n",
       "203                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.   \n",
       "204                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "205                                                                                                                                                                                                                                                                                                                                                                                                              Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "206                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "207                                                                                                                                                                                                                                                                                                                            Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self   \n",
       "208                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.   \n",
       "209                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "210                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.   \n",
       "211                                                                                                                                                                                                                        Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer   \n",
       "212                                                                                                                                                                                                                                                    Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict   \n",
       "213                                                                                                                                                                                              Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``   \n",
       "214     Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"   \n",
       "215                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "216                                                                                                                                                                                                                                                                                                                                                                                                         Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "217           Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional   \n",
       "218                                                                                                                                                                                                                                           Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -> Linear(in_features=2, out_features=2, bias=True)   \n",
       "219                                                                                                                                                                                                                                                                                                                                                                                                       Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "220                                                                                 Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, buf in self.named_buffers():\\n    >>>     if name in ['running_var']:\\n    >>>         print(buf.size())   \n",
       "221                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, module in model.named_children():\\n    >>>     if name in ['conv4', 'conv5']:\\n    >>>         print(module)   \n",
       "222    Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear   \n",
       "223                                                                               Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, param in self.named_parameters():\\n    >>>     if name in ['bias']:\\n    >>>         print(param.size())   \n",
       "224                                                                                                                                                                                                                                                                                       Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for param in model.parameters():\\n    >>>     print(type(param), param.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "225                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``   \n",
       "226           Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff   \n",
       "227           Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -> None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h   \n",
       "228           Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -> None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.   \n",
       "229             Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but    \n",
       "230          Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -> tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n   \n",
       "231             Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -> None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions   \n",
       "232                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.   \n",
       "233                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Alias for :func:`add_module`.   \n",
       "234                                                                                                                                                                                                                                                                                                                                                  Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.   \n",
       "235                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -> None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.   \n",
       "236                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -> None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.   \n",
       "237                                                                                                                                                                                                               Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "238                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "239                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`   \n",
       "240    Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li   \n",
       "241                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     See :meth:`torch.Tensor.share_memory_`.   \n",
       "242       Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o   \n",
       "243      Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving    \n",
       "244                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self   \n",
       "245                                                                                                                                                                                                                                                                                                                                                                                                           Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "246                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self   \n",
       "247                                                                                                                                                                                                                                                                                                                                                                                                             Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "248                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.   \n",
       "249                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
       "250  Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -> None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    >>> @torch.no_grad()\\n    >>> def init_weights(m):\\n    >>>     print(m)\\n    >>>     if type(m) is nn.Linear:\\n    >>>         m.weight.fill_(1.0)\\n    >>>         print(m.weight)\\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    >>> net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi   \n",
       "251                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "252                                                                                                                                                                                                                                                                                                                                                      Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for buf in model.buffers():\\n    >>>     print(type(buf), buf.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "253                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module   \n",
       "254                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.   \n",
       "255                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "256                                                                                                                                                                                                                                                                                                                                                                                                              Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "257                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "258                                                                                                                                                                                                                                                                                                                            Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self   \n",
       "259                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.   \n",
       "260                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "261                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.   \n",
       "262                                                                                                                                                                                                                        Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer   \n",
       "263                                                                                                                                                                                                                                                    Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict   \n",
       "264                                                                                                                                                                                              Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``   \n",
       "265     Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"   \n",
       "266                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "267                                                                                                                                                                                                                                                                                                                                                                                                         Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "268           Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional   \n",
       "269                                                                                                                                                                                                                                           Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -> Linear(in_features=2, out_features=2, bias=True)   \n",
       "270                                                                                                                                                                                                                                                                                                                                                                                                       Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "271                                                                                 Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, buf in self.named_buffers():\\n    >>>     if name in ['running_var']:\\n    >>>         print(buf.size())   \n",
       "272                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, module in model.named_children():\\n    >>>     if name in ['conv4', 'conv5']:\\n    >>>         print(module)   \n",
       "273    Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear   \n",
       "274                                                                               Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, param in self.named_parameters():\\n    >>>     if name in ['bias']:\\n    >>>         print(param.size())   \n",
       "275                                                                                                                                                                                                                                                                                       Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for param in model.parameters():\\n    >>>     print(type(param), param.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "276                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``   \n",
       "277           Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff   \n",
       "278           Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -> None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h   \n",
       "279           Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -> None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.   \n",
       "280             Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but    \n",
       "281          Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -> tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n   \n",
       "282             Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -> None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions   \n",
       "283                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.   \n",
       "284                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Alias for :func:`add_module`.   \n",
       "285                                                                                                                                                                                                                                                                                                                                                  Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.   \n",
       "286                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -> None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.   \n",
       "287                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -> None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.   \n",
       "288                                                                                                                                                                                                               Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "289                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`   \n",
       "290    Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li   \n",
       "291                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     See :meth:`torch.Tensor.share_memory_`.   \n",
       "292       Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o   \n",
       "293      Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving    \n",
       "294                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self   \n",
       "295                                                                                                                                                                                                                                                                                                                                                                                                           Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self   \n",
       "297                                                                                                                                                                                                                                                                                                                                                                                                             Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "298                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.   \n",
       "299                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
       "300  Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -> None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    >>> @torch.no_grad()\\n    >>> def init_weights(m):\\n    >>>     print(m)\\n    >>>     if type(m) is nn.Linear:\\n    >>>         m.weight.fill_(1.0)\\n    >>>         print(m.weight)\\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    >>> net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi   \n",
       "301                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "302                                                                                                                                                                                                                                                                                                                                                      Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for buf in model.buffers():\\n    >>>     print(type(buf), buf.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "303                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module   \n",
       "304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.   \n",
       "305                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "306                                                                                                                                                                                                                                                                                                                                                                                                              Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "307                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "308                                                                                                                                                                                                                                                                                                                            Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self   \n",
       "309                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.   \n",
       "310                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "311                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.   \n",
       "312                                                                                                                                                                                                                        Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer   \n",
       "313                                                                                                                                                                                                                                                    Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict   \n",
       "314                                                                                                                                                                                              Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``   \n",
       "315     Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"   \n",
       "316                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "317                                                                                                                                                                                                                                                                                                                                                                                                         Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "318           Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional   \n",
       "319                                                                                                                                                                                                                                           Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -> Linear(in_features=2, out_features=2, bias=True)   \n",
       "320                                                                                                                                                                                                                                                                                                                                                                                                       Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "321                                                                                 Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, buf in self.named_buffers():\\n    >>>     if name in ['running_var']:\\n    >>>         print(buf.size())   \n",
       "322                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, module in model.named_children():\\n    >>>     if name in ['conv4', 'conv5']:\\n    >>>         print(module)   \n",
       "323    Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear   \n",
       "324                                                                               Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, param in self.named_parameters():\\n    >>>     if name in ['bias']:\\n    >>>         print(param.size())   \n",
       "325                                                                                                                                                                                                                                                                                       Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for param in model.parameters():\\n    >>>     print(type(param), param.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "326                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``   \n",
       "327           Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff   \n",
       "328           Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -> None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h   \n",
       "329           Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -> None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.   \n",
       "330             Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but    \n",
       "331          Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -> tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n   \n",
       "332             Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -> None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions   \n",
       "333                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.   \n",
       "334                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Alias for :func:`add_module`.   \n",
       "335                                                                                                                                                                                                                                                                                                                                                  Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.   \n",
       "336                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -> None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.   \n",
       "337                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -> None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.   \n",
       "338                                                                                                                                                                                                               Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "339                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`   \n",
       "340    Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li   \n",
       "341                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     See :meth:`torch.Tensor.share_memory_`.   \n",
       "342       Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o   \n",
       "343      Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving    \n",
       "344                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self   \n",
       "345                                                                                                                                                                                                                                                                                                                                                                                                           Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "346                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self   \n",
       "347                                                                                                                                                                                                                                                                                                                                                                                                             Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "348                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.   \n",
       "349                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
       "350  Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -> None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    >>> @torch.no_grad()\\n    >>> def init_weights(m):\\n    >>>     print(m)\\n    >>>     if type(m) is nn.Linear:\\n    >>>         m.weight.fill_(1.0)\\n    >>>         print(m.weight)\\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    >>> net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi   \n",
       "351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "352                                                                                                                                                                                                                                                                                                                                                      Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for buf in model.buffers():\\n    >>>     print(type(buf), buf.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "353                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module   \n",
       "354                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.   \n",
       "355                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "356                                                                                                                                                                                                                                                                                                                                                                                                              Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "357                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "358                                                                                                                                                                                                                                                                                                                            Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self   \n",
       "359                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.   \n",
       "360                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "361                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.   \n",
       "362                                                                                                                                                                                                                        Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer   \n",
       "363                                                                                                                                                                                                                                                    Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict   \n",
       "364                                                                                                                                                                                              Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``   \n",
       "365     Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"   \n",
       "366                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "367                                                                                                                                                                                                                                                                                                                                                                                                         Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "368           Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional   \n",
       "369                                                                                                                                                                                                                                           Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -> Linear(in_features=2, out_features=2, bias=True)   \n",
       "370                                                                                                                                                                                                                                                                                                                                                                                                       Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "371                                                                                 Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, buf in self.named_buffers():\\n    >>>     if name in ['running_var']:\\n    >>>         print(buf.size())   \n",
       "372                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, module in model.named_children():\\n    >>>     if name in ['conv4', 'conv5']:\\n    >>>         print(module)   \n",
       "373    Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear   \n",
       "374                                                                               Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, param in self.named_parameters():\\n    >>>     if name in ['bias']:\\n    >>>         print(param.size())   \n",
       "375                                                                                                                                                                                                                                                                                       Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for param in model.parameters():\\n    >>>     print(type(param), param.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "376                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``   \n",
       "377           Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff   \n",
       "378           Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -> None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h   \n",
       "379           Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -> None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.   \n",
       "380             Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but    \n",
       "381          Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -> tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n   \n",
       "382             Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -> None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions   \n",
       "383                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.   \n",
       "384                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Alias for :func:`add_module`.   \n",
       "385                                                                                                                                                                                                                                                                                                                                                  Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.   \n",
       "386                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -> None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.   \n",
       "387                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -> None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.   \n",
       "388                                                                                                                                                                                                               Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "389                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`   \n",
       "390    Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li   \n",
       "391                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     See :meth:`torch.Tensor.share_memory_`.   \n",
       "392       Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o   \n",
       "393      Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving    \n",
       "394                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self   \n",
       "395                                                                                                                                                                                                                                                                                                                                                                                                           Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "396                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self   \n",
       "397                                                                                                                                                                                                                                                                                                                                                                                                             Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "398                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.   \n",
       "399                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
       "400  Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -> None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    >>> @torch.no_grad()\\n    >>> def init_weights(m):\\n    >>>     print(m)\\n    >>>     if type(m) is nn.Linear:\\n    >>>         m.weight.fill_(1.0)\\n    >>>         print(m.weight)\\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    >>> net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi   \n",
       "401                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "402                                                                                                                                                                                                                                                                                                                                                      Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for buf in model.buffers():\\n    >>>     print(type(buf), buf.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "403                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module   \n",
       "404                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.   \n",
       "405                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "406                                                                                                                                                                                                                                                                                                                                                                                                              Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "407                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "408                                                                                                                                                                                                                                                                                                                            Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self   \n",
       "409                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.   \n",
       "410                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "411                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Generates a JTensor of sinusoids with different frequencies.   \n",
       "412                                                                                                                                                                                                                        Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer   \n",
       "413                                                                                                                                                                                                                                                    Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict   \n",
       "414                                                                                                                                                                                              Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``   \n",
       "415     Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"   \n",
       "416                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "417                                                                                                                                                                                                                                                                                                                                                                                                         Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "418           Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional   \n",
       "419                                                                                                                                                                                                                                           Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -> Linear(in_features=2, out_features=2, bias=True)   \n",
       "420                                                                                                                                                                                                                                                                                                                                                                                                       Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "421                                                                                 Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, buf in self.named_buffers():\\n    >>>     if name in ['running_var']:\\n    >>>         print(buf.size())   \n",
       "422                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, module in model.named_children():\\n    >>>     if name in ['conv4', 'conv5']:\\n    >>>         print(module)   \n",
       "423    Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear   \n",
       "424                                                                               Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, param in self.named_parameters():\\n    >>>     if name in ['bias']:\\n    >>>         print(param.size())   \n",
       "425                                                                                                                                                                                                                                                                                       Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for param in model.parameters():\\n    >>>     print(type(param), param.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "426                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``   \n",
       "427           Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff   \n",
       "428           Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -> None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h   \n",
       "429           Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -> None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.   \n",
       "430             Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but    \n",
       "431          Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -> tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n   \n",
       "432             Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -> None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions   \n",
       "433                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.   \n",
       "434                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Alias for :func:`add_module`.   \n",
       "435                                                                                                                                                                                                                                                                                                                                                  Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.   \n",
       "436                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -> None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.   \n",
       "437                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -> None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.   \n",
       "438                                                                                                                                                                                                               Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "439                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`   \n",
       "440    Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li   \n",
       "441                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     See :meth:`torch.Tensor.share_memory_`.   \n",
       "442       Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o   \n",
       "443      Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving    \n",
       "444                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self   \n",
       "445                                                                                                                                                                                                                                                                                                                                                                                                           Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "446                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self   \n",
       "447                                                                                                                                                                                                                                                                                                                                                                                                             Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "448                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.   \n",
       "449                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Add a child module to the current module.\\n\\nThe module can be accessed as an attribute using the given name.\\n\\nArgs:\\n    name (str): name of the child module. The child module can be\\n        accessed from this module using the given name\\n    module (Module): child module to be added to the module.   \n",
       "450  Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\\n\\nTypical use includes initializing the parameters of a model\\n(see also :ref:`nn-init-doc`).\\n\\nArgs:\\n    fn (:class:`Module` -> None): function to be applied to each submodule\\n\\nReturns:\\n    Module: self\\n\\nExample::\\n\\n    >>> @torch.no_grad()\\n    >>> def init_weights(m):\\n    >>>     print(m)\\n    >>>     if type(m) is nn.Linear:\\n    >>>         m.weight.fill_(1.0)\\n    >>>         print(m.weight)\\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\\n    >>> net.apply(init_weights)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containing:\\n    tensor([[1., 1.],\\n            [1., 1.]], requires_grad=True)\\n    Linear(in_features=2, out_features=2, bias=True)\\n    Parameter containi   \n",
       "451                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Casts all floating point parameters and buffers to ``bfloat16`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "452                                                                                                                                                                                                                                                                                                                                                      Return an iterator over module buffers.\\n\\nArgs:\\n    recurse (bool): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module.\\n\\nYields:\\n    torch.Tensor: module buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for buf in model.buffers():\\n    >>>     print(type(buf), buf.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "453                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules.\\n\\nYields:\\n    Module: a child module   \n",
       "454                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Compile this Module's forward using :func:`torch.compile`.\\n\\nThis Module's `__call__` method is compiled and all arguments are passed as-is\\nto :func:`torch.compile`.\\n\\nSee :func:`torch.compile` for details on the arguments for this function.   \n",
       "455                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Move all model parameters and buffers to the CPU.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "456                                                                                                                                                                                                                                                                                                                                                                                                              Move all model parameters and buffers to the GPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "457                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Casts all floating point parameters and buffers to ``double`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "458                                                                                                                                                                                                                                                                                                                            Set the module in evaluation mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.eval()` and several similar mechanisms that may be confused with it.\\n\\nReturns:\\n    Module: self   \n",
       "459                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Return the extra representation of the module.\\n\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.   \n",
       "460                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Casts all floating point parameters and buffers to ``float`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "461                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Define the computation performed at every call.\\n\\nShould be overridden by all subclasses.\\n\\n.. note::\\n    Although the recipe for forward pass needs to be defined within\\n    this function, one should call the :class:`Module` instance afterwards\\n    instead of this since the former takes care of running the\\n    registered hooks while the latter silently ignores them.   \n",
       "462                                                                                                                                                                                                                        Return the buffer given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the buffer\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.Tensor: The buffer referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not a\\n        buffer   \n",
       "463                                                                                                                                                                                                                                                    Return any extra state to include in the module's state_dict.\\n\\nImplement this and a corresponding :func:`set_extra_state` for your module\\nif you need to store extra state. This function is called when building the\\nmodule's `state_dict()`.\\n\\nNote that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\n\\nReturns:\\n    object: Any extra state to store in the module's state_dict   \n",
       "464                                                                                                                                                                                              Return the parameter given by ``target`` if it exists, otherwise throw an error.\\n\\nSee the docstring for ``get_submodule`` for a more detailed\\nexplanation of this method's functionality as well as how to\\ncorrectly specify ``target``.\\n\\nArgs:\\n    target: The fully-qualified string name of the Parameter\\n        to look for. (See ``get_submodule`` for how to specify a\\n        fully-qualified string.)\\n\\nReturns:\\n    torch.nn.Parameter: The Parameter referenced by ``target``\\n\\nRaises:\\n    AttributeError: If the target string references an invalid\\n        path or resolves to something that is not an\\n        ``nn.Parameter``   \n",
       "465     Return the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n            )\\n            (linear): Linear(in_features=100, out_features=200, bias=True)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\\n\\nTo check whether or not we have the ``linear`` submodule, we\\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\\nwe have the ``conv`` submodule, we would call\\n``get_submodule(\"   \n",
       "466                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Casts all floating point parameters and buffers to ``half`` datatype.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nReturns:\\n    Module: self   \n",
       "467                                                                                                                                                                                                                                                                                                                                                                                                         Move all model parameters and buffers to the IPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "468           Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\\n\\nIf :attr:`strict` is ``True``, then\\nthe keys of :attr:`state_dict` must exactly match the keys returned\\nby this module's :meth:`~torch.nn.Module.state_dict` function.\\n\\n.. warning::\\n    If :attr:`assign` is ``True`` the optimizer must be created after\\n    the call to :attr:`load_state_dict` unless\\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\\n\\nArgs:\\n    state_dict (dict): a dict containing parameters and\\n        persistent buffers.\\n    strict (bool, optional): whether to strictly enforce that the keys\\n        in :attr:`state_dict` match the keys returned by this module's\\n        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n    assign (bool, optional   \n",
       "469                                                                                                                                                                                                                                           Return an iterator over all modules in the network.\\n\\nYields:\\n    Module: a module in the network\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear(in_features=2, out_features=2, bias=True)\\n    )\\n    1 -> Linear(in_features=2, out_features=2, bias=True)   \n",
       "470                                                                                                                                                                                                                                                                                                                                                                                                       Move all model parameters and buffers to the MTIA.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "471                                                                                 Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all buffer names.\\n    recurse (bool, optional): if True, then yields buffers of this module\\n        and all submodules. Otherwise, yields only buffers that\\n        are direct members of this module. Defaults to True.\\n    remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\\n\\nYields:\\n    (str, torch.Tensor): Tuple containing the name and buffer\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, buf in self.named_buffers():\\n    >>>     if name in ['running_var']:\\n    >>>         print(buf.size())   \n",
       "472                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\n\\nYields:\\n    (str, Module): Tuple containing a name and child module\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, module in model.named_children():\\n    >>>     if name in ['conv4', 'conv5']:\\n    >>>         print(module)   \n",
       "473    Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\n\\nArgs:\\n    memo: a memo to store the set of modules already added to the result\\n    prefix: a prefix that will be added to the name of the module\\n    remove_duplicate: whether to remove the duplicated module instances in the result\\n        or not\\n\\nYields:\\n    (str, Module): Tuple of name and module\\n\\nNote:\\n    Duplicate modules are returned only once. In the following\\n    example, ``l`` will be returned only once.\\n\\nExample::\\n\\n    >>> l = nn.Linear(2, 2)\\n    >>> net = nn.Sequential(l, l)\\n    >>> for idx, m in enumerate(net.named_modules()):\\n    ...     print(idx, '->', m)\\n\\n    0 -> ('', Sequential(\\n      (0): Linear(in_features=2, out_features=2, bias=True)\\n      (1): Linear   \n",
       "474                                                                               Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\n\\nArgs:\\n    prefix (str): prefix to prepend to all parameter names.\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n    remove_duplicate (bool, optional): whether to remove the duplicated\\n        parameters in the result. Defaults to True.\\n\\nYields:\\n    (str, Parameter): Tuple containing the name and parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for name, param in self.named_parameters():\\n    >>>     if name in ['bias']:\\n    >>>         print(param.size())   \n",
       "475                                                                                                                                                                                                                                                                                       Return an iterator over module parameters.\\n\\nThis is typically passed to an optimizer.\\n\\nArgs:\\n    recurse (bool): if True, then yields parameters of this module\\n        and all submodules. Otherwise, yields only parameters that\\n        are direct members of this module.\\n\\nYields:\\n    Parameter: module parameter\\n\\nExample::\\n\\n    >>> # xdoctest: +SKIP(\"undefined vars\")\\n    >>> for param in model.parameters():\\n    >>>     print(type(param), param.size())\\n    <class 'torch.Tensor'> (20L,)\\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)   \n",
       "476                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Register a backward hook on the module.\\n\\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\\nthe behavior of this function will change in future versions.\\n\\nReturns:\\n    :class:`torch.utils.hooks.RemovableHandle`:\\n        a handle that can be used to remove the added hook by calling\\n        ``handle.remove()``   \n",
       "477           Add a buffer to the module.\\n\\nThis is typically used to register a buffer that should not be\\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\\nis not a parameter, but is part of the module's state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module's\\n:attr:`state_dict`.\\n\\nBuffers can be accessed as attributes using given names.\\n\\nArgs:\\n    name (str): name of the buffer. The buffer can be accessed\\n        from this module using the given name\\n    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\\n        that run on buff   \n",
       "478           Register a forward hook on the module.\\n\\nThe hook will be called every time after :func:`forward` has computed an output.\\n\\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after :func:`forward` is called. The hook\\nshould have the following signature::\\n\\n    hook(module, args, output) -> None or modified output\\n\\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\\n``kwargs`` given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature::\\n\\n    h   \n",
       "479           Register a forward pre-hook on the module.\\n\\nThe hook will be called every time before :func:`forward` is invoked.\\n\\n\\nIf ``with_kwargs`` is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won't be\\npassed to the hooks and only to the ``forward``. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature::\\n\\n    hook(module, args) -> None or modified input\\n\\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned.   \n",
       "480             Register a backward hook on the module.\\n\\nThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\\n\\n    1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\\n    2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\\n       with respect to module outputs.\\n    3. If none of the module outputs require gradients, then the hooks will not fire.\\n\\nThe hook should have the following signature::\\n\\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\\n\\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but    \n",
       "481          Register a backward pre-hook on the module.\\n\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature::\\n\\n    hook(module, grad_output) -> tuple[Tensor, ...], Tensor or None\\n\\nThe :attr:`grad_output` is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of :attr:`grad_output` in\\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\\nall non-Tensor arguments.\\n\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module's forward function.\\n\\n.. warning ::\\n   \n",
       "482             Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, incompatible_keys) -> None\\n\\nThe ``module`` argument is the current module that this hook is registered\\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\\nis a ``list`` of ``str`` containing the missing keys and\\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\\n\\nThe given incompatible_keys can be modified inplace if needed.\\n\\nNote that the checks performed when calling :func:`load_state_dict` with\\n``strict=True`` are affected by modifications the hook makes to\\n``missing_keys`` or ``unexpected_keys``, as expected. Additions   \n",
       "483                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\n\\nArguments:\\n    hook (Callable): Callable hook that will be invoked before\\n        loading the state dict.   \n",
       "484                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Alias for :func:`add_module`.   \n",
       "485                                                                                                                                                                                                                                                                                                                                                  Add a parameter to the module.\\n\\nThe parameter can be accessed as an attribute using given name.\\n\\nArgs:\\n    name (str): name of the parameter. The parameter can be accessed\\n        from this module using the given name\\n    param (Parameter or None): parameter to be added to the module. If\\n        ``None``, then operations that run on parameters, such as :attr:`cuda`,\\n        are ignored. If ``None``, the parameter is **not** included in the\\n        module's :attr:`state_dict`.   \n",
       "486                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, state_dict, prefix, local_metadata) -> None\\n\\nThe registered hooks can modify the ``state_dict`` inplace.   \n",
       "487                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\\n\\nIt should have the following signature::\\n    hook(module, prefix, keep_vars) -> None\\n\\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\\ncall is made.   \n",
       "488                                                                                                                                                                                                               Change if autograd should record operations on parameters in this module.\\n\\nThis method sets the parameters' :attr:`requires_grad` attributes\\nin-place.\\n\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\n\\nSee :ref:`locally-disable-grad-doc` for a comparison between\\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\\n\\nArgs:\\n    requires_grad (bool): whether autograd should record operations on\\n                          parameters in this module. Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "489                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Set extra state contained in the loaded `state_dict`.\\n\\nThis function is called from :func:`load_state_dict` to handle any extra state\\nfound within the `state_dict`. Implement this function and a corresponding\\n:func:`get_extra_state` for your module if you need to store extra state within its\\n`state_dict`.\\n\\nArgs:\\n    state (dict): Extra state from the `state_dict`   \n",
       "490    Set the submodule given by ``target`` if it exists, otherwise throw an error.\\n\\n.. note::\\n    If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\\n    or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\\n    the method will only attempt to replace an existing submodule and throw an error if\\n    the submodule does not exist.\\n\\nFor example, let's say you have an ``nn.Module`` ``A`` that\\nlooks like this:\\n\\n.. code-block:: text\\n\\n    A(\\n        (net_b): Module(\\n            (net_c): Module(\\n                (conv): Conv2d(3, 3, 3)\\n            )\\n            (linear): Linear(3, 3)\\n        )\\n    )\\n\\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\\nsubmodule ``net_b``, which itself has two submodules ``net_c``\\nand ``li   \n",
       "491                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     See :meth:`torch.Tensor.share_memory_`.   \n",
       "492       Return a dictionary containing references to the whole state of the module.\\n\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to ``None`` are not included.\\n\\n.. note::\\n    The returned object is a shallow copy. It contains references\\n    to the module's parameters and buffers.\\n\\n.. warning::\\n    Currently ``state_dict()`` also accepts positional arguments for\\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\\n    this is being deprecated and keyword arguments will be enforced in\\n    future releases.\\n\\n.. warning::\\n    Please avoid the use of argument ``destination`` as it is not\\n    designed for end-users.\\n\\nArgs:\\n    destination (dict, optional): If provided, the state o   \n",
       "493      Move and/or cast the parameters and buffers.\\n\\nThis can be called as\\n\\n.. function:: to(device=None, dtype=None, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(dtype, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(tensor, non_blocking=False)\\n   :noindex:\\n\\n.. function:: to(memory_format=torch.channels_last)\\n   :noindex:\\n\\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\\n(if given). The integral parameters and buffers will be moved\\n:attr:`device`, if that is given, but with dtypes unchanged. When\\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving    \n",
       "494                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Move the parameters and buffers to the specified device without copying storage.\\n\\nArgs:\\n    device (:class:`torch.device`): The desired device of the parameters\\n        and buffers in this module.\\n    recurse (bool): Whether parameters and buffers of submodules should\\n        be recursively moved to the specified device.\\n\\nReturns:\\n    Module: self   \n",
       "495                                                                                                                                                                                                                                                                                                                                                                                                           Set the module in training mode.\\n\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\\netc.\\n\\nArgs:\\n    mode (bool): whether to set training mode (``True``) or evaluation\\n                 mode (``False``). Default: ``True``.\\n\\nReturns:\\n    Module: self   \n",
       "496                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Casts all parameters and buffers to :attr:`dst_type`.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArgs:\\n    dst_type (type or string): the desired type\\n\\nReturns:\\n    Module: self   \n",
       "497                                                                                                                                                                                                                                                                                                                                                                                                             Move all model parameters and buffers to the XPU.\\n\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\n\\n.. note::\\n    This method modifies the module in-place.\\n\\nArguments:\\n    device (int, optional): if specified, all parameters will be\\n        copied to that device\\n\\nReturns:\\n    Module: self   \n",
       "498                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Reset gradients of all model parameters.\\n\\nSee similar function under :class:`torch.optim.Optimizer` for more context.\\n\\nArgs:\\n    set_to_none (bool): instead of setting to zero, set the grads to None.\\n        See :meth:`torch.optim.Optimizer.zero_grad` for details.   \n",
       "506                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Cache for decoding.   \n",
       "507                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Reversible instance normalization.   \n",
       "508                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Updates the running stats.   \n",
       "\n",
       "                                                                                                   file  \n",
       "0               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "1               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "2               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "3               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "4               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "5               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "6               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "7               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "8               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "9               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "10              c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py  \n",
       "16               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
       "17               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
       "18               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
       "19               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
       "20               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
       "21               c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\configs.py  \n",
       "31           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "32           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "33           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "34           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "35           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "36           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "37           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "38           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "39           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "40           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "41           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "42           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "43           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "44           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "45           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "46           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "47           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "48           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "49           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "50           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "51           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "52           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "53           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "54           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "55           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "56           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "57           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "58           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "59           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "60           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "61           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "62           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "63           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "64           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "65           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "66           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "67           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "68           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "69           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "70           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "71           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "72           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "73           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "74           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "75           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "76           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "77           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "78           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "79           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "80           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "81           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "82           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "83           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "84           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "85           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "86           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "87           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "88           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "89           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "90           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "91           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "92           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "93           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "94           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "95           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "96           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "97           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "98           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "99           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "100          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "101          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "102          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "103          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "104          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "105          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "106          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "107          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "108          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "109          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "110          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "111          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "112          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "113          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "114          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "115          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "116          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "117          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "118          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "119          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "120          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "121          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "122          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "123          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "124          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "125          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "126          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "127          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "128          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "129          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "130          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "131          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "132          c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\dense.py  \n",
       "136  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "137  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "138  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "139  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "140  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "141  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "142  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "143  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "144  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "145  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "146  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "147  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "148  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "149  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "150  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "151  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "152  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "153  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "154  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "155  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "156  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "157  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "158  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "159  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "160  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "161  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "162  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "163  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "164  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "165  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "166  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "167  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "168  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "169  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "170  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "171  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "172  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "173  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "174  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "175  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "176  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "177  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "178  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "179  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "180  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "181  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "182  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "183  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "184  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "185  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "186  c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\normalization.py  \n",
       "189    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "190    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "191    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "192    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "193    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "194    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "195    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "196    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "197    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "198    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "199    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "200    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "201    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "202    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "203    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "204    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "205    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "206    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "207    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "208    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "209    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "210    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "211    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "212    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "213    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "214    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "215    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "216    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "217    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "218    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "219    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "220    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "221    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "222    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "223    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "224    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "225    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "226    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "227    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "228    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "229    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "230    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "231    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "232    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "233    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "234    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "235    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "236    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "237    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "238    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "239    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "240    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "241    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "242    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "243    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "244    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "245    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "246    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "247    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "248    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "249    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "250    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "251    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "252    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "253    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "254    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "255    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "256    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "257    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "258    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "259    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "260    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "261    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "262    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "263    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "264    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "265    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "266    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "267    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "268    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "269    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "270    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "271    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "272    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "273    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "274    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "275    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "276    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "277    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "278    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "279    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "280    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "281    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "282    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "283    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "284    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "285    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "286    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "287    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "288    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "289    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "290    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "291    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "292    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "293    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "294    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "295    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "296    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "297    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "298    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "299    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "300    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "301    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "302    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "303    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "304    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "305    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "306    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "307    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "308    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "309    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "310    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "311    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "312    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "313    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "314    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "315    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "316    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "317    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "318    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "319    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "320    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "321    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "322    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "323    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "324    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "325    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "326    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "327    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "328    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "329    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "330    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "331    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "332    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "333    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "334    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "335    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "336    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "337    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "338    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "339    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "340    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "341    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "342    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "343    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "344    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "345    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "346    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "347    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "348    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "349    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "350    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "351    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "352    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "353    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "354    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "355    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "356    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "357    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "358    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "359    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "360    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "361    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "362    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "363    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "364    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "365    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "366    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "367    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "368    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "369    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "370    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "371    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "372    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "373    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "374    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "375    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "376    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "377    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "378    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "379    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "380    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "381    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "382    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "383    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "384    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "385    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "386    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "387    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "388    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "389    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "390    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "391    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "392    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "393    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "394    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "395    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "396    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "397    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "398    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "399    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "400    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "401    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "402    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "403    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "404    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "405    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "406    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "407    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "408    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "409    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "410    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "411    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "412    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "413    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "414    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "415    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "416    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "417    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "418    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "419    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "420    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "421    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "422    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "423    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "424    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "425    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "426    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "427    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "428    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "429    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "430    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "431    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "432    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "433    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "434    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "435    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "436    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "437    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "438    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "439    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "440    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "441    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "442    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "443    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "444    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "445    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "446    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "447    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "448    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "449    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "450    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "451    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "452    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "453    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "454    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "455    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "456    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "457    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "458    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "459    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "460    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "461    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "462    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "463    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "464    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "465    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "466    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "467    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "468    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "469    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "470    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "471    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "472    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "473    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "474    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "475    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "476    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "477    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "478    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "479    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "480    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "481    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "482    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "483    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "484    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "485    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "486    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "487    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "488    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "489    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "490    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "491    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "492    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "493    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "494    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "495    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "496    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "497    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "498    c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\transformer.py  \n",
       "506           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\util.py  \n",
       "507           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\util.py  \n",
       "508           c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\torch\\util.py  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b62f1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TimesFM Library Features\n",
      "================================================================================\n",
      "\n",
      "Total features found: 7\n",
      "\n",
      "\n",
      "                     name    type                                 module                                                                                                                                  methods                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  docstring\n",
      "0          ForecastConfig   class                        timesfm.configs                                                                                                                                           Options for forecasting.\\n\\nAttributes:\\n  max_context: The maximum context length. This is used by the complied decode\\n    function at inference time during batched inference. Any input time series\\n    with length less than max_context will be padded with zeros, and with\\n    length greater than max_context will be truncated.\\n  max_horizon: The maximum horizon length. This is used by the complied decode\\n    function at inference time during batched inference. The compiled cached\\n    decoding function will by default forecast till max_horizon.\\n  normalize_inputs: Whether to normalize the inputs. This is useful when the\\n    raw inputs are of extremely large or small magnitudes which may result in\\n    numerical issues.\\n  window_size: The window size for decomposed forecasting.\\n    TODO(siriuz42):implement it.\\n  per_core_batch_size: The batch size per core. Used at inference time during\\n    batched inference when multiple GPU / TPU devices are used.\\n  use_continuous_quantile_head: Whether to use a separate continuous quantile\\n    head to avoid quantile collapsing.\\n  force_flip_invariance: Whether to force flip invariance. TimesFM guarantees\\n    that TimesFM(aX + b) = a * TimesFM(x) + b for a >= 0 by default. This flag\\n    extends it to a < 0 as well.\\n  infer_is_positive: Whether to guarantee nonnegativity of the output if the\\n    input is nonnegative.\\n  fix_quantile_crossing: Whether to fix quantile crossing.\\n  return_backcast: Whether to return backcast.\n",
      "1  TimesFM_2p5_200M_torch   class  timesfm.timesfm_2p5.timesfm_2p5_torch  compile, forecast, forecast_with_covariates, from_pretrained, generate_model_card, load_checkpoint, model, push_to_hub, save_pretrained                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                PyTorch implementation of TimesFM 2.5 with 200M parameters.\n",
      "2                 configs  module                                    N/A                                                                                                                                      NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <module 'timesfm.configs' from 'c:\\\\Users\\\\hashimoto.ryohei\\\\miniconda3\\\\envs\\\\kaiseki\\\\Lib\\\\site-pa\n",
      "3                    flax  module                                    N/A                                                                                                                                      NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <module 'timesfm.flax' from 'c:\\\\Users\\\\hashimoto.ryohei\\\\miniconda3\\\\envs\\\\kaiseki\\\\Lib\\\\site-packa\n",
      "4             timesfm_2p5  module                                    N/A                                                                                                                                      NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <module 'timesfm.timesfm_2p5' (<_frozen_importlib_external.NamespaceLoader object at 0x00000212D7980\n",
      "5       timesfm_2p5_torch  module                                    N/A                                                                                                                                      NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <module 'timesfm.timesfm_2p5.timesfm_2p5_torch' from 'c:\\\\Users\\\\hashimoto.ryohei\\\\miniconda3\\\\envs\\\n",
      "6                   torch  module                                    N/A                                                                                                                                      NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <module 'timesfm.torch' from 'c:\\\\Users\\\\hashimoto.ryohei\\\\miniconda3\\\\envs\\\\kaiseki\\\\Lib\\\\site-pack\n",
      "\n",
      "================================================================================\n",
      "Summary by Type\n",
      "================================================================================\n",
      "type\n",
      "module    5\n",
      "class     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Classes Details\n",
      "================================================================================\n",
      "\n",
      "Class: ForecastConfig\n",
      "Module: timesfm.configs\n",
      "Methods: \n",
      "Description: Options for forecasting.\n",
      "\n",
      "Attributes:\n",
      "  max_context: The maximum context length. This is used by the complied decode\n",
      "    function at inference time during batched inference. Any input time series\n",
      "    ...\n",
      "\n",
      "Class: TimesFM_2p5_200M_torch\n",
      "Module: timesfm.timesfm_2p5.timesfm_2p5_torch\n",
      "Methods: compile, forecast, forecast_with_covariates, from_pretrained, generate_model_card, load_checkpoint, model, push_to_hub, save_pretrained\n",
      "Description: PyTorch implementation of TimesFM 2.5 with 200M parameters....\n",
      "\n",
      "✓ Features saved to 'timesfm_features.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import inspect\n",
    "import timesfm\n",
    "\n",
    "def get_library_features(module):\n",
    "    \"\"\"ライブラリの機能を取得してDataFrameに格納する\"\"\"\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # モジュール内のすべての要素を取得\n",
    "    for name, obj in inspect.getmembers(module):\n",
    "        # プライベート属性をスキップ\n",
    "        if name.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        feature_info = {\n",
    "            'name': name,\n",
    "            'type': type(obj).__name__,\n",
    "            'module': getattr(obj, '__module__', 'N/A'),\n",
    "        }\n",
    "        \n",
    "        # クラスの場合\n",
    "        if inspect.isclass(obj):\n",
    "            feature_info['type'] = 'class'\n",
    "            # メソッド一覧を取得\n",
    "            methods = [m for m in dir(obj) if not m.startswith('_') and callable(getattr(obj, m, None))]\n",
    "            feature_info['methods'] = ', '.join(methods[:10])  # 最初の10個のみ\n",
    "            feature_info['docstring'] = inspect.getdoc(obj) or 'No description'\n",
    "            \n",
    "        # 関数の場合\n",
    "        elif inspect.isfunction(obj) or inspect.isbuiltin(obj):\n",
    "            feature_info['type'] = 'function'\n",
    "            try:\n",
    "                sig = inspect.signature(obj)\n",
    "                feature_info['signature'] = str(sig)\n",
    "            except:\n",
    "                feature_info['signature'] = 'N/A'\n",
    "            feature_info['docstring'] = inspect.getdoc(obj) or 'No description'\n",
    "            \n",
    "        # その他の属性\n",
    "        else:\n",
    "            feature_info['docstring'] = str(obj)[:100] if not callable(obj) else 'N/A'\n",
    "        \n",
    "        features.append(feature_info)\n",
    "    \n",
    "    # DataFrameに変換\n",
    "    df = pd.DataFrame(features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# timesfmライブラリの機能を取得\n",
    "df_features = get_library_features(timesfm)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"=\" * 80)\n",
    "print(\"TimesFM Library Features\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal features found: {len(df_features)}\")\n",
    "print(\"\\n\")\n",
    "print(df_features.to_string())\n",
    "\n",
    "# カテゴリ別に集計\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary by Type\")\n",
    "print(\"=\" * 80)\n",
    "print(df_features['type'].value_counts())\n",
    "\n",
    "# クラスだけを抽出\n",
    "classes_df = df_features[df_features['type'] == 'class'].copy()\n",
    "if not classes_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Classes Details\")\n",
    "    print(\"=\" * 80)\n",
    "    for idx, row in classes_df.iterrows():\n",
    "        print(f\"\\nClass: {row['name']}\")\n",
    "        print(f\"Module: {row['module']}\")\n",
    "        print(f\"Methods: {row['methods']}\")\n",
    "        print(f\"Description: {row['docstring'][:200]}...\")\n",
    "\n",
    "# 関数だけを抽出\n",
    "functions_df = df_features[df_features['type'] == 'function'].copy()\n",
    "if not functions_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Functions Details\")\n",
    "    print(\"=\" * 80)\n",
    "    for idx, row in functions_df.iterrows():\n",
    "        print(f\"\\nFunction: {row['name']}\")\n",
    "        print(f\"Signature: {row.get('signature', 'N/A')}\")\n",
    "        print(f\"Description: {row['docstring'][:200] if len(row['docstring']) > 200 else row['docstring']}\")\n",
    "\n",
    "# CSVに保存する場合\n",
    "df_features.to_csv('timesfm_features.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\n✓ Features saved to 'timesfm_features.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63f53d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TimesFM Detailed Analysis\n",
      "================================================================================\n",
      "\n",
      "Total items: 15\n",
      "Public items: 15\n",
      "\n",
      "\n",
      "Distribution by Type:\n",
      "type\n",
      "function    8\n",
      "module      5\n",
      "class       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Available Classes:\n",
      "================================================================================\n",
      "  - ForecastConfig\n",
      "  - TimesFM_2p5_200M_torch\n",
      "\n",
      "================================================================================\n",
      "Available Functions:\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import inspect\n",
    "import timesfm\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def analyze_module_deeply(module, module_name='timesfm'):\n",
    "    \"\"\"モジュールを深く分析してDataFrameに格納\"\"\"\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    def analyze_object(obj, obj_name, parent_name=''):\n",
    "        \"\"\"オブジェクトを再帰的に分析\"\"\"\n",
    "        full_name = f\"{parent_name}.{obj_name}\" if parent_name else obj_name\n",
    "        \n",
    "        feature = {\n",
    "            'full_name': full_name,\n",
    "            'name': obj_name,\n",
    "            'parent': parent_name,\n",
    "            'type': '',\n",
    "            'is_public': not obj_name.startswith('_'),\n",
    "            'module': getattr(obj, '__module__', 'N/A'),\n",
    "            'signature': '',\n",
    "            'docstring': '',\n",
    "            'attributes': '',\n",
    "        }\n",
    "        \n",
    "        if inspect.isclass(obj):\n",
    "            feature['type'] = 'class'\n",
    "            feature['docstring'] = (inspect.getdoc(obj) or '')[:500]\n",
    "            \n",
    "            # クラスのメソッドとプロパティを取得\n",
    "            methods = []\n",
    "            properties = []\n",
    "            for attr_name in dir(obj):\n",
    "                if attr_name.startswith('_'):\n",
    "                    continue\n",
    "                try:\n",
    "                    attr = getattr(obj, attr_name)\n",
    "                    if callable(attr):\n",
    "                        methods.append(attr_name)\n",
    "                    else:\n",
    "                        properties.append(attr_name)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            feature['attributes'] = f\"Methods: {', '.join(methods[:5])}; Properties: {', '.join(properties[:5])}\"\n",
    "            \n",
    "        elif inspect.isfunction(obj) or inspect.ismethod(obj):\n",
    "            feature['type'] = 'function'\n",
    "            try:\n",
    "                feature['signature'] = str(inspect.signature(obj))\n",
    "            except:\n",
    "                feature['signature'] = 'N/A'\n",
    "            feature['docstring'] = (inspect.getdoc(obj) or '')[:500]\n",
    "            \n",
    "        elif inspect.ismodule(obj):\n",
    "            feature['type'] = 'module'\n",
    "            feature['docstring'] = (inspect.getdoc(obj) or '')[:500]\n",
    "            \n",
    "        else:\n",
    "            feature['type'] = type(obj).__name__\n",
    "            feature['docstring'] = str(obj)[:200] if not callable(obj) else ''\n",
    "        \n",
    "        return feature\n",
    "    \n",
    "    # トップレベルの要素を分析\n",
    "    for name, obj in inspect.getmembers(module):\n",
    "        if not name.startswith('_'):\n",
    "            feature = analyze_object(obj, name)\n",
    "            all_features.append(feature)\n",
    "            \n",
    "            # クラスの場合、メソッドも詳細に分析\n",
    "            if inspect.isclass(obj) and obj.__module__.startswith(module_name):\n",
    "                for method_name, method_obj in inspect.getmembers(obj):\n",
    "                    if not method_name.startswith('_') and (inspect.isfunction(method_obj) or inspect.ismethod(method_obj)):\n",
    "                        method_feature = analyze_object(method_obj, method_name, name)\n",
    "                        all_features.append(method_feature)\n",
    "    \n",
    "    df = pd.DataFrame(all_features)\n",
    "    return df\n",
    "\n",
    "# 詳細分析を実行\n",
    "df_detailed = analyze_module_deeply(timesfm)\n",
    "\n",
    "# 公開機能のみをフィルタ\n",
    "df_public = df_detailed[df_detailed['is_public'] == True].copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TimesFM Detailed Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal items: {len(df_detailed)}\")\n",
    "print(f\"Public items: {len(df_public)}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# タイプ別の集計\n",
    "print(\"Distribution by Type:\")\n",
    "print(df_public['type'].value_counts())\n",
    "\n",
    "# クラス一覧\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Available Classes:\")\n",
    "print(\"=\" * 80)\n",
    "classes = df_public[df_public['type'] == 'class']['name'].tolist()\n",
    "for cls in classes:\n",
    "    print(f\"  - {cls}\")\n",
    "\n",
    "# 関数一覧\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Available Functions:\")\n",
    "print(\"=\" * 80)\n",
    "top_level_funcs = df_public[(df_public['type'] == 'function') & (df_public['parent'] == '')]['name'].tolist()\n",
    "for func in top_level_funcs:\n",
    "    print(f\"  - {func}\")\n",
    "\n",
    "# Excel出力する場合\n",
    "with pd.ExcelWriter('timesfm_analysis.xlsx', engine='openpyxl') as writer:\n",
    "    df_public.to_excel(writer, sheet_name='All Features', index=False)\n",
    "    df_public[df_public['type'] == 'class'].to_excel(writer, sheet_name='Classes', index=False)\n",
    "    df_public[df_public['type'] == 'function'].to_excel(writer, sheet_name='Functions', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e242d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo: C:\\model_info\\timesfm_repo\n",
      "Rows: 56\n",
      "Saved: C:\\model_info\\timesfm_repo_feature_hits.xlsx\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Dict, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "REPO_URL = \"https://github.com/google-research/timesfm\"\n",
    "CLONE_DIR = (Path.cwd() / \"timesfm_repo\").resolve()  # ←絶対パスになります\n",
    "\n",
    "\n",
    "# =========\n",
    "# 1) clone\n",
    "# =========\n",
    "def ensure_cloned(repo_url: str, clone_dir: Path) -> None:\n",
    "    if clone_dir.exists() and any(clone_dir.iterdir()):\n",
    "        return\n",
    "    clone_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"--depth\", \"1\", repo_url, str(clone_dir)],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) ファイルからテキスト抽出\n",
    "# =========================\n",
    "TEXT_EXTS = {\".py\", \".md\", \".rst\", \".txt\", \".sh\", \".yml\", \".yaml\", \".toml\", \".ipynb\"}\n",
    "\n",
    "def read_text_from_file(p: Path) -> Optional[str]:\n",
    "    try:\n",
    "        if p.suffix == \".ipynb\":\n",
    "            nb = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "            chunks = []\n",
    "            for cell in nb.get(\"cells\", []):\n",
    "                src = cell.get(\"source\", [])\n",
    "                if isinstance(src, list):\n",
    "                    chunks.append(\"\".join(src))\n",
    "                elif isinstance(src, str):\n",
    "                    chunks.append(src)\n",
    "            return \"\\n\".join(chunks)\n",
    "        else:\n",
    "            return p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) 機能カテゴリ（痕跡）定義\n",
    "# =========================\n",
    "FEATURE_PATTERNS: Dict[str, List[str]] = {\n",
    "    \"再学習/微調整(fine-tuning)\": [\n",
    "        r\"\\bfinetune\\b\", r\"fine[-_ ]tune\", r\"\\btrain(ing)?\\b\",\n",
    "        r\"\\blora\\b\", r\"\\bdora\\b\", r\"\\bpeft\\b\",\n",
    "        r\"\\bpaxml\\b\", r\"\\bcheckpoint\\b\", r\"\\borbax\\b\",\n",
    "        r\"\\bnum[-_ ]epochs\\b\", r\"\\blearning[-_ ]rate\\b\",\n",
    "    ],\n",
    "    \"精度評価(metrics/evaluation)\": [\n",
    "        r\"\\beval\\b\", r\"\\bmetric(s)?\\b\", r\"\\bbenchmark\\b\",\n",
    "        r\"\\bmae\\b\", r\"\\bmse\\b\", r\"\\brmse\\b\", r\"\\bmape\\b\", r\"\\bsmape\\b\",\n",
    "        r\"\\bloss\\b\", r\"\\bvalidation\\b\", r\"\\bearly[-_ ]stop(ping)?\\b\",\n",
    "    ],\n",
    "    \"可視化(visualization)\": [\n",
    "        r\"\\bmatplotlib\\b\", r\"\\bseaborn\\b\", r\"\\bplot\\b\", r\"\\bchart\\b\",\n",
    "        r\"\\bplt\\.\", r\"\\bplotly\\b\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Hit:\n",
    "    feature: str\n",
    "    file_path: str\n",
    "    ext: str\n",
    "    matches: int\n",
    "    snippets: List[str]\n",
    "\n",
    "\n",
    "def find_hits_in_text(text: str, patterns: List[str], max_snips: int = 3) -> Tuple[int, List[str]]:\n",
    "    total = 0\n",
    "    snips: List[str] = []\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # 行単位で“どの行が引っかかったか”を拾う（初心者でも追跡しやすい）\n",
    "    for i, line in enumerate(lines):\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, line, flags=re.IGNORECASE):\n",
    "                total += 1\n",
    "                if len(snips) < max_snips:\n",
    "                    # 周辺行も少し付ける\n",
    "                    ctx = \"\\n\".join(lines[max(0, i-1): min(len(lines), i+2)])\n",
    "                    snips.append(ctx)\n",
    "                break\n",
    "    return total, snips\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) 走査 → df化\n",
    "# =========================\n",
    "def scan_repo_to_df(repo_dir: Path) -> pd.DataFrame:\n",
    "    hits: List[Hit] = []\n",
    "\n",
    "    for p in repo_dir.rglob(\"*\"):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        if p.suffix.lower() not in TEXT_EXTS:\n",
    "            continue\n",
    "\n",
    "        text = read_text_from_file(p)\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        for feat, pats in FEATURE_PATTERNS.items():\n",
    "            n, snips = find_hits_in_text(text, pats)\n",
    "            if n > 0:\n",
    "                hits.append(\n",
    "                    Hit(\n",
    "                        feature=feat,\n",
    "                        file_path=str(p.resolve()),\n",
    "                        ext=p.suffix.lower(),\n",
    "                        matches=n,\n",
    "                        snippets=snips,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame([h.__dict__ for h in hits])\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 見やすく：強い痕跡（matchesが多い）順\n",
    "    df = df.sort_values([\"feature\", \"matches\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # snippetsは長いのでJSON文字列化（Excelに落としても崩れにくい）\n",
    "    df[\"snippets\"] = df[\"snippets\"].apply(lambda xs: json.dumps(xs, ensure_ascii=False, indent=2))\n",
    "    return df\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    ensure_cloned(REPO_URL, CLONE_DIR)\n",
    "    df = scan_repo_to_df(CLONE_DIR)\n",
    "\n",
    "    out_xlsx = (Path.cwd() / \"timesfm_repo_feature_hits.xlsx\").resolve()\n",
    "    with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as w:\n",
    "        df.to_excel(w, index=False, sheet_name=\"hits\")\n",
    "\n",
    "    print(\"Repo:\", CLONE_DIR)\n",
    "    print(\"Rows:\", len(df))\n",
    "    print(\"Saved:\", out_xlsx)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60c0c543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解析を開始します...\n",
      "Processing Configs: src/timesfm/configs.py\n",
      "Warning: File not found: src/timesfm/configs.py\n",
      "Processing TorchModel: src/timesfm/timesfm_2p5/timesfm_2p5_torch.py\n",
      "Warning: File not found: src/timesfm/timesfm_2p5/timesfm_2p5_torch.py\n",
      "Processing BaseModel: src/timesfm/timesfm_2p5/timesfm_2p5_base.py\n",
      "Warning: File not found: src/timesfm/timesfm_2p5/timesfm_2p5_base.py\n",
      "データが見つかりませんでした。パスを確認してください。\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "def get_annotation_str(annotation: Optional[ast.AST]) -> str:\n",
    "    \"\"\"型アノテーションを文字列に変換するヘルパー関数\"\"\"\n",
    "    if annotation is None:\n",
    "        return \"Any\"\n",
    "    try:\n",
    "        if hasattr(ast, 'unparse'):\n",
    "            return ast.unparse(annotation)\n",
    "        else:\n",
    "            # Python 3.8以下用（簡易版）\n",
    "            if isinstance(annotation, ast.Name):\n",
    "                return annotation.id\n",
    "            return \"ComplexType\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def get_value_str(value_node: Optional[ast.AST]) -> str:\n",
    "    \"\"\"デフォルト値を文字列に変換するヘルパー関数\"\"\"\n",
    "    if value_node is None:\n",
    "        return \"-\"\n",
    "    try:\n",
    "        if hasattr(ast, 'unparse'):\n",
    "            return ast.unparse(value_node)\n",
    "        # リテラル値の処理\n",
    "        if isinstance(value_node, (ast.Constant, ast.Str, ast.Num)):\n",
    "            return str(value_node.value) if hasattr(value_node, 'value') else str(value_node.s)\n",
    "        return \"ComplexValue\"\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "def analyze_file(file_path: str, target_classes: List[str] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"指定されたファイルを解析し、クラス・メソッド・引数の情報を抽出する\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: File not found: {file_path}\")\n",
    "        return []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            tree = ast.parse(f.read())\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            class_name = node.name\n",
    "            if target_classes and class_name not in target_classes:\n",
    "                continue\n",
    "\n",
    "            # Docstringの取得 (クラス)\n",
    "            class_doc = ast.get_docstring(node)\n",
    "            \n",
    "            # 1. Dataclassフィールドの解析 (Configs用)\n",
    "            # クラス直下のAnnotated Assignment (例: max_context: int = 512) を探す\n",
    "            for item in node.body:\n",
    "                if isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name):\n",
    "                    field_name = item.target.id\n",
    "                    field_type = get_annotation_str(item.annotation)\n",
    "                    default_val = get_value_str(item.value)\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"Category\": \"Configuration\",\n",
    "                        \"Class\": class_name,\n",
    "                        \"Method/Field\": \"(Config Field)\",\n",
    "                        \"Argument\": field_name,\n",
    "                        \"Type\": field_type,\n",
    "                        \"Required\": \"No\" if item.value else \"Yes\",\n",
    "                        \"Default\": default_val,\n",
    "                        \"Description\": \"See Class Docstring\" \n",
    "                    })\n",
    "\n",
    "            # 2. メソッドの解析\n",
    "            for item in node.body:\n",
    "                if isinstance(item, ast.FunctionDef):\n",
    "                    method_name = item.name\n",
    "                    if method_name.startswith(\"_\") and method_name != \"__init__\":\n",
    "                        continue # プライベートメソッドは除外\n",
    "\n",
    "                    method_doc = ast.get_docstring(item)\n",
    "                    \n",
    "                    # 引数のデフォルト値処理\n",
    "                    # defaultsは後ろの引数から対応するため、位置合わせが必要\n",
    "                    num_defaults = len(item.args.defaults)\n",
    "                    num_args = len(item.args.args)\n",
    "                    args_with_defaults = item.args.args[num_args - num_defaults:]\n",
    "                    defaults_map = {}\n",
    "                    for arg, default in zip(args_with_defaults, item.args.defaults):\n",
    "                        defaults_map[arg.arg] = get_value_str(default)\n",
    "\n",
    "                    # 全引数をスキャン\n",
    "                    for arg in item.args.args:\n",
    "                        arg_name = arg.arg\n",
    "                        if arg_name == \"self\":\n",
    "                            continue\n",
    "                        \n",
    "                        arg_type = get_annotation_str(arg.annotation)\n",
    "                        is_required = arg_name not in defaults_map\n",
    "                        default_val = defaults_map.get(arg_name, \"-\")\n",
    "\n",
    "                        results.append({\n",
    "                            \"Category\": \"Method Argument\",\n",
    "                            \"Class\": class_name,\n",
    "                            \"Method/Field\": method_name,\n",
    "                            \"Argument\": arg_name,\n",
    "                            \"Type\": arg_type,\n",
    "                            \"Required\": \"Yes\" if is_required else \"No\",\n",
    "                            \"Default\": default_val,\n",
    "                            \"Description\": (method_doc.split('\\n')[0] if method_doc else \"\")[:100]\n",
    "                        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # 解析対象のファイルパス定義\n",
    "    # ※ 環境に合わせてパスを修正してください\n",
    "    target_files = {\n",
    "        \"Configs\": (\"src/timesfm/configs.py\", [\"ForecastConfig\"]),\n",
    "        \"TorchModel\": (\"src/timesfm/timesfm_2p5/timesfm_2p5_torch.py\", [\"TimesFM_2p5_200M_torch\"]),\n",
    "        \"BaseModel\": (\"src/timesfm/timesfm_2p5/timesfm_2p5_base.py\", [\"TimesFM_2p5_200M_torch\", \"TimesFM_2p5Base\"]),\n",
    "    }\n",
    "\n",
    "    all_data = []\n",
    "    print(\"解析を開始します...\")\n",
    "\n",
    "    for label, (path, classes) in target_files.items():\n",
    "        print(f\"Processing {label}: {path}\")\n",
    "        data = analyze_file(path, target_classes=classes)\n",
    "        all_data.extend(data)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"データが見つかりませんでした。パスを確認してください。\")\n",
    "        return\n",
    "\n",
    "    # DataFrame作成\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # 見やすい順序に並べ替え\n",
    "    sort_order = [\"Configuration\", \"Method Argument\"]\n",
    "    df[\"Category\"] = pd.Categorical(df[\"Category\"], categories=sort_order, ordered=True)\n",
    "    df = df.sort_values(by=[\"Category\", \"Class\", \"Method/Field\"]).reset_index(drop=True)\n",
    "\n",
    "    # 結果の表示\n",
    "    print(\"\\n=== TimesFM Execution Parameters ===\")\n",
    "    print(df.to_markdown(index=False))\n",
    "    \n",
    "    # CSV出力\n",
    "    df.to_csv(\"timesfm_execution_table.csv\", index=False)\n",
    "    print(\"\\n'timesfm_execution_table.csv' に保存しました。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaiseki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
